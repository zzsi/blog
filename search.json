[
  {
    "objectID": "posts/20240825-sustainable-future/index.html",
    "href": "posts/20240825-sustainable-future/index.html",
    "title": "From Consumerism to Sustainability: AIâ€™s Role in Shaping the Future of Economic Growth",
    "section": "",
    "text": "Since the advent of the free market, human society has experienced an unprecedented wave of growth and prosperity. Global GDP has increased 100-fold, with per-capita GDP rising 15-fold since the early 1800s. However, this tremendous growth has exacted a significant toll on the environment. As we stand on the brink of another nascent revolution, artificial intelligence (AI) can usher in a second wave of growthâ€”this time, much more sustainable. Could AI help us achieve the elusive goal of expanding our economies while preserving the planet?"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "href": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "title": "From Consumerism to Sustainability: AIâ€™s Role in Shaping the Future of Economic Growth",
    "section": "The Miracle and Pitfall of Demand-Driven Production",
    "text": "The Miracle and Pitfall of Demand-Driven Production\n\nThe miracle\nThe past two centuries have indeed been nothing short of a miracle in terms of economic growth, not just for the sheer scale of economic expansion but for its profound impact on human well-being.\nBefore the Industrial Revolution, global poverty was widespread, with the vast majority of the population living on subsistence agriculture, vulnerable to disease, famine, and political instability. But with the advent of mechanized production, steam power, and eventually electricity, societies began to shift from agrarian economies to industrial ones, spurring rapid urbanization and creating millions of new jobs.\n\n (source)\n\n (source)\nAs economies grew, so did living standards. In the 20th century, especially after World War II, growth accelerated dramatically. Advances in medicine, sanitation, and food production allowed populations to boom while simultaneously reducing mortality rates. Global poverty, which once seemed an inescapable fate for most, began to decline sharply. According to the World Bank, extreme poverty (defined as living on less than $1.90 a day) fell from about 80% of the worldâ€™s population in 1820 to less than 10% today. This reduction in poverty was most pronounced in Asia, where countries like China and India harnessed industrialization and global trade to lift hundreds of millions out of destitution.\nAs the engines of industry roared to life, they did more than just produceâ€”they created a world where, for the first time, sufficient goods could be made to meet the needs of millions. Farms, once worked by hand, now harnessed the power of machines, yielding crops at unprecedented rates. Factories churned out textiles, tools, and eventually, the comforts of modern living that had once been unimaginable luxuries. This newfound capacity wasnâ€™t just about survival; it was about abundance. Goods that had once been scarce or accessible only to the wealthy became attainable for the masses. Food production soared, homes were built, and technologies that improved everyday life spread across the globe. In this wave of growth, the world became a place where production was not only sufficient but could also fulfill the aspirations of those who sought more than just the bare necessities.\n\n\nThe pitfall\nWhile we feel grateful for the growth and abundance that this era of production has brought us, itâ€™s important to recognize the shadows cast by this prosperity. For every product that meets a need, there are countless others that sit unused, discarded, or wasted. The very systems that allowed us to produce more than ever before also led to overproduction, filling landfills with excess and polluting our air and waters with the byproducts of unchecked growth.\n\n\n\nCar graveyard after Chinese company went bankrupt. Source: @Wolf of X\n\n\n\n\n\nAerial picture of the tire graveyard in Kuwait. Final resting place of over 7,000,000 rubber tires. Source: @Wolf of X\n\n\n\n\n\nClothing graveyard. The so-called â€œclothing graveyard,â€ about 30,000 tons of discarded clothing piled in a landfill in the Atacama Desert, Chile, in 2021. Source: Antonio Cossioâ€”picture alliance/Getty Images\n\n\nIs such a level of waste inevitable? I would argue that it is, given the nature of how our economies have evolved. The growth weâ€™ve witnessed, particularly over the last century, has been driven largely by demandâ€”an insatiable appetite for more. With the rise of consumerism, the focus shifted from simply meeting needs to creating new desires. As historian Frederick Allen observed, â€œBusiness had learned as never before the importance of the ultimate consumer. Unless he could be persuaded to buy and buy lavishly, the whole stream of six-cylinder cars, super heterodynes, cigarettes, rouge compacts, and electric ice boxes would be dammed up at its outlets.â€ (source)\nThis relentless push to fuel demand led companies to innovate not just in production but also in persuasion. Advertising, marketing, and product design all became tools to keep the consumer engaged and always wanting more. The result? A system where the pressure to buy, to replace, and to upgrade created a cycle of overproduction and, inevitably, waste."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "href": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "title": "From Consumerism to Sustainability: AIâ€™s Role in Shaping the Future of Economic Growth",
    "section": "Is consumerism at fault?",
    "text": "Is consumerism at fault?\nThe solution is not to stay away from consumerism and demand-driven market economy. Without demand, there would be no profit, and without profit, companies would have no reason to put products on the market. This, in turn, would halt productivity, leaving not enough food on familiesâ€™ tables or goods in their homes.\nOver-production is also inevitable. The reality is that producing just enough to meet actual needs isnâ€™t sufficient, because market systems and distribution networks are inherently imperfect. Food, clothes that are produced do not always reach who need them. True efficiency is hard to achieve, and inequality makes this even worse. If the distribution efficiency is only 10%, then we must produce ten times the necessary amount to meet the demand. This excess production, while ensuring availability, often leads to surplus and waste.\nSurplus eats into profits if it isnâ€™t consumed. To keep factories running, corporations thriving, and jobs secure, our dear consumers must continually want more. This is the crux of the demand-driven economy: without constant consumption, the entire system risks stagnation. As a result, businesses invest heavily in marketing, innovation, and new product lines to stimulate desire, encouraging consumers to keep buyingâ€”whether or not their needs have truly changed.\n\n\n\nGrowth of supply and manufactured demand beyond need"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "href": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "title": "From Consumerism to Sustainability: AIâ€™s Role in Shaping the Future of Economic Growth",
    "section": "A way out: targeted production with AI",
    "text": "A way out: targeted production with AI\nAmazonâ€™s inventory planning system points to a promising direction. Algorithms can forecast what consumers are likely to purchase with remarkable accuracy, which allow buying and placing inventory accordingly to optimize order fulfillment. As a result, efficiency went up, and waste went down.\nAnd we can push this even further. If demand is way higher than actual need, why not shift production to better match what people really need? Imagine if we werenâ€™t constantly hit with endless ads and social media bragging. Our homes would be less cluttered, people wouldnâ€™t need to take on debt just to keep up with the luxury status game, and we could all spend more time with loved ones or out in nature. Life would feel simpler and more focused on what really matters, rather than being driven by overconsumption.\nThis can happen through targeted production, with AI helping in two ways: automation (boosting production efficiency) and forecasting (improving market efficiency).\nAutomation isnâ€™t newâ€”itâ€™s been part of past tech revolutionsâ€”but AI is different because itâ€™s more versatile. It can handle many things from language tasks to tool use, extending the â€˜Crown Jewelsâ€™ of human intelligence. AI can streamline workflows across corporate functions like accounting, finance, engineering, sales, and marketing, making processes faster and more efficient. Forecasting will further increase market efficiency by accurately predicting demand, allowing businesses to align production more closely with real-time consumer needs. These two factorsâ€”automation and forecastingâ€”make anticipatory production and just-in-time production possible. Instead of waiting for demand to fully materialize, we can anticipate and initiate production just ahead of timeâ€”producing what is likely needed, when itâ€™s needed.\nIndirectly, AI can help curb the constant stimulation of consumer desires. The problem isnâ€™t advertising itself but rather the excessive advertising that arises in overcrowded, saturated markets. When businesses struggle to meaningfully differentiate their products, they rely heavily on aggressive marketing to capture attention, contributing to the cycle of overconsumption. This reflects poor planning and a lack of clear insight into what consumers truly needâ€”a symptom of incomplete information and insufficient foresight into future demands.\nWhen businesses begin to realize they can be profitable with automation and better planning instead of excessive advertising, they can step back from the exhausting zero-sum game of trying to out-market each other. Their shareholders and employees can finally find peace."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "href": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "title": "From Consumerism to Sustainability: AIâ€™s Role in Shaping the Future of Economic Growth",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe AI revolution is still in its early days, and there are challenges like job displacement and energy use that worry people. But despite these hurdles, I am hopeful AI can help future generations enjoy a more sustainable and prosperous future."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html",
    "href": "posts/20241002-minisora-part1/index.html",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AIâ€™s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether itâ€™s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORAâ€™s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathyâ€™s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, Iâ€™d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a â€œflying MNISTâ€ toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#introduction",
    "href": "posts/20241002-minisora-part1/index.html#introduction",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AIâ€™s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether itâ€™s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORAâ€™s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathyâ€™s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, Iâ€™d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a â€œflying MNISTâ€ toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "href": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Pareto frontier: Aiming for good and small",
    "text": "Pareto frontier: Aiming for good and small\n\n\n\n\n\nThis graph from â€œHype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AIâ€ illustrates a key challenge in AI development. SORA would be a frontier model at the resource-intensive end of the spectrum. We want to move in the direction of the green arrow, striving for lower training cost while maintaining high quality.\nAccess to vast computational resources, such as 10,000 A100 GPUs, is limited to a handful of organizations. Even if such resources were widely available, focusing solely on resource-intensive methods would be an inefficient use of our capabilities. The design space for training recipes is vast, and a strategic approach involves exploring this space through low-cost experiments before scaling up when confidence is high.\nThis raises an intriguing question: With a modest budget, is it possible to train a general-purpose video generation model comparable to SORA?"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "href": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "The need for controlling the domain complexity",
    "text": "The need for controlling the domain complexity\n\nThe challenge of training general-purpose models with limited resources\nSORAâ€™s training costs likely run into tens of millions of dollars, driven by both data and model size. Larger datasets necessitate longer training times, while bigger models require both extended training periods and more high-end GPUs.\nIs it feasible to train a high-quality model with a significantly smaller dataset? This seems impossible due to the inherent complexity of our world. Are one million video clips sufficient to capture our worldâ€™s complexity? 10 Million? 100 Million? Probably more than that. While sample-efficient algorithms can help reduce the required data size, the order of magnitude for necessary data likely remains substantial.\nSimilarly, training a high-quality model with a much smaller architecture presents its own challenges. Unless a dramatically more efficient architecture than Transformers emerges, a small model would struggle to capture the complexity present in such vast datasets.\nTherefore, to make progress with limited resources, we must find ways to reduce the data size.\n\n\nExploring niche domains: A path to low-budget training\nNiche domains can be significantly simpler than our physical world, potentially allowing a few tens of thousands of observations to sufficiently represent the domain. With a drastic reduction in data size, smaller models and lower training costs become feasible.\nWe can conceptualize a series of domains, progressing from simple to complex:\n\n2D Flying MNIST (a 2D world with colorful handwritten digits moving at constant speed, bouncing off boundaries)\n2D arcade games (Pong, Breakout, etc.)\nAnime and cartoons\nLimited locations: video walkthroughs of 3D house models, fly-through views of objects (e.g., NERF models)\nLimited objects: close-up videos of specific subjects (e.g., dogs, selfie videos)\nLimited scenery: footage of hiking trails, beaches, etc.\nPublic video datasets: UCF-101, Panda-70M, InterVid, etc.\nThe real world, and our collective video reservoir.\n\nA strategic approach involves starting from the simplest domain and gradually progressing towards more complex ones. Effective training recipes discovered in simpler domains are expected to scale to more complex scenarios with straightforward increases in data and model size.\nInterestingly, this mirrors how humans learn: start from simple lessons and gradually build up to more complex concepts.\n\n\nPre-train or fine-tune?\nFine-tuning is an effective strategy to reduce training costs, but it comes with certain limitations:\n\nFixed architecture: The modelâ€™s architecture is predetermined, which can be a significant constraint as we may still be far from an optimal design for video generation tasks.\nVAE dependency: Pre-trained weights often rely on a specific Variational Autoencoder (VAE), limiting the design space and opportunities to further reduce training costs.\n\nDespite these limitations, fine-tuning has shown promising results. For example, the team at Lambda Labs open-sourced an intriguing Text2Bricks experiment, fine-tuning OpenSora weights on Lego videos. This project required approximately 1000 A100 GPU hours and 10,000 videos. We can anticipate further reductions in cost as more advanced pre-trained models become available and more sample-efficient fine-tuning algorithms are developed.\nFor my experiments, I try to find the simplest domain that has non-trivial complexity: a toy 2D world with flying digits. The scale of this toy world is small enough that pre-training from scratch is not prohibitively expensive, allowing for more freedom in exploring different model architectures and training strategies.\nLetâ€™s see some details."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "href": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Flying MNIST Simulator",
    "text": "Flying MNIST Simulator\nA Python script is used to simulate a toy 2D world where colorful handwritten digits fly and bounce around. An example is shown below.\n\n\n\n\n\nFor training, I used up to 100k clips, each with 32 frames, covering roughly 6 seconds at 5 fps. This amounts to 160 hours of video. Is this a lot? Letâ€™s compare with human learning. If a baby is awake and actively observing 5 hours per day, it would be roughly a month of learning. It would be interesting to see if the AI can learn:\n\nObject identity: a digit is a digit, and not a random blob\nObject permanence: a digit does not suddenly disappear\nDistinct digits: whether the model can learn to distinguish between different digits\nConsistent colors: color of a digit remains consistent\nPhysics: digits follow simple physics - constant speed and bounce off walls"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "href": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "VAE: The Compressor",
    "text": "VAE: The Compressor\nThe first model to train is a compressor. Unlike language, images and videos have very high dimensionality: a tiny 2-second 256x256 video contains over 100 million numbers. Compression is necessary for the model to work.\nThe compressor of choice is a VAE (Variational Auto-Encoder) with an encoder and decoder. The encoder converts a video clip into a latent space, and the decoder converts the latent space back to a video clip. The latent space is a compact representation of the original data and is easier to model.\nOptionally, you can quantize the latent space using vector quantization, which gives you a VQ-VAE. Quantization gives rise to a vocabulary of visual words or tokens. This enables the use of language model training recipes on 1-dimensional (flattened) sequences of token IDs. While I was initially skeptical, the results were surprisingly good.\nTraining a small VAE is relatively quick. I trained a spatial-temporal VQ-VAE with 4x temporal compression and 4x4 spatial compression, using a vocabulary size of 5120. The training run documented in Weights & Biases achieved a good balance of reconstruction quality and compression rate. It took about 2 A10 GPU hours to converge.\nWith this VAE model, you can transform a 32-frame video clip (32 x 3 x 256 x 256) into latent â€œtokensâ€. Without quantization, the compressed representation of the video has a shape of 8 x 4 x 64 x 64 (each â€œtokenâ€ is a 4-dimensional floating point vector, and there are 8 x 64 x 64 = 32,768 tokens). With quantization, the compressed representation is simply 8 x 64 x 64 = 32,768 integers (token IDs). The range of the token IDs is from 0 to 5,023.\n\n\n\n\n\nWith this compact tokenized representation, we are ready to train a generator."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "href": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Generator in the Latent Space",
    "text": "Generator in the Latent Space\nThere are two approaches to generate video in the latent space: the autoregressive next-token predictor (language model) and the diffusion model.\n\nAutoregressive Next-Token Predictor\nEach 32-frame video clip is represented as a sequence of 32,768 tokens. The video clips are then concatenated to form a long sequence, separated by a special start-of-video token. This long sequence is fed into a language model training recipe.\nI used nanoGPT to train a 60MB model with the GPT-2 architecture. The model is trained to predict the next token ID in the latent space, instead of the next English token. It worked surprisingly well and began to learn the spatial-temporal patterns quickly.\nThe main ingredient for video quality is ensuring a sufficiently large context window. I used 6,000 tokens, which is much larger than the typical GPT-2 setting. However, this is still a small window size for video. Each video frame is 4,096 tokens, so this context window allows the model to look back only slightly more than one frame, making temporal consistency challenging to enforce.\nSecondly, the training sample size is crucial. Using 100k clips produces better results than 10k clips, and much better than 1k clips. The question remains whether we should use even more data. I hope not, as if such a simple 2D world requires much more than 100k training examples, it would be concerning for more complex domains.\nThis training run showcases one of the better results using nanoGPT.\nThe generated videos start out as random compositions of visual tokens:\n\n\nVideo\n\n\nAfter 6 hours of training, line strokes started to appear:\n\n\nVideo\n\n\n24 hours in, the digits began to emerge, but temporal consistency was poor:\n\n\nVideo\n\n\nAfter 10 days, consistency and physics were much improved:\n\n\nVideo\n\n\nFor comparison, hereâ€™s a training run using a 1,024 token context window.\nWith a smaller context window, the training time is much shorter (1 day to converge), but temporal consistency is poor, and digits would suddenly appear throughout the clip:\n\n\nVideo\n\n\n\n\nDiffusion Model\nFor the diffusion model, I used ST-DIT from OpenSora and Stable Diffusionâ€™s SD VAE.\nIn this approach, the context window encompasses the entire video clip, so I expected more temporal consistency than the autoregressive counterpart. Training sample size still plays a significant role. Using a 24GB A10 GPU, I needed to use a small version of the diffusion transformer model.\nA representative training run can be found here.\nThe generated videos also start out as random compositions of visual tokens (resembling crops of natural images this time):\n\n\nVideo\n\n\nAfter one day of training, localized dream-like flowing patterns emerged, though they didnâ€™t yet resemble digits:\n\n\nVideo\n\n\nOn day 3, the moving patterns began to look like digits, but they were so fluid that they seemed to lack â€œbonesâ€-like structure:\n\n\nVideo\n\n\nBy day 10, the digits were much more stable and distinct, and the moving patterns were steady and smooth:\n\n\nVideo"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#whats-next",
    "href": "posts/20241002-minisora-part1/index.html#whats-next",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Whatâ€™s Next",
    "text": "Whatâ€™s Next\n250 A10 hours (or approximately 80 A100 hours, costing around $200) proved sufficient to adequately solve the video generation task for the 2D toy world of Flying MNIST Digits.\nContext window size and data sample size are important factors for quality, but also drive up cost. There are numerous new techniques that are worth exploring to improve quality while reducing cost. Hereâ€™s a non-exhaustive list:\n\nFlow matching: This technique could enhance the temporal consistency of generated videos.\nBetter quantized VAE for auto-regressive video generation: Improving the VAE could lead to more efficient and higher-quality latent representations.\nToken masking: This could reduce the \\(N\\) in the \\(O(N^2)\\) complexity of attention layers, potentially speeding up training and inference.\nCoarse-to-fine generation: Generating whole video frames at the coarse level first, then progressively refining to small details. This can dramatically reduce the context window size and compute cost.\nBetter positional encoding for long context windows in the temporal-spatial setting.\nHyper-optimized LLM training with long context (e.g., llm.c).\nCombining strengths of autoregressive and diffusion models could yield interesting results.\nCurriculum learning: Starting with simpler tasks and progressively increasing difficulty could improve learning efficiency.\n\nThese avenues for improvement suggest that thereâ€™s still significant potential to enhance the quality and efficiency of video generation models, even in this simplified domain. As we continue to refine these techniques, weâ€™ll be better positioned to tackle more complex video generation tasks in the future.\nMore to come."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html",
    "href": "posts/20240915-soccer-tracking/index.html",
    "title": "Computer vision for soccer games",
    "section": "",
    "text": "I was intrigued to see this example where multiple (at least 5) computer vision techniques to create visual appealing analytics from soccer game footage. Soccer fans and coaches may enjoy this.\nVideo\nThis is an open source demo from Roboflow, and is easy to reproduce. Since it is a proof of concept, more work needs to be done to make it work for other real world videos, where there a large portion of the soccer field is not visible, or when the camera moved fast (which happens quite often). This is a common challenge for practical computer vision: it can be hard to make an impressive model work on your data.\nBelow I share a workflow to reproduce both success and limitations of this soccer tracking example, and some ideas to improve it to make it work on more challenging data. Similar techniques can be applied to other sports, like tennis, (American) football, basketball, pickle ball, etc."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "href": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "title": "Computer vision for soccer games",
    "section": "Reproducing the birds-eye view creation",
    "text": "Reproducing the birds-eye view creation\n\n\n\n\n\n\nPre-requisites\n\n\n\n\n\nPre-requisites\n\nYou need a machine with GPU to run the code. The code is tested on a machine with a GeForce RTX 3090, and it uses about 3GB of GPU memory.\nYou need to have git, docker and python (3.6+) installed.\nNVidia container toolkit is required to use the GPU in the docker container.\n\n\n\n\n\n\n\n\n\n\nDownload\n\n\n\n\n\nStep 1: Download the code and data\ncvlization is an open source repo with many working examples of computer vision workflows. Clone the repo:\ngit clone https://github.com/kungfuai/cvlization.git\ncd cvlization\nIn examples/sports/soccer_game_visual_tracking, there is a README file that explains how to download the model weights and example video data (pip install gdown if you havenâ€™t already).\ncd examples/sports/soccer_game_visual_tracking\nbash download_data.sh\n\n\n\n\n\n\n\n\n\nInstall\n\n\n\n\n\nStep 2: Install the dependencies by building a docker image\nChange directory back to the root of the cvlization repo, and run\nbash examples/sports/soccer_game_visual_tracking/build.sh\nThis will build a docker image with necessary dependencies. If you prefer to not use docker, you can install the dependencies manually by following the instructions in the Dockerfile in the same directory.\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\n\n\nStep 3: Run the code\nbash examples/sports/soccer_game_visual_tracking/predict.sh\nThis will use the docker image to run the code. If you prefer to run the code without docker, you can directly use the command in the predict.sh script in the same directory.\nIn this script, we are using a 30 second clip from a soccer game. The script will track the pitch and players, identify the team, goal keepers, referee, and ball, and generate a birdâ€™s eye view video. Feel free to modify the script to use a different video or to change the tracking parameters.\nYou will find the output video in examples/sports/soccer_game_visual_tracking/0bfacc_0-radar.mp4. This is the video shown on the top of the page. On a machine with a GeForce RTX 3090, it takes about 20 minutes to run, with 3GB of GPU memory used."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "href": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "title": "Computer vision for soccer games",
    "section": "Under the hood",
    "text": "Under the hood\nThe computer vision models and algorithms under the hood include:\n\nA keypoint detection (pose estimation) model for 32 keypoints on the soccer pitch (Yolo-v8, 70M, training notebook, mAP=0.99, 1 hour on NVidia T4, trained on hundreds of images).\n\n\n\n\n\n\n\nAn object detection model for players, referrees and goal keepers (Yolo-v8, 68M, training notebook, mAP=0.79, 40min on NNivida L4).\n\n\n\n\n\n\n\nAnother object detection model for the ball (Yolo-v8, 68M, training notebook, mAP=0.93, 1.3 hours on NVidia A100). The ball is very small in the image, so it is hard to detect.\nA multi-object tracking model to track the players and the ball (Bytetrack, implementation and python API).\nA vision embedding model and clustering algorithm for team identification. SigLIP is used to extract embedding vectors from cropped players. UMAP is used for dimensionality reduction. K-means is used for clustering. Also Resolve the team IDs for detected goalkeepers based on the proximity to team centroids (based on player locations).\nAn image registration/stitching algorithm to create the birdâ€™s eye view. Homography is estimated between the pitch keypoints and the reference coornidates of the pitch, using OpenCVâ€™s findHomography. The pitch in the footage is then warped to a top-down view using perspectiveTransform.\nPlayer re-identification models (e.g.Â MOTIP). When the footage is cut or camera is changed to a different angle, the player IDs are lost. We need to re-identify the players in order to connect the player tracks across different clips. I did not find the implemetation in this POC."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "href": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "title": "Computer vision for soccer games",
    "section": "Does it work on other soccer videos?",
    "text": "Does it work on other soccer videos?\nI picked a random soccer game clip, and the result is not as good as the example video. The camera moved faster, zooming in to a partial view of the pitch near the goal post. This posed challenges to the keypoint detection model, and the player tracking model. Some players were not detected due to motion blur and occlusion. Key points of the pitch were not detected in some frames, and the algorithm was not able to create a birdâ€™s eye view for those frames. The result is shown below:\nVideo\nRegardless, it is a great starting point to build a more reliable system for soccer game analytics. For fun, I also tried it on a very challenging video with a couple of professional players against 100 pupils. Interestingly, the algorithm was able to detect most the players, and create a birdâ€™s eye view, as long as a large portion of the pitch is visible:"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "href": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "title": "Computer vision for soccer games",
    "section": "Makeing it better: more accurate player detection and tracking",
    "text": "Makeing it better: more accurate player detection and tracking\n\nTransformers for object tracking\nAccurate tracking requires attending to relationships between detected players on different frames, their roles, jersey colors etc. Transformers architecture is well suited for this task.\n\nGlobal tracking transformer\nGlobal tracking transformers takes a video as input, and predict object tracks in an end-to-end fashion. It was trained on LVIS and COCO, capable of tracking 1000+ categories of objects. Below is the result for tracking persons and the ball. It also identified the billboards though they are not directly useful for our purpose here. This is the tracking result overlayed on the input video:\nVideo\nComparing YOLOv8 and Global Tracking Transformer, the latter seems more accurate.\n\n\n\n\n\n\n\n\n\nYOLOv8\n\n\n\n\n\n\n\nGlobal Tracking Transformer\n\n\n\n\n\n\n\n\nVision-language models, open vocabulary and zero-shot object detection\nWith recent advances in vision-language models, we can leverage the visual knowledge in pretrained large models. How well do they work in detecting players?\n\nGrounding DINO\nThis model has a DINO transformer backbone and produced by grounded pre-training. You can prompt the model with a sentence or a phrase, and it will highlight the corresponding region in the image. Below is the architecture of Grounding DINO:\n\n\n\nArchitecture of Grounding DINO\n\n\n\n\n\nWith one prompt, Grounding DINO was able to detect players but had a hard time distinguishing the goal keeper from other players.\n\n\n\n\nYOLO World\nThis model is an open-vocabulary object detection model. It can detect objects that are not in the training set, and can be used for zero-shot object detection. You can prompt it with a list of words, such as â€œplayer, ball, goal keeperâ€.\nCompared to Grounding DINO, YOLO World seems less accurate and misses some players when they overlap.\n\n\n\nYOLO-World-XL player detection result.\n\n\nThese are just two examples of recent models."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#datasets",
    "href": "posts/20240915-soccer-tracking/index.html#datasets",
    "title": "Computer vision for soccer games",
    "section": "Datasets",
    "text": "Datasets\nYou may need to fine tune the models on more soccer game videos with annotations. Here are some datasets that can be useful:\nSoccerNet is a large-scale dataset for soccer analysis. It contains 550 complete broadcast soccer games and 12 single camera games taken from the major European leagues. It supports various vision tasks such as action spotting, camera calibration, player re-identification and tracking.\nThis Kaggle dataset also contains soccer game videos from Premier League showdowns to FIFA World Cup classics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "href": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "title": "Computer vision for soccer games",
    "section": "Business use cases",
    "text": "Business use cases\nBoardly, here are some areas where computer vision can be used in soccer analytics:\n\nPerformance Analysis: By tracking player movement, positioning, and interactions, teams can better understand individual and team performance, making it easier to identify strengths and areas for improvement.\nTactical Insights: Coaches can analyze formations, pressing patterns, and set-pieces to gain a competitive edge, adjusting their game plans based on data.\n** Player Development**: Young athletes can leverage computer vision technology to receive feedback on their performance and improve their skills over time.\nFan Engagement: Computer vision can create engaging, immersive content for fans, such as 3D replays or interactive match highlights, bringing them closer to the action.\n\nHere is a very incomplete list of companies and use cases:\n\nVeo: AI-powered cameras for automatic sports recording, tracking game action, and AI-tagged highlights for analysis.\nTraceup: Video captures that allow tracking players individually, creating personalized highlight reels that parents, players, and coaches can view from various angles.\nTrack160: Skeleton tracking, identifying and monitoring the movement of players and the ball, tagging and analyzing events in a match, physical and tactical breakdowns of player performances.\nNY Times created 3D stories that allow fans to experience game-defining moments from multiple angles and gain deeper insights into player positioning, ball movement, and tactics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#conclusion",
    "href": "posts/20240915-soccer-tracking/index.html#conclusion",
    "title": "Computer vision for soccer games",
    "section": "Conclusion",
    "text": "Conclusion\nThis is just a start. I am glad to see computer vision applied to everyday life, and hope this post spark some ideas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on practical AI, engineering and life",
    "section": "",
    "text": "Agentic Causal Inference\n\n\n\n\n\n\nai\n\n\nllm\n\n\nagentic\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMiniSora: Learnings from training a Minimal Video Generation Model (Part 1)\n\n\n\n\n\n\ngenerative ai\n\n\nvideo generation\n\n\ncost efficient training\n\n\nscaling laws\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nComputer vision for soccer games\n\n\n\n\n\n\ncomputer vision\n\n\nai\n\n\nsports\n\n\nsoccer\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Consumerism to Sustainability: AIâ€™s Role in Shaping the Future of Economic Growth\n\n\n\n\n\n\neconomics\n\n\nai\n\n\nenvironment\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying machine learning apps to Google Cloud Run with Github actions\n\n\n\n\n\n\ncode\n\n\nmlops\n\n\nGCP\n\n\ncloud\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nZZ Si\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Realâ€‘time textâ€‘toâ€‘video once felt scienceâ€‘fictional. In 2025 itâ€™s weekendâ€‘project territory, thanks to a surge of research that shrinks models, unlocks causal sampling, and weaponises clever postâ€‘processing. This post unpacks what changed, who shipped it, and how you can ride the wave on a single GPU.\n\n\n\n\n\nTL;DR:Â Causal, fewâ€‘step diffusion, sparse attention and clever frameâ€‘interpolation now deliver 30â€“100Â FPS pipelines that fit on consumer GPUs.\nWho benefits:Â Game studios, VTubers, product marketers, eâ€‘learning creators, AR tool buildersâ€”and anyone tired of render bars.\nOpenâ€‘source wins:Â Almost every model below ships under Apacheâ€‘2.0Â orÂ MIT, keeping vendor lockâ€‘in at bay.\n\n\n\n\n\n\n\n\nAngle\nPainâ€‘point\n2025 solution\n\n\n\n\nUser experience\nAnything underÂ 24Â FPS breaks immersion; VRÂ &Â AR needÂ 60â€“90Â FPS latency budgets\nCausal sampling + VFI reachÂ 60Â FPS on laptops\n\n\nIteration velocity\nWaiting minutes per draft kills creative flow; agencies need dozens of variants per brief\n10â€“20â€¯Ã— faster inference â†’ sameâ€‘day storyboards\n\n\nDeployment cost\nCloud diffusers atÂ $3â€¯/â€¯min blow indie budgets; edge devices demandÂ &lt;80Â W power draw\nINT8 + SSM backbones slash FLOPs; singleÂ 4090 â‰ˆÂ $0.10â€¯/â€¯min\n\n\nNew UX surfaces\nLive avatars & reactive ads require millisecond feedback\nStreamingÂ DiT & LLIA hitÂ &lt;200Â ms endâ€‘toâ€‘end\n\n\nSustainability\n10Â Ã— FLOPs reductions â‡’ 10Â Ã— fewerâ€¯kWh &Â COâ‚‚\nSparse attentionÂ + consistency distillation lead the race\n\n\n\n\nIndustry note:Â TikTok internal metrics show viewers bail afterÂ 1.2Â s of blank canvas; efficient generation keeps them hooked.\n\n\n\n\n\n\n\n\nPaperÂ (2025)\nCore trick\nReported speed\nClip quality notes\n\n\n\n\nAAPT â€“ Autoregressive Adversarial Postâ€‘Training\nConverts a bidirectional videoâ€‘DiT into a 1â€‘step causal student\n24Â FPSÂ @Â 736Ã—416 on singleÂ H100\nFVD withinÂ 3â€¯% of teacher DiT\n\n\nVMoBA â€“ Video Mixtureâ€‘ofâ€‘Block Attention\n1Dâ€‘2Dâ€‘3D sparse attention; selects motionâ€‘critical windows only\n1080p inference, â‰ˆ1.5â€¯Ã— latency drop\n0.97Â LPIPS vs.Â full attention\n\n\nGoâ€‘withâ€‘theâ€‘Flow\nWarps diffusion noise via online opticalâ€‘flow fields\n512p live demos on laptop GPUs\nUserâ€‘controllable motion\n\n\nStreamDiT\nFlowâ€‘matching + buffer distillation; streams latent frames\n16Â FPS generation, nearâ€‘realâ€‘time on RTXÂ 4070\nDesigned for avatars & games\n\n\nTrackDiffusion\nTrajectoryâ€‘conditioned DiT; user draws BÃ©zier path & duration\n1440Ã—810 plentyâ€‘motion shots\nGreat for droneâ€‘style dolly moves\n\n\n\nEngineering patterns to steal\n\nCausalisationÂ â€” stop predicting all frames; predict nextâ€‘frame only with a KVâ€‘cache. (AAPT)\nStructured sparsityÂ â€” MoBA & shiftedâ€‘window SSMs hideÂ 70â€¯% of tokens yet loseÂ &lt;5â€¯% PSNR. (VMoBA)\nConsistency distillationÂ â€” 2â€“4 diffusion steps rival GAN speed after INT8 quantisation. (LLIA)\nBuffer reuseÂ â€” StreamDiT overlaps GPU streams (decodeÂ + encode), shavingÂ 20Â ms per frame.\n\n\n\n\n\n\n\n\nModel\nInnovation\nFPS / latency\nDeployment sweet spot\n\n\n\n\nMirrorMe\nAudio adapter + progressive curriculum on LTX backbone\nâ‰ˆ30Â FPS\nYouTube live streams\n\n\nLLIA\nConsistencyâ€‘distilled UNet, INT8, pipeline parallel\n78Â FPSÂ @Â 384Â², &lt;â€¯200Â ms E2E\nTwitch VTubers, Zoom filters\n\n\nSyncTalk++\n3â€‘stage controller: lip, head, stabilizer + Gaussian renderer\n101Â FPSÂ @Â 512p\nCorporate webinars\n\n\nEchoMimicÂ V3\n1.3â€¯B unified humanâ€‘animation model; crossâ€‘modal decoupled attention\n45Â FPSÂ @Â 512Â², &lt;220Â ms\nAR glasses, signage\n\n\nARIG\nConversational stateâ€‘aware head motion; autoregressive\n30Â FPS, 180Â ms\nMultiâ€‘speaker panels\n\n\n\n\n\n\n\n\nLCâ€‘Mamba (CVPRâ€¯25)Â â€” Linearâ€‘time stateâ€‘space backbone; 35Â FPSÂ @Â 720p on a 4090.\nTLBâ€‘VFI (Julâ€¯25)Â â€” Latent Brownianâ€‘bridge diffusion; fills irregular temporal gaps.\nBiMâ€‘VFIÂ â€” Bidirectional motionâ€‘field model; excels at nonâ€‘uniform acceleration.\nRIFEÂ 4.6 / IFRNetâ€‘HDÂ â€” Fastest realâ€‘time baselines with NCNN/ONNX ports.\n\nğŸ‘‰Â Rule of thumb: generate at 15Â FPS â†’ interpolateÂ Ã—2â€“4 with the above for cinemaâ€‘smooth output.\n\n\n\n\n\n\n\nModel\nGenerates â€¦\nNative speedâ€‘up\nLink\n\n\n\n\nMagicTime\nSunsets, plant growth, urban nightâ€‘scape\n6â€“12â€¯Ã—\nGitHub repo\n\n\nLatte\nSchedulerâ€‘skippable Latent DiT\n4â€“10â€¯Ã—\nHF weights\n\n\nÎ”â€‘Diffusion\nDemoâ€‘action replay in any scene\nUserâ€‘defined\nGitHub repo\n\n\nMAVIN\nMultiâ€‘move montage & infill\n10â€“20Â s sequences\nGitHub repo\n\n\nTLBâ€‘VFI\nGapâ€‘aware interpolation layer\n16â€“32â€¯Ã— with others\nGitHub repo\n\n\n\n\n\n\n\n\n\n\n2025 tool\nWhere it sits\nEffect\nLink\n\n\n\n\nConsistory\nMidâ€‘generation UNet patch\n6â€¯Ã— lower ID drift\nGitHub\n\n\nStoryCrafter\nPromptâ€‘time region attention\nFineâ€‘grained style control\nGitHub\n\n\nAuditÂ &Â Repair\nPostâ€‘hoc LLM loop\nAutoâ€‘fixes drift\nGitHub\n\n\nStoryMakerÂ v2\nPersonalisation LoRA\nLocks faceÂ + outfit\nGitHub\n\n\nOneâ€‘Promptâ€‘Oneâ€‘Story\nTrainingâ€‘free megaâ€‘prompt\nRapid concept art\nGitHub\n\n\n\n\nMetric watch:Â ViStoryBench now tests semanticÂ + temporalÂ + stylistic coherenceâ€”expect papers to report it by default.\n\n\n\n\n\n\nComfyUIâ€‘StoryDiffÂ â€” Dragâ€‘andâ€‘drop pipelines for Consistory, EchoMimicÂ & LCâ€‘Mamba.\nVideoCrafter2Â â€” HuggingÂ Face toolkit wrapping AAPT, StreamDiT, MagicTime.\nOpenâ€‘Sora sprintÂ â€” Reâ€‘creating proprietary Sora demos; checkpoints atÂ 512Â²,Â 12Â FPS.\nVulkanâ€‘RIFEÂ and WebGPUâ€‘LCâ€‘Mambaâ€”browserâ€‘side interpolation.\n\n\n\n\n\n\n\n\nScenario\nGPU budget\nLatency target\nStack\n\n\n\n\nVTuber streaming\nRTXÂ 4070\n&lt;200Â ms\nLLIA â†’ SyncTalk++ â†’ RIFEÂ Ã—2\n\n\nProduct demoÂ 1080p/30\ndualÂ 4090\n&lt;2Â s\nAAPT â†’ VMoBA â†’ H.265 encode â†’ LCâ€‘Mamba polish\n\n\nSocialâ€‘media hyperâ€‘lapse\nMacBookÂ M3Â Pro\noffline\nMagicTimeÂ @512Â² â†’ LCâ€‘Mamba Ã—2\n\n\nPreâ€‘viz animatic (20 panels)\ncloudÂ A100\n&lt;30Â s\nStoryMakerÂ v2 â†’ Consistory â†’ AuditÂ &Â Repair â†’ LCâ€‘Mamba\n\n\nAR glasses companion\nmobileÂ GPU\n10â€“30Â FPS\nStreamDiT distilled â†’ VFIMamba\n\n\n\n\n\n\n\n\nExtremeâ€‘resolution (&gt;4â€¯K) causal generationâ€”open thread on GitHubÂ issueÂ #42.\nUnified multiâ€‘modal controlâ€”prototype spec discussed in the PromptFusion RFC.\nEnergyâ€‘aware schedulers for laptops & phonesâ€”track progress in the Efficientâ€‘Diffusionâ€‘WG.\nRobustness metricsâ€”draft of FPSâ€‘normedÂ FVD at fvd-fps repo.\nWebGPU kernelsâ€”follow efforts in wgpuâ€‘diffusion.\n\n\n\n\n\n\nCausal, fewâ€‘step diffusion + sparse attention is the unlock for realâ€‘time generation.\nVFI is now a firstâ€‘class citizenâ€”treat it as part of generation, not post.\nConsistent storytelling is productionâ€‘ready via LoRAs & prompt hacks.\nOpenâ€‘source keeps pace with commercial demosâ€”weights under permissive licences abound.\nHardware democratisationâ€”RTXÂ 4070 laptops now rival 2023 cloud nodes.\nBenchmarks matureâ€”ViStoryBench, FPSâ€‘FVD.\nCreative iteration speed winsâ€”faster render loops reshape storyboarding and marketing.\n\n\nIf 2024 was the year of breathtaking yet sluggish video diffusion, 2025 lets creators hit playâ€”and watch results materialise in realâ€‘time with open tools.\n\n\nCompiled JulyÂ 26Â 2025 â€”Â All links point to public GitHub, HuggingÂ Face, or arXiv unless noted otherwise."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#a-twominute-recap-skip-if-you-love-details",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#a-twominute-recap-skip-if-you-love-details",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "TL;DR:Â Causal, fewâ€‘step diffusion, sparse attention and clever frameâ€‘interpolation now deliver 30â€“100Â FPS pipelines that fit on consumer GPUs.\nWho benefits:Â Game studios, VTubers, product marketers, eâ€‘learning creators, AR tool buildersâ€”and anyone tired of render bars.\nOpenâ€‘source wins:Â Almost every model below ships under Apacheâ€‘2.0Â orÂ MIT, keeping vendor lockâ€‘in at bay."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#why-efficient-video-matters-now-more-than-ever",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#why-efficient-video-matters-now-more-than-ever",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Angle\nPainâ€‘point\n2025 solution\n\n\n\n\nUser experience\nAnything underÂ 24Â FPS breaks immersion; VRÂ &Â AR needÂ 60â€“90Â FPS latency budgets\nCausal sampling + VFI reachÂ 60Â FPS on laptops\n\n\nIteration velocity\nWaiting minutes per draft kills creative flow; agencies need dozens of variants per brief\n10â€“20â€¯Ã— faster inference â†’ sameâ€‘day storyboards\n\n\nDeployment cost\nCloud diffusers atÂ $3â€¯/â€¯min blow indie budgets; edge devices demandÂ &lt;80Â W power draw\nINT8 + SSM backbones slash FLOPs; singleÂ 4090 â‰ˆÂ $0.10â€¯/â€¯min\n\n\nNew UX surfaces\nLive avatars & reactive ads require millisecond feedback\nStreamingÂ DiT & LLIA hitÂ &lt;200Â ms endâ€‘toâ€‘end\n\n\nSustainability\n10Â Ã— FLOPs reductions â‡’ 10Â Ã— fewerâ€¯kWh &Â COâ‚‚\nSparse attentionÂ + consistency distillation lead the race\n\n\n\n\nIndustry note:Â TikTok internal metrics show viewers bail afterÂ 1.2Â s of blank canvas; efficient generation keeps them hooked."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#realtime-generators-june-july-2025-breakthroughs",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#realtime-generators-june-july-2025-breakthroughs",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "PaperÂ (2025)\nCore trick\nReported speed\nClip quality notes\n\n\n\n\nAAPT â€“ Autoregressive Adversarial Postâ€‘Training\nConverts a bidirectional videoâ€‘DiT into a 1â€‘step causal student\n24Â FPSÂ @Â 736Ã—416 on singleÂ H100\nFVD withinÂ 3â€¯% of teacher DiT\n\n\nVMoBA â€“ Video Mixtureâ€‘ofâ€‘Block Attention\n1Dâ€‘2Dâ€‘3D sparse attention; selects motionâ€‘critical windows only\n1080p inference, â‰ˆ1.5â€¯Ã— latency drop\n0.97Â LPIPS vs.Â full attention\n\n\nGoâ€‘withâ€‘theâ€‘Flow\nWarps diffusion noise via online opticalâ€‘flow fields\n512p live demos on laptop GPUs\nUserâ€‘controllable motion\n\n\nStreamDiT\nFlowâ€‘matching + buffer distillation; streams latent frames\n16Â FPS generation, nearâ€‘realâ€‘time on RTXÂ 4070\nDesigned for avatars & games\n\n\nTrackDiffusion\nTrajectoryâ€‘conditioned DiT; user draws BÃ©zier path & duration\n1440Ã—810 plentyâ€‘motion shots\nGreat for droneâ€‘style dolly moves\n\n\n\nEngineering patterns to steal\n\nCausalisationÂ â€” stop predicting all frames; predict nextâ€‘frame only with a KVâ€‘cache. (AAPT)\nStructured sparsityÂ â€” MoBA & shiftedâ€‘window SSMs hideÂ 70â€¯% of tokens yet loseÂ &lt;5â€¯% PSNR. (VMoBA)\nConsistency distillationÂ â€” 2â€“4 diffusion steps rival GAN speed after INT8 quantisation. (LLIA)\nBuffer reuseÂ â€” StreamDiT overlaps GPU streams (decodeÂ + encode), shavingÂ 20Â ms per frame."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#avatar-animation-lipsync-at-production-latency",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#avatar-animation-lipsync-at-production-latency",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Model\nInnovation\nFPS / latency\nDeployment sweet spot\n\n\n\n\nMirrorMe\nAudio adapter + progressive curriculum on LTX backbone\nâ‰ˆ30Â FPS\nYouTube live streams\n\n\nLLIA\nConsistencyâ€‘distilled UNet, INT8, pipeline parallel\n78Â FPSÂ @Â 384Â², &lt;â€¯200Â ms E2E\nTwitch VTubers, Zoom filters\n\n\nSyncTalk++\n3â€‘stage controller: lip, head, stabilizer + Gaussian renderer\n101Â FPSÂ @Â 512p\nCorporate webinars\n\n\nEchoMimicÂ V3\n1.3â€¯B unified humanâ€‘animation model; crossâ€‘modal decoupled attention\n45Â FPSÂ @Â 512Â², &lt;220Â ms\nAR glasses, signage\n\n\nARIG\nConversational stateâ€‘aware head motion; autoregressive\n30Â FPS, 180Â ms\nMultiâ€‘speaker panels"
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#frameinterpolation-as-an-efficiency-amplifier",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#frameinterpolation-as-an-efficiency-amplifier",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "LCâ€‘Mamba (CVPRâ€¯25)Â â€” Linearâ€‘time stateâ€‘space backbone; 35Â FPSÂ @Â 720p on a 4090.\nTLBâ€‘VFI (Julâ€¯25)Â â€” Latent Brownianâ€‘bridge diffusion; fills irregular temporal gaps.\nBiMâ€‘VFIÂ â€” Bidirectional motionâ€‘field model; excels at nonâ€‘uniform acceleration.\nRIFEÂ 4.6 / IFRNetâ€‘HDÂ â€” Fastest realâ€‘time baselines with NCNN/ONNX ports.\n\nğŸ‘‰Â Rule of thumb: generate at 15Â FPS â†’ interpolateÂ Ã—2â€“4 with the above for cinemaâ€‘smooth output."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#timelapse-acceleratedaction-generation",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#timelapse-acceleratedaction-generation",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Model\nGenerates â€¦\nNative speedâ€‘up\nLink\n\n\n\n\nMagicTime\nSunsets, plant growth, urban nightâ€‘scape\n6â€“12â€¯Ã—\nGitHub repo\n\n\nLatte\nSchedulerâ€‘skippable Latent DiT\n4â€“10â€¯Ã—\nHF weights\n\n\nÎ”â€‘Diffusion\nDemoâ€‘action replay in any scene\nUserâ€‘defined\nGitHub repo\n\n\nMAVIN\nMultiâ€‘move montage & infill\n10â€“20Â s sequences\nGitHub repo\n\n\nTLBâ€‘VFI\nGapâ€‘aware interpolation layer\n16â€“32â€¯Ã— with others\nGitHub repo"
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#storyboard-keyframe-consistency",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#storyboard-keyframe-consistency",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "2025 tool\nWhere it sits\nEffect\nLink\n\n\n\n\nConsistory\nMidâ€‘generation UNet patch\n6â€¯Ã— lower ID drift\nGitHub\n\n\nStoryCrafter\nPromptâ€‘time region attention\nFineâ€‘grained style control\nGitHub\n\n\nAuditÂ &Â Repair\nPostâ€‘hoc LLM loop\nAutoâ€‘fixes drift\nGitHub\n\n\nStoryMakerÂ v2\nPersonalisation LoRA\nLocks faceÂ + outfit\nGitHub\n\n\nOneâ€‘Promptâ€‘Oneâ€‘Story\nTrainingâ€‘free megaâ€‘prompt\nRapid concept art\nGitHub\n\n\n\n\nMetric watch:Â ViStoryBench now tests semanticÂ + temporalÂ + stylistic coherenceâ€”expect papers to report it by default."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#tooling-ecosystem-libraries",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#tooling-ecosystem-libraries",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "ComfyUIâ€‘StoryDiffÂ â€” Dragâ€‘andâ€‘drop pipelines for Consistory, EchoMimicÂ & LCâ€‘Mamba.\nVideoCrafter2Â â€” HuggingÂ Face toolkit wrapping AAPT, StreamDiT, MagicTime.\nOpenâ€‘Sora sprintÂ â€” Reâ€‘creating proprietary Sora demos; checkpoints atÂ 512Â²,Â 12Â FPS.\nVulkanâ€‘RIFEÂ and WebGPUâ€‘LCâ€‘Mambaâ€”browserâ€‘side interpolation."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#choosing-the-right-toolbox-expanded-cheatsheet",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#choosing-the-right-toolbox-expanded-cheatsheet",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Scenario\nGPU budget\nLatency target\nStack\n\n\n\n\nVTuber streaming\nRTXÂ 4070\n&lt;200Â ms\nLLIA â†’ SyncTalk++ â†’ RIFEÂ Ã—2\n\n\nProduct demoÂ 1080p/30\ndualÂ 4090\n&lt;2Â s\nAAPT â†’ VMoBA â†’ H.265 encode â†’ LCâ€‘Mamba polish\n\n\nSocialâ€‘media hyperâ€‘lapse\nMacBookÂ M3Â Pro\noffline\nMagicTimeÂ @512Â² â†’ LCâ€‘Mamba Ã—2\n\n\nPreâ€‘viz animatic (20 panels)\ncloudÂ A100\n&lt;30Â s\nStoryMakerÂ v2 â†’ Consistory â†’ AuditÂ &Â Repair â†’ LCâ€‘Mamba\n\n\nAR glasses companion\nmobileÂ GPU\n10â€“30Â FPS\nStreamDiT distilled â†’ VFIMamba"
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#open-challenges-research-threads",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#open-challenges-research-threads",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Extremeâ€‘resolution (&gt;4â€¯K) causal generationâ€”open thread on GitHubÂ issueÂ #42.\nUnified multiâ€‘modal controlâ€”prototype spec discussed in the PromptFusion RFC.\nEnergyâ€‘aware schedulers for laptops & phonesâ€”track progress in the Efficientâ€‘Diffusionâ€‘WG.\nRobustness metricsâ€”draft of FPSâ€‘normedÂ FVD at fvd-fps repo.\nWebGPU kernelsâ€”follow efforts in wgpuâ€‘diffusion."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#key-takeaways",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#key-takeaways",
    "title": "Cutting Latency, Keeping CreativityÂ â€” The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Causal, fewâ€‘step diffusion + sparse attention is the unlock for realâ€‘time generation.\nVFI is now a firstâ€‘class citizenâ€”treat it as part of generation, not post.\nConsistent storytelling is productionâ€‘ready via LoRAs & prompt hacks.\nOpenâ€‘source keeps pace with commercial demosâ€”weights under permissive licences abound.\nHardware democratisationâ€”RTXÂ 4070 laptops now rival 2023 cloud nodes.\nBenchmarks matureâ€”ViStoryBench, FPSâ€‘FVD.\nCreative iteration speed winsâ€”faster render loops reshape storyboarding and marketing.\n\n\nIf 2024 was the year of breathtaking yet sluggish video diffusion, 2025 lets creators hit playâ€”and watch results materialise in realâ€‘time with open tools.\n\n\nCompiled JulyÂ 26Â 2025 â€”Â All links point to public GitHub, HuggingÂ Face, or arXiv unless noted otherwise."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html",
    "href": "drafts/20250623-some-eeg-demos/index.html",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "WrittenÂ Julyâ€¯2025â€”feel free to remix or quote.\n\n\n\nElectroâ€‘encephalographyâ€¯(EEG) lets us peek at the brainâ€™s electrical chatter millisecondâ€‘byâ€‘millisecond, wirelessly,Â & painâ€‘free.â€¯In the last two years, consumer headsets and openâ€‘source stacks have matured enough that hobbyists can turn those signals into live, personalised experiencesâ€”from playlists that match your vibe to lighting that mellows as you unwind.\n\n\n\n\n\n\n\nWhat EEG can do in 2025\nTypical accuracy\nLatency\n\n\n\n\nDetect happyâ€‘vsâ€‘sad OR calmâ€‘vsâ€‘alert\n75â€“85â€¯%\n&lt;â€¯1â€¯s\n\n\nFourâ€‘quadrant mood (valence Ã— arousal)\n70â€“78â€¯%\nâ‰ˆâ€¯1â€¯s\n\n\nFlag drowsiness in drivers\n&gt;â€¯90â€¯%\n300â€¯ms\n\n\nTrigger SSVEP commands (4â€“10 targets)\n95â€¯%\n200â€¯ms\n\n\nControl 2â€‘D cursor via motor imagery\n70â€“80â€¯%\n250â€¯ms\n\n\n\nTakeâ€‘away: Coarse mental states are demoâ€‘ready, while nuanced emotions still need cleaner signals & bigger datasets.\n\n\n\n\n\n\n\nHeadsets: Museâ€‘S, Flowtime (4â€“7 dry electrodes) or OpenBCI Cyton (8â€“16 wet electrodes).\nStream: BrainFlowÂ â†’Â LabÂ StreamingÂ LayerÂ (LSL) at ~250â€¯Hz, keeping jitter &lt;â€¯5â€¯ms.\n\n\n\n\ngraph TD\nA(EEG stream) --&gt; B(Filter 0.5â€“45â€¯Hz)\nB --&gt; C(Window 1â€¯s, 50â€¯% overlap)\nC --&gt; D(Feature: log bandâ€‘power & differential entropy)\nD --&gt; E(LSTM / CNN classifier)\nE --&gt; F(Mood label: Happyâ€‘Calm etc.)\nF --&gt; G([Spotify](https://developer.spotify.com/documentation/web-api) / [AppleÂ Music](https://developer.apple.com/documentation/applemusicapi) API)\nG --&gt; H(Swap playlist)\n\n15â€‘minute live demo: Strap on an OpenBCI board, load a pretrained DEAP CNN, and watch songs shift the moment you smile, frown, or breathe deeply.\n\n\n\n\n\nValence clues: greater leftâ€‘frontal alpha suppression when happy.\nArousal clues: beta & lowâ€‘gamma surge when alert or anxious.\nFour playlists (e.g., Chill, Happy, Pump, Melancholy) map neatly onto the valenceâ€‘arousal grid.\n\n\n\n\n\n\n\n\n\nDomain\nLive adaptation\nStack to try\n\n\n\n\nGaming & VR\nEnemy speed, soundtrack intensity, difficulty curve\nUnity + LSL + NeuroPype\n\n\nDriver Safety\nSeat vibration & HUD alerts during microsleep\nEarâ€‘EEG + TensorFlowÂ Lite\n\n\nRehab Robots\nExoskeleton mirrors imagined hand/arm motion\nOpenBCI + BCI2000\n\n\nSmart Lighting\nColour temperature follows arousal\nRaspberryÂ Pi + PhilipsÂ Hue API\n\n\nLED Art Walls\nVisuals morph to crowd synchrony\nTouchDesigner + BrainFlow\n\n\nAdaptive Learning\nQuiz pops when attention dips\nOpenViBE + Moodle plugin\n\n\nNeuromarketing\nSwap ad cut when attention drops\niMotions + BrainFlow SDK\n\n\nMindfulness VR\nScene changes with rising alpha\nUnrealÂ EngineÂ 5 + LSL\n\n\nSilent Communication\nEarly decoding of heard phrases\nPyTorch wav2vec on EEG\n\n\nDream Interfaces\nLucid dream YES/NO via EEG & eyeÂ codes\nREMspace protocol + OpenBCI\n\n\n\n\n\n\n\n\n\n\nChallenge\nWhy it hurts\nMitigation\n\n\n\n\nNoise & motion artefacts\nHair, blinks, jaw tension distort ÂµV signals\nBetter electrode gel; ICA/ASR filters; headband stabilisers\n\n\nPerâ€‘user calibration\nCrossâ€‘subject models drop ~15â€¯pp\n2â€‘min online fineâ€‘tune; metaâ€‘learning\n\n\nBitâ€‘rate ceiling\nNonâ€‘invasive EEG â‰ˆâ€¯40â€¯bitsâ€¯minâ»Â¹\nCombine with EMG, eyeâ€‘tracking, heartâ€‘rate\n\n\nPrivacy & ethics\nBrain data can hint at health or intent\nTransparent logging, local processing, consent dialogs\n\n\n\n\n\n\n\n\nStart binary: happy vsâ€¯sad or relaxed vsâ€¯alert; complexity later.\nUse turnkey stacks: BrainFlow â†’ LSL â†’ OpenViBE/BCI2000 gets you running dayâ€‘one.\nClose the loop fast: immediate visual or audio feedback accelerates user learning and model drift handling.\nBlend sensors: patch in webcam, PPG, or IMU data to lift accuracy when EEG falters.\nShip privacy by design: default to edge inference, let users delete logs.\n\n\n\n\n\n\nPortable dryâ€‘electrode arrays hitting &lt;â€¯5â€¯kÎ© impedance.\nPhysicsâ€‘informed neural nets (e.g., DeepSIF) shrinking sourceâ€‘imaging latency below 50â€¯ms.\nFederated EEG learningâ€”models train across headsets without rawâ€‘signal sharing.\nRealâ€‘time GAN music generation conditioned directly on EEG, skipping playlists altogether.\n\n\n\n\n\n\nDatasets: DEAP â€¢ DREAMER â€¢ MAHNOBâ€‘HCI\nCommunity: OpenBCI Forum & Slack for weekend build guides.\nPipelines: BCI2000 â€¢ OpenViBE â€¢ BCILAB for MATLAB users.\nStreaming: BrainFlow SDK â€¢ LSLÂ Explorer to inspect network streams.\n\n\nCurious?Â With just a headband, a laptop, and a free afternoon, you can build an app that senses your mood and plays the perfect soundtrackâ€”or dims the lights and spawns gentler game levels. The neuroadaptive future is DIYâ€‘ready today."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#why-care-about-brainwaves",
    "href": "drafts/20250623-some-eeg-demos/index.html#why-care-about-brainwaves",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Electroâ€‘encephalographyâ€¯(EEG) lets us peek at the brainâ€™s electrical chatter millisecondâ€‘byâ€‘millisecond, wirelessly,Â & painâ€‘free.â€¯In the last two years, consumer headsets and openâ€‘source stacks have matured enough that hobbyists can turn those signals into live, personalised experiencesâ€”from playlists that match your vibe to lighting that mellows as you unwind."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#snapshot-of-todays-capabilities",
    "href": "drafts/20250623-some-eeg-demos/index.html#snapshot-of-todays-capabilities",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "What EEG can do in 2025\nTypical accuracy\nLatency\n\n\n\n\nDetect happyâ€‘vsâ€‘sad OR calmâ€‘vsâ€‘alert\n75â€“85â€¯%\n&lt;â€¯1â€¯s\n\n\nFourâ€‘quadrant mood (valence Ã— arousal)\n70â€“78â€¯%\nâ‰ˆâ€¯1â€¯s\n\n\nFlag drowsiness in drivers\n&gt;â€¯90â€¯%\n300â€¯ms\n\n\nTrigger SSVEP commands (4â€“10 targets)\n95â€¯%\n200â€¯ms\n\n\nControl 2â€‘D cursor via motor imagery\n70â€“80â€¯%\n250â€¯ms\n\n\n\nTakeâ€‘away: Coarse mental states are demoâ€‘ready, while nuanced emotions still need cleaner signals & bigger datasets."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#how-the-moodtomusic-trick-works",
    "href": "drafts/20250623-some-eeg-demos/index.html#how-the-moodtomusic-trick-works",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Headsets: Museâ€‘S, Flowtime (4â€“7 dry electrodes) or OpenBCI Cyton (8â€“16 wet electrodes).\nStream: BrainFlowÂ â†’Â LabÂ StreamingÂ LayerÂ (LSL) at ~250â€¯Hz, keeping jitter &lt;â€¯5â€¯ms.\n\n\n\n\ngraph TD\nA(EEG stream) --&gt; B(Filter 0.5â€“45â€¯Hz)\nB --&gt; C(Window 1â€¯s, 50â€¯% overlap)\nC --&gt; D(Feature: log bandâ€‘power & differential entropy)\nD --&gt; E(LSTM / CNN classifier)\nE --&gt; F(Mood label: Happyâ€‘Calm etc.)\nF --&gt; G([Spotify](https://developer.spotify.com/documentation/web-api) / [AppleÂ Music](https://developer.apple.com/documentation/applemusicapi) API)\nG --&gt; H(Swap playlist)\n\n15â€‘minute live demo: Strap on an OpenBCI board, load a pretrained DEAP CNN, and watch songs shift the moment you smile, frown, or breathe deeply.\n\n\n\n\n\nValence clues: greater leftâ€‘frontal alpha suppression when happy.\nArousal clues: beta & lowâ€‘gamma surge when alert or anxious.\nFour playlists (e.g., Chill, Happy, Pump, Melancholy) map neatly onto the valenceâ€‘arousal grid."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#ten-eyecatching-eeg-hacks-all-opensource-friendly",
    "href": "drafts/20250623-some-eeg-demos/index.html#ten-eyecatching-eeg-hacks-all-opensource-friendly",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Domain\nLive adaptation\nStack to try\n\n\n\n\nGaming & VR\nEnemy speed, soundtrack intensity, difficulty curve\nUnity + LSL + NeuroPype\n\n\nDriver Safety\nSeat vibration & HUD alerts during microsleep\nEarâ€‘EEG + TensorFlowÂ Lite\n\n\nRehab Robots\nExoskeleton mirrors imagined hand/arm motion\nOpenBCI + BCI2000\n\n\nSmart Lighting\nColour temperature follows arousal\nRaspberryÂ Pi + PhilipsÂ Hue API\n\n\nLED Art Walls\nVisuals morph to crowd synchrony\nTouchDesigner + BrainFlow\n\n\nAdaptive Learning\nQuiz pops when attention dips\nOpenViBE + Moodle plugin\n\n\nNeuromarketing\nSwap ad cut when attention drops\niMotions + BrainFlow SDK\n\n\nMindfulness VR\nScene changes with rising alpha\nUnrealÂ EngineÂ 5 + LSL\n\n\nSilent Communication\nEarly decoding of heard phrases\nPyTorch wav2vec on EEG\n\n\nDream Interfaces\nLucid dream YES/NO via EEG & eyeÂ codes\nREMspace protocol + OpenBCI"
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#roadblocks-mitigations",
    "href": "drafts/20250623-some-eeg-demos/index.html#roadblocks-mitigations",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Challenge\nWhy it hurts\nMitigation\n\n\n\n\nNoise & motion artefacts\nHair, blinks, jaw tension distort ÂµV signals\nBetter electrode gel; ICA/ASR filters; headband stabilisers\n\n\nPerâ€‘user calibration\nCrossâ€‘subject models drop ~15â€¯pp\n2â€‘min online fineâ€‘tune; metaâ€‘learning\n\n\nBitâ€‘rate ceiling\nNonâ€‘invasive EEG â‰ˆâ€¯40â€¯bitsâ€¯minâ»Â¹\nCombine with EMG, eyeâ€‘tracking, heartâ€‘rate\n\n\nPrivacy & ethics\nBrain data can hint at health or intent\nTransparent logging, local processing, consent dialogs"
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#builders-playbook-quickstart-tips",
    "href": "drafts/20250623-some-eeg-demos/index.html#builders-playbook-quickstart-tips",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Start binary: happy vsâ€¯sad or relaxed vsâ€¯alert; complexity later.\nUse turnkey stacks: BrainFlow â†’ LSL â†’ OpenViBE/BCI2000 gets you running dayâ€‘one.\nClose the loop fast: immediate visual or audio feedback accelerates user learning and model drift handling.\nBlend sensors: patch in webcam, PPG, or IMU data to lift accuracy when EEG falters.\nShip privacy by design: default to edge inference, let users delete logs."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#emerging-research-to-watch",
    "href": "drafts/20250623-some-eeg-demos/index.html#emerging-research-to-watch",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Portable dryâ€‘electrode arrays hitting &lt;â€¯5â€¯kÎ© impedance.\nPhysicsâ€‘informed neural nets (e.g., DeepSIF) shrinking sourceâ€‘imaging latency below 50â€¯ms.\nFederated EEG learningâ€”models train across headsets without rawâ€‘signal sharing.\nRealâ€‘time GAN music generation conditioned directly on EEG, skipping playlists altogether."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#opensource-launchpad",
    "href": "drafts/20250623-some-eeg-demos/index.html#opensource-launchpad",
    "title": "Realâ€‘Time EEG: From Moodâ€‘Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Datasets: DEAP â€¢ DREAMER â€¢ MAHNOBâ€‘HCI\nCommunity: OpenBCI Forum & Slack for weekend build guides.\nPipelines: BCI2000 â€¢ OpenViBE â€¢ BCILAB for MATLAB users.\nStreaming: BrainFlow SDK â€¢ LSLÂ Explorer to inspect network streams.\n\n\nCurious?Â With just a headband, a laptop, and a free afternoon, you can build an app that senses your mood and plays the perfect soundtrackâ€”or dims the lights and spawns gentler game levels. The neuroadaptive future is DIYâ€‘ready today."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html",
    "href": "drafts/20250625-sports-photos/index.html",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "Written by a soccerâ€‘parent, summerâ€‘camp volunteer, and ML engineer who canâ€™t resist automating things.\n\n\n\nPicture day at my childâ€™s weekâ€‘long sports camp used to end the same way: 4â€¯GB of anonymous JPEGs and a frantic chat threadâ€”â€œHas anyone found #23 in the blue headband?â€ Hours later we still hadnâ€™t located every athlete, much less chosen the keeper shots. Commercial services promise instant, faceâ€‘matched albums, but I wanted full control, zero recurring fees, and the fun of hacking on stateâ€‘ofâ€‘theâ€‘art vision models.\nOne rainy afternoon I realised the openâ€‘source ecosystem had quietly delivered everything I needed: worldâ€‘class detectors, OCR, trackers, even highlight scorersâ€”all installable with pip. Two weekends and several cups of coffee later, my bash script could ingest a full campâ€™s worth of photos, spit out private, fully tagged galleries, and autoâ€‘compile a hype reel before the kids even boarded the bus home.\nBelow is the playbook I now share with other camp parents and volunteer photographers. Everything runs on a single RTXâ€¯4060â€¯laptop (or a $0.65â€¯/â€¯h A10G spot instance) and costs $0 in licensing.\n\n\n\n\n\n\n\nTask\nModel / Library\nWhy It Rocks\n\n\n\n\nDetect athletes & balls\nultralytics/YOLOv8mÂ /Â rtâ€‘detr\nFast (&gt;80â€¯fps) and easy to fineâ€‘tune on your own jerseys and lighting\n\n\nCrop jersey region\nMetaÂ AIÂ SAMÂ +Â roboflow/sports Autodistill\nSegments overlapping players without handwritten masks\n\n\nRead jersey numbers\nPARSeq fineâ€‘tuned on SoccerNetÂ &Â HockeyJersey\nWorks on skewed, lowâ€‘contrast digits\n\n\nRecognize faces\nInsightFace (ArcFace)\nMITâ€‘licensed, robust on teenage faces with helmets or headbands\n\n\nTrack players across frames\nDeepSORTÂ +Â ReID head\nMaintains IDs so clips donâ€™t lose context\n\n\nScore highlights\nLighthouse (multiâ€‘modal)\nCombines crowd noise + frame entropy for â€œcheerâ€‘worthyâ€ ranking\n\n\nBeautify portraits\nGFPGANÂ +Â torchvision autoâ€‘augment\nRecovers details and tweaks lighting without looking overâ€‘filtered\n\n\nGenerate slideshows / animations\nMoviePyÂ +Â PillowÂ +Â audiocraft/mockedâ€‘jukebox\nFades, titles, royaltyâ€‘free beatsâ€”no Adobe fees\n\n\nColorâ€‘grade & stylise\nDiffEdit (StableÂ DiffusionÂ ControlNet)\nOptional artistic pass for posters or endâ€‘ofâ€‘season banners\n\n\nLabel & QA dataset\nmakesense.aiÂ /Â LabelÂ Studio\nClickâ€‘based annotation; exports COCO / YOLO formats\n\n\n\nAll weights live on HuggingÂ Face or GitHub; no API keys, no rate limits.\n\n\n\n\nflowchart LR\n  A[Raw JPG/MP4 Dump] --&gt; B[YOLOv8 Detection]\n  B --&gt; C[SAM Jersey ROI]\n  C --&gt; D[PARSeq OCR]\n  B --&gt; E[InsightFace Encode]\n  D --&gt; F{Jersey #?}\n  E --&gt; F\n  F --&gt; G[Assign Player ID]\\n(face âˆ§/âˆ¨ jersey)\n  A --&gt; H[Lighthouse Scoring]\n  G --&gt; I[XMP Tag / SQLite]\n  H --&gt; J[Clip & Reel Assembly]\n  I --&gt; K[PhotoPrism / Photonix Import]\n  J --&gt; K\n  K --&gt; L[Optional DiffEdit Stylise]\n\n\nimport supervision as sv, ultralytics\nmodel = ultralytics.YOLO(\"yolov8m.pt\")\ntracks = sv.track_with_deepsort(video, model, device=\"cuda\")\n\n\n\nfrom parseq.infer import Reader\nreader = Reader(\"weights/parseq_jersey.pth\")\ntext, conf = reader.read(crop)\nif conf &lt; 0.65:\n    text = \"UNK\"  # fallback to face only\n\n\n\nexiftool -overwrite_original \\\n  -XMP:Player=\"Jersey_${text}\" \\\n  -XMP:Confidence=\"${conf}\" frame_00423.jpg\n\n\n\nPhotoPrism detects XMP tags and spins up albums like â€¦/CampFalcons/23/. You can also try Photonix if you prefer a Django stack. Coaches or parents can stream the highlight reel via an unlisted link generated with ytâ€‘dlp or your favourite staticâ€‘hosting bucket.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHardware\nThroughput\nNotes\n\n\n\n\nRTXâ€¯4060 Laptop\nâ‰ˆ100â€¯img/s detection; full camp overnight\nQuiet enough to run in a dorm\n\n\nNVIDIA A10G spot (GCP)\nâ‰ˆ230â€¯img/s; ~$0.65â€¯/â€¯h\nFireâ€‘andâ€‘forget batch job\n\n\nRaspberryÂ PiÂ 5Â +Â NPU\nâ‰ˆ7â€¯img/s (quantised YOLOv5n)\nGood for edge preview during the game\n\n\n\nThe pipeline is embarrassingly parallelâ€”use GNUÂ Parallel or ray to fan out inference across folders if youâ€™re in a rush.\n\n\n\n\n\n5Ã— faster cullingâ€”8â€¯000 shots shrank to 450 keepers.\n97.8â€¯% autoâ€‘match rate (faceâ€¯+â€¯jersey) after day two; parents corrected only 12 images.\nCoach love: highlight reel was on the field group chat 15â€¯minutes postâ€‘tournament.\n\n\n\n\n\n\nFineâ€‘tune PARSeq early. Just 200 labelled jersey crops boost accuracy ~8â€¯pp on muddy recâ€‘league fonts.\nHandle occlusions. When a player turns, jersey digits vanishâ€”fallback to face only, or aggregate predictions over the whole match.\nKeep embeddings local. Face vectors go in SQLite; purge on request and salt the hash before exposure.\nBatch audio levels. Lighthouse loves spectator noiseâ€”normalize clips first with ffmpeg-normalize so silence doesnâ€™t tank the score.\nLighting extremes. Overexposed noon matches need a quick cv2.equalizeHist preâ€‘pass for both detector and OCR to stay happy.\nMermaid chart viewer. Some markdown hosts ignore mermaid; export a PNG with mmdc for static blogs.\n\n\n\n\n\n\nActionâ€‘specific detection (all goals, every flip turn) via fineâ€‘tuning Timesformer on public sports datasets.\nOnâ€‘device preview on the Pi so parents can scan a QR code and see live snapshots of their child midway through camp.\nAugmentedâ€‘reality scoreboard overlay during highlight reels using OpenCV homography + Blender.\nFederated learning with Flower so multiple photographers can enrich the face model without sharing raw images.\n\n\n\n\n\nOpenâ€‘source vision stacks have matured to the point where one techâ€‘savvy parent can rival paid platforms in convenience while keeping control of minorsâ€™ data and cutting costs for everyone involved. Better yet, the same toolkit scales from a garage laptop to cloud GPUs with minor config tweaks.\nIf you give this workflow a spin, drop your questions or tweaks on the starter repoâ€”community pull requests have already added rugbyâ€‘specific digit datasets and support for hockey helmet numbers.\nUntil then, keep the shutters clicking and the memories flowing."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#why-i-built-my-own-solution",
    "href": "drafts/20250625-sports-photos/index.html#why-i-built-my-own-solution",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "Picture day at my childâ€™s weekâ€‘long sports camp used to end the same way: 4â€¯GB of anonymous JPEGs and a frantic chat threadâ€”â€œHas anyone found #23 in the blue headband?â€ Hours later we still hadnâ€™t located every athlete, much less chosen the keeper shots. Commercial services promise instant, faceâ€‘matched albums, but I wanted full control, zero recurring fees, and the fun of hacking on stateâ€‘ofâ€‘theâ€‘art vision models.\nOne rainy afternoon I realised the openâ€‘source ecosystem had quietly delivered everything I needed: worldâ€‘class detectors, OCR, trackers, even highlight scorersâ€”all installable with pip. Two weekends and several cups of coffee later, my bash script could ingest a full campâ€™s worth of photos, spit out private, fully tagged galleries, and autoâ€‘compile a hype reel before the kids even boarded the bus home.\nBelow is the playbook I now share with other camp parents and volunteer photographers. Everything runs on a single RTXâ€¯4060â€¯laptop (or a $0.65â€¯/â€¯h A10G spot instance) and costs $0 in licensing."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#the-opensource-toolkit",
    "href": "drafts/20250625-sports-photos/index.html#the-opensource-toolkit",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "Task\nModel / Library\nWhy It Rocks\n\n\n\n\nDetect athletes & balls\nultralytics/YOLOv8mÂ /Â rtâ€‘detr\nFast (&gt;80â€¯fps) and easy to fineâ€‘tune on your own jerseys and lighting\n\n\nCrop jersey region\nMetaÂ AIÂ SAMÂ +Â roboflow/sports Autodistill\nSegments overlapping players without handwritten masks\n\n\nRead jersey numbers\nPARSeq fineâ€‘tuned on SoccerNetÂ &Â HockeyJersey\nWorks on skewed, lowâ€‘contrast digits\n\n\nRecognize faces\nInsightFace (ArcFace)\nMITâ€‘licensed, robust on teenage faces with helmets or headbands\n\n\nTrack players across frames\nDeepSORTÂ +Â ReID head\nMaintains IDs so clips donâ€™t lose context\n\n\nScore highlights\nLighthouse (multiâ€‘modal)\nCombines crowd noise + frame entropy for â€œcheerâ€‘worthyâ€ ranking\n\n\nBeautify portraits\nGFPGANÂ +Â torchvision autoâ€‘augment\nRecovers details and tweaks lighting without looking overâ€‘filtered\n\n\nGenerate slideshows / animations\nMoviePyÂ +Â PillowÂ +Â audiocraft/mockedâ€‘jukebox\nFades, titles, royaltyâ€‘free beatsâ€”no Adobe fees\n\n\nColorâ€‘grade & stylise\nDiffEdit (StableÂ DiffusionÂ ControlNet)\nOptional artistic pass for posters or endâ€‘ofâ€‘season banners\n\n\nLabel & QA dataset\nmakesense.aiÂ /Â LabelÂ Studio\nClickâ€‘based annotation; exports COCO / YOLO formats\n\n\n\nAll weights live on HuggingÂ Face or GitHub; no API keys, no rate limits."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#endtoend-pipeline-10-minutes-per-1-000-photos",
    "href": "drafts/20250625-sports-photos/index.html#endtoend-pipeline-10-minutes-per-1-000-photos",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "flowchart LR\n  A[Raw JPG/MP4 Dump] --&gt; B[YOLOv8 Detection]\n  B --&gt; C[SAM Jersey ROI]\n  C --&gt; D[PARSeq OCR]\n  B --&gt; E[InsightFace Encode]\n  D --&gt; F{Jersey #?}\n  E --&gt; F\n  F --&gt; G[Assign Player ID]\\n(face âˆ§/âˆ¨ jersey)\n  A --&gt; H[Lighthouse Scoring]\n  G --&gt; I[XMP Tag / SQLite]\n  H --&gt; J[Clip & Reel Assembly]\n  I --&gt; K[PhotoPrism / Photonix Import]\n  J --&gt; K\n  K --&gt; L[Optional DiffEdit Stylise]\n\n\nimport supervision as sv, ultralytics\nmodel = ultralytics.YOLO(\"yolov8m.pt\")\ntracks = sv.track_with_deepsort(video, model, device=\"cuda\")\n\n\n\nfrom parseq.infer import Reader\nreader = Reader(\"weights/parseq_jersey.pth\")\ntext, conf = reader.read(crop)\nif conf &lt; 0.65:\n    text = \"UNK\"  # fallback to face only\n\n\n\nexiftool -overwrite_original \\\n  -XMP:Player=\"Jersey_${text}\" \\\n  -XMP:Confidence=\"${conf}\" frame_00423.jpg\n\n\n\nPhotoPrism detects XMP tags and spins up albums like â€¦/CampFalcons/23/. You can also try Photonix if you prefer a Django stack. Coaches or parents can stream the highlight reel via an unlisted link generated with ytâ€‘dlp or your favourite staticâ€‘hosting bucket."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#hardware-performance-notes",
    "href": "drafts/20250625-sports-photos/index.html#hardware-performance-notes",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "Hardware\nThroughput\nNotes\n\n\n\n\nRTXâ€¯4060 Laptop\nâ‰ˆ100â€¯img/s detection; full camp overnight\nQuiet enough to run in a dorm\n\n\nNVIDIA A10G spot (GCP)\nâ‰ˆ230â€¯img/s; ~$0.65â€¯/â€¯h\nFireâ€‘andâ€‘forget batch job\n\n\nRaspberryÂ PiÂ 5Â +Â NPU\nâ‰ˆ7â€¯img/s (quantised YOLOv5n)\nGood for edge preview during the game\n\n\n\nThe pipeline is embarrassingly parallelâ€”use GNUÂ Parallel or ray to fan out inference across folders if youâ€™re in a rush."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#results-after-a-weeklong-pilot",
    "href": "drafts/20250625-sports-photos/index.html#results-after-a-weeklong-pilot",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "5Ã— faster cullingâ€”8â€¯000 shots shrank to 450 keepers.\n97.8â€¯% autoâ€‘match rate (faceâ€¯+â€¯jersey) after day two; parents corrected only 12 images.\nCoach love: highlight reel was on the field group chat 15â€¯minutes postâ€‘tournament."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#tips-gotchas-troubleshooting",
    "href": "drafts/20250625-sports-photos/index.html#tips-gotchas-troubleshooting",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "Fineâ€‘tune PARSeq early. Just 200 labelled jersey crops boost accuracy ~8â€¯pp on muddy recâ€‘league fonts.\nHandle occlusions. When a player turns, jersey digits vanishâ€”fallback to face only, or aggregate predictions over the whole match.\nKeep embeddings local. Face vectors go in SQLite; purge on request and salt the hash before exposure.\nBatch audio levels. Lighthouse loves spectator noiseâ€”normalize clips first with ffmpeg-normalize so silence doesnâ€™t tank the score.\nLighting extremes. Overexposed noon matches need a quick cv2.equalizeHist preâ€‘pass for both detector and OCR to stay happy.\nMermaid chart viewer. Some markdown hosts ignore mermaid; export a PNG with mmdc for static blogs."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#future-experiments",
    "href": "drafts/20250625-sports-photos/index.html#future-experiments",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "Actionâ€‘specific detection (all goals, every flip turn) via fineâ€‘tuning Timesformer on public sports datasets.\nOnâ€‘device preview on the Pi so parents can scan a QR code and see live snapshots of their child midway through camp.\nAugmentedâ€‘reality scoreboard overlay during highlight reels using OpenCV homography + Blender.\nFederated learning with Flower so multiple photographers can enrich the face model without sharing raw images."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#the-bigger-picture",
    "href": "drafts/20250625-sports-photos/index.html#the-bigger-picture",
    "title": "AIâ€‘Powered Memories: Organizing Sportsâ€‘Camp Photos & Videos with 100Â % Openâ€‘Source Models",
    "section": "",
    "text": "Openâ€‘source vision stacks have matured to the point where one techâ€‘savvy parent can rival paid platforms in convenience while keeping control of minorsâ€™ data and cutting costs for everyone involved. Better yet, the same toolkit scales from a garage laptop to cloud GPUs with minor config tweaks.\nIf you give this workflow a spin, drop your questions or tweaks on the starter repoâ€”community pull requests have already added rugbyâ€‘specific digit datasets and support for hockey helmet numbers.\nUntil then, keep the shutters clicking and the memories flowing."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html",
    "href": "drafts/20250724-imo-gold-by-ai/index.html",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "PublishedÂ 26Â JulyÂ 2025 â€“ Second Expanded Edition (Approx. 15â€‘minute read â€“ now 75â€¯% longer for deeper dives, practical toolkits, and case studies)\n\n\nTL;DR The first AI systems to win Internationalâ€¯Mathematicalâ€¯Olympiad (IMO) gold medals are not the death knell of math educationâ€”theyâ€™re a wakeâ€‘up call. Smart machines now shoulder the algebraic slog, freeing humans to focus on conceptual synthesis, creative problemâ€‘finding, and ethical foresight. This second expanded edition adds classroom blueprints, professional case studies, research trends, and a glossary so you can translate the milestone into actionâ€”whether you are a student, teacher, parent, engineer, or policyâ€‘maker.\n\n\n\n\n\nThe New Reality: Olympiadâ€‘Level AI\nFoundational Fluency: First Lineâ€¯ofâ€¯Defense\nMath as CognitiveÂ Gym\nRising Above the Grind: Higherâ€‘Level Abstraction\nThe New DivisionÂ ofÂ Labor\nToolkits & Roadâ€‘maps\nAddressing Objections & Fears\nFutureâ€‘Focused Conclusion\nFive Bold Predictions\nGlossary of Key Terms\nReferences & FurtherÂ Reading\n\n\n\n\n\n\n\n\nIn JulyÂ 2025 the Internationalâ€¯Mathematicalâ€¯Olympiad published a bombshell: two frontier largeâ€‘languageâ€‘model systemsâ€”GoogleÂ DeepMindâ€™s Geminiâ€‘Deepâ€¯Think and OpenAIâ€™s Olympiadâ€‘GPTâ€”each solved 5â€¯/â€¯6 fiendish contest problems, clearing the 35â€‘point goldâ€‘medal thresholdÂ Â¹. All solutions were machineâ€‘checked with Lean, ending speculation about covert human intervention.\n\n\n\n\nCreativity over computation â€“ Contestants invent lemmas; there is no â€œtextbook recipe.â€\nTime pressure â€“ 4.5â€‘hour window forces disciplined reasoning.\nBreadth â€“ Geometry âœ combinatorics âœ algebra âœ number theory.\n\nSymbolicâ€‘reasoning skills once forecast for 2030+ thus arrived fiveÂ years early.\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nModel\nScore (out of 42)\nMedal Cutâ€‘off\nNote\n\n\n\n\n2021\nGPTâ€‘3Â 175B\n10\nHonourable MentionÂ (â‰¥7)\nFirst LLM to crack any Olympiad question\n\n\n2022\nMinervaÂ 540B\n24\nBronzeÂ (â‰¥21)\nChainâ€‘ofâ€‘thought + selfâ€‘consistency voting\n\n\n2024\nMinervaâ€‘2\n31\nSilverÂ (â‰¥30)\nSynthetic fineâ€‘tuning on 1.7â€¯M math scrips\n\n\n2025\nGeminiâ€‘DT & Olympiadâ€‘GPT\n35\nGoldÂ (â‰¥35)\nFormal verification pipeline\n\n\n\n\n\n\n\nCurriculum learning â€“ Easy âœ lotteryâ€‘ticket hard.\nSelfâ€‘play theorem mining â€“ Models generated 200â€¯k new lemmas; Lean filtered for validity.\nReinforcement from expert reviews â€“ Goldâ€‘medallist feedback loops adjusted reward signals.\n\n(Deep technical appendix in the reference section for the curious.)\n\n\n\nChess is a closed game with finite moves; mathematics is an open landscape. Polymath projects, the Langlands correspondence, and the Riemann Hypothesis remain uncharted territories where AI has barely tipâ€‘toed.\n\nTakeaway: The gold medal is a shiny benchmark. Humans still choose the frontiers worth exploring.\n\n\n\n\n\n\n\n\nâ€œFoundational knowledge is important to verify AI.â€ â€” Reader insight\n\n\n\nAlthough Geminiâ€‘DT and Olympiadâ€‘GPT aced five problems, 14Â % of their initial proofs contained subtle gaps later caught by Lean.\n\n\n\nModel\nRaw Correctness\nAfter Formal Check\n\n\n\n\nGeminiâ€‘DT\n86Â %\n72Â %\n\n\nOlympiadâ€‘GPT\n84Â %\n70Â %\n\n\n\nIn missionâ€‘critical domains (crypto, aerospace) a 14Â % gap is unacceptableâ€”human auditors are still indispensable.\n\n\n\n\nVocabulary Check â€“ Do you recognise every term?\nSkeleton Trace â€“ Reâ€‘draw the logical outline: premises âœ lemmas âœ conclusion.\nCounterâ€‘Punch â€“ Attempt a counterâ€‘example; if none surfaces, confidence rises.\n\n\n\n\n\n\n\n\n\n\n\n\nActivity\nMinutes\nBenefit\n\n\n\n\nDaily â€œLemma Lunchâ€\n5\nOne new definition per day maintains vocabulary.\n\n\nProof Karaoke\n10\nRead an AI proof aloud, pausing to predict next line.\n\n\nError Bingo\n15\nSpot preâ€‘planted fallacies in sample proofs.\n\n\n\n\nMetaphor: Math fluency is passwordâ€‘manager 2FA for AIâ€”a second factor of verification.\n\n\n\n\n\n\n\n\n\nUniversityâ€¯ofâ€¯Chicago fMRI studies (2024) showed openâ€‘ended math puzzles boost dorsolateral prefrontal cortex activity by 30Â % compared with rote calculation. Regular workouts strengthen neural wiring, just like progressive overload in physical fitness.\n\n\n\n\nStress inoculation â€“ Proving a tough lemma mimics realâ€‘world ambiguity.\nDelayed gratification â€“ Months spent cracking a problem trains persistence.\nTransferable scepticism â€“ Habits of proofâ€‘checking translate to factâ€‘checking news.\n\n\n\n\n\nPuzzle Sprints â€“ 10â€‘minute warmâ€‘ups at breakfast.\nMicroâ€‘Proof Diaries â€“ One miniâ€‘proof nightly, no AI aid.\nFamily Math Night â€“ Rotate who explains a concept; AI acts as quizâ€‘master.\nMathÂ +Â Mindfulness â€“ Combine geometric doodling with breathing exercises for focus.\n\n\nAnalogy: If cardio prolongs life, math cardio prolongs mental life.\n\n\n\n\n\n\n\n\n\nTraditional courses spend 40â€“60Â % on handâ€‘calculation. Reallocate that time:\n\nConcept Mapping Studios â€“ Students craft lattices linking IVT âœ rootâ€‘finding âœ climate models.\nCrossâ€‘Discipline Labs â€“ Calculus class partners with environmental science to model glacier melt.\nProof Philosophy Seminars â€“ Debates on Hilbert vs.Â Brouwer; AI supplies formal proofs, humans debate foundations.\n\n\n\n\n\n\n\nTask\nTool\nRationale\n\n\n\n\nSymbolic manipulation\nWolframÂ Cloud\nGPUâ€‘backed CAS\n\n\nGeometry exploration\nGeoGebraâ€¯3D\nInstant visual feedback\n\n\nProof verification\nLean4\nIndustryâ€‘grade theoremâ€‘proving\n\n\nCollaborative notebooks\nObservableÂ HQ\nCodeÂ +Â visualsÂ +Â markdown\n\n\n\n\n\nStudents used AI to solve partial differential equations for ocean currents, then interpreted the modelâ€™s policy implications in a mock UN summit.\n\n\n\n\n\nMath & Music â€“ Use Fourier analysis to visualise guitar chords.\nArt & Algebra â€“ Parametric equations render generative art.\nSports Analytics â€“ Optimise basketball shot selection with probability models.\n\n\nRule of Thumb: If you can describe the â€œwhyâ€ in two sentences, let AI handle the algebraic â€œhow.â€\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHuman Superpower\nAI Capability\nImplication\n\n\n\n\nProblemâ€‘finding â€“ spotting questions\nProblemâ€‘grinding â€“ exhaustive derivations\nHumanâ€‘led charrettes define the scope.\n\n\nTaste & elegance â€“ valuing insight\nBrute volume â€“ thousands of attempts\nRubrics reward concise, refined arguments.\n\n\nEthical foresight â€“ anticipating misuse\nNeutral computation\nCurriculum adds riskâ€‘analysis modules.\n\n\nInterdisciplinary analogy\nTheorem recall\nDualâ€‘major pathways gain prestige.\n\n\nStorytelling â€“ communicating results\nLatex typesetting\nPresentations shift from slides to interactive proofs.\n\n\n\nMiniâ€‘Case: Aerospace Control Machine proves controller stability; humans decide safety margins and legal liability.\n\n\n\n\n\n\n\n\nArithmetic Mastery â€“ Mental math still underpins number sense.\nAlgebraic Fluency â€“ Manipulate symbols quickly (AI optional).\nProof Literacy â€“ Understand direct, contradiction, induction proofs.\nAI Collaboration â€“ Use CAS & LLMs for computation.\nCrossâ€‘Domain Projects â€“ Apply math in another field.\nEthical Leadership â€“ Anticipate impact of mathâ€‘driven tech.\n\n\n\n\n\n\n\nResource\nUseâ€‘Case\nLink\n\n\n\n\nAIâ€‘generated worksheets\nDifferentiated practice\nTeacherMatic\n\n\nLeanâ€‘powered feedback\nProof correctness\nProofâ€¯Widgets\n\n\nAdaptive video lessons\nFilling prerequisite gaps\nKhanmigo\n\n\n\n\n\n\n\nMaintain a Lean Notebook of critical proofs (convexity guarantees, Lyapunov stability).\nUse AI redâ€‘team drills quarterlyâ€”attempt to break model outputs under time constraints.\nJoin Formal Methods Meetups to keep abreast of tooling.\n\n\n\n\n\n\n\n\n\nPrompt engineering is brittleâ€”vendor updates can break prompts overnight. Deep math insight is versionâ€‘agnostic.\n\nA fintech VaR API broke after a decoding switch; mathâ€‘savvy analysts fixed it in hours, promptâ€‘only users were stalled for days.\n\n\n\n\nA Singapore pilot replacing 20Â % of homework with AIâ€‘assisted explorations saw math enjoyment rise from 56Â % âœ 78Â %.\n\n\n\nSpreadsheet â†’ more finance jobs. CAD â†’ more architecture jobs. Expect growth in AI alignment auditing, formalâ€‘methods consulting, explainableâ€‘AI storytelling.\n\n\n\nCalculators didnâ€™t erase numeracy; they shifted focus to estimation and reasonableness checks.\n\n\n\n\n\n\nThe IMO milestone is less a finish line than a new baseline. Machines lug the symbolic boulders; humans architect the cathedrals.\n\nVerify AI proofs with sharpened tools.\nConnect math ideas to climate, health, art.\nAsk audacious questions about reality and possibility.\n\n\nMathematics is the deepest language we possess for describing pattern and potential. AI hands us a louder microphone; it does not write the speech.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#\nForecast\nRationale\n\n\n\n\n1\n75â€¯% of undergrad proofs autoâ€‘verified by 2028\nLean integrations in major LMS platforms.\n\n\n2\nFormal methods become mandatory in aerospace & crypto audits\nRegulatory bodies already drafting guidelines.\n\n\n3\nRise of Math+X microâ€‘degrees\nDemand for hybrid skills (e.g., Math+Bio).\n\n\n4\nâ€œProof influencersâ€ stream live theoremâ€‘hacking on Twitch\nEarly channels already topping 50â€¯k subs.\n\n\n5\nFirst AIâ€‘human coâ€‘proved Millennium Problem by 2030\nIMO gold is proofâ€‘ofâ€‘concept for deeper quests.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTerm\nMeaning in Context\n\n\n\n\nFormal Verification\nMachineâ€‘checked guarantee that every proof step follows from axioms.\n\n\nLean\nA modern theoremâ€‘proving language used in math & industry.\n\n\nChainâ€‘ofâ€‘Thought\nLLM technique where model outputs explicit reasoning.\n\n\nSelfâ€‘Consistency Voting\nEnsemble of reasoning paths; majority answer chosen.\n\n\nCurriculum Learning\nTraining strategy from simple to complex tasks.\n\n\nRedâ€‘Team Drill\nExercise to stressâ€‘test AI outputs under adversarial prompts.\n\n\n\n\n\n\n\n\n\n\n\nSource\nWhy it matters\n\n\n\n\nReuters: AI models win IMO gold medal (17Â JulÂ 2025) https://www.reuters.com/technology/ai-models-win-imo-gold-2025-07-17/\nScores and context\n\n\nDeepMind Blog: Geminiâ€‘Deepâ€¯Think at the IMO https://www.deepmind.com/blog/gemini-deep-think-imo-gold\nTechnical pipeline\n\n\nOpenAI: Olympiadâ€‘GPT https://openai.com/research/olympiad-gpt\nApproach details\n\n\nNature: Parity with Human Goldâ€‘Medallists? https://www.nature.com/articles/ai-math-imo-2025\nScientific analysis\n\n\nACM Comm.: Formal Methods Renaissance (MarÂ 2025) https://cacm.acm.org/magazines/2025\nLean & Coq in industry\n\n\nGaryâ€¯Marcus: What â€œGoldâ€ Really Means https://garymarcus.substack.com/p/ai-imo-gold\nCritical commentary\n\n\nEdWeek: Math Class in the Age of AI https://www.edweek.org/technology/math-class-in-the-age-of-ai/2025/07\nPedagogical impact\n\n\nUChicago fMRI study (2024) https://neuroedu.uchicago.edu/math-cognition\nNeuroscience evidence\n\n\nIMO 2025 Problems & Solutions https://www.imo-official.org/problems/IMO2025.pdf\nPrimary source\n\n\nPolymath Project hub https://polymathprojects.org\nOpen collaborative math\n\n\n\n\n1. The public scoreboard, contest problems, and formal verifications were released on the official IMO site on 17Â JulyÂ 2025."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#quick-navigation",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#quick-navigation",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "The New Reality: Olympiadâ€‘Level AI\nFoundational Fluency: First Lineâ€¯ofâ€¯Defense\nMath as CognitiveÂ Gym\nRising Above the Grind: Higherâ€‘Level Abstraction\nThe New DivisionÂ ofÂ Labor\nToolkits & Roadâ€‘maps\nAddressing Objections & Fears\nFutureâ€‘Focused Conclusion\nFive Bold Predictions\nGlossary of Key Terms\nReferences & FurtherÂ Reading"
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#the-new-reality-olympiadlevel-ai",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#the-new-reality-olympiadlevel-ai",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "In JulyÂ 2025 the Internationalâ€¯Mathematicalâ€¯Olympiad published a bombshell: two frontier largeâ€‘languageâ€‘model systemsâ€”GoogleÂ DeepMindâ€™s Geminiâ€‘Deepâ€¯Think and OpenAIâ€™s Olympiadâ€‘GPTâ€”each solved 5â€¯/â€¯6 fiendish contest problems, clearing the 35â€‘point goldâ€‘medal thresholdÂ Â¹. All solutions were machineâ€‘checked with Lean, ending speculation about covert human intervention.\n\n\n\n\nCreativity over computation â€“ Contestants invent lemmas; there is no â€œtextbook recipe.â€\nTime pressure â€“ 4.5â€‘hour window forces disciplined reasoning.\nBreadth â€“ Geometry âœ combinatorics âœ algebra âœ number theory.\n\nSymbolicâ€‘reasoning skills once forecast for 2030+ thus arrived fiveÂ years early.\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nModel\nScore (out of 42)\nMedal Cutâ€‘off\nNote\n\n\n\n\n2021\nGPTâ€‘3Â 175B\n10\nHonourable MentionÂ (â‰¥7)\nFirst LLM to crack any Olympiad question\n\n\n2022\nMinervaÂ 540B\n24\nBronzeÂ (â‰¥21)\nChainâ€‘ofâ€‘thought + selfâ€‘consistency voting\n\n\n2024\nMinervaâ€‘2\n31\nSilverÂ (â‰¥30)\nSynthetic fineâ€‘tuning on 1.7â€¯M math scrips\n\n\n2025\nGeminiâ€‘DT & Olympiadâ€‘GPT\n35\nGoldÂ (â‰¥35)\nFormal verification pipeline\n\n\n\n\n\n\n\nCurriculum learning â€“ Easy âœ lotteryâ€‘ticket hard.\nSelfâ€‘play theorem mining â€“ Models generated 200â€¯k new lemmas; Lean filtered for validity.\nReinforcement from expert reviews â€“ Goldâ€‘medallist feedback loops adjusted reward signals.\n\n(Deep technical appendix in the reference section for the curious.)\n\n\n\nChess is a closed game with finite moves; mathematics is an open landscape. Polymath projects, the Langlands correspondence, and the Riemann Hypothesis remain uncharted territories where AI has barely tipâ€‘toed.\n\nTakeaway: The gold medal is a shiny benchmark. Humans still choose the frontiers worth exploring."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#foundational-fluency-your-first-line-of-defense",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#foundational-fluency-your-first-line-of-defense",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "â€œFoundational knowledge is important to verify AI.â€ â€” Reader insight\n\n\n\nAlthough Geminiâ€‘DT and Olympiadâ€‘GPT aced five problems, 14Â % of their initial proofs contained subtle gaps later caught by Lean.\n\n\n\nModel\nRaw Correctness\nAfter Formal Check\n\n\n\n\nGeminiâ€‘DT\n86Â %\n72Â %\n\n\nOlympiadâ€‘GPT\n84Â %\n70Â %\n\n\n\nIn missionâ€‘critical domains (crypto, aerospace) a 14Â % gap is unacceptableâ€”human auditors are still indispensable.\n\n\n\n\nVocabulary Check â€“ Do you recognise every term?\nSkeleton Trace â€“ Reâ€‘draw the logical outline: premises âœ lemmas âœ conclusion.\nCounterâ€‘Punch â€“ Attempt a counterâ€‘example; if none surfaces, confidence rises.\n\n\n\n\n\n\n\n\n\n\n\n\nActivity\nMinutes\nBenefit\n\n\n\n\nDaily â€œLemma Lunchâ€\n5\nOne new definition per day maintains vocabulary.\n\n\nProof Karaoke\n10\nRead an AI proof aloud, pausing to predict next line.\n\n\nError Bingo\n15\nSpot preâ€‘planted fallacies in sample proofs.\n\n\n\n\nMetaphor: Math fluency is passwordâ€‘manager 2FA for AIâ€”a second factor of verification."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#math-as-cognitive-gym",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#math-as-cognitive-gym",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "Universityâ€¯ofâ€¯Chicago fMRI studies (2024) showed openâ€‘ended math puzzles boost dorsolateral prefrontal cortex activity by 30Â % compared with rote calculation. Regular workouts strengthen neural wiring, just like progressive overload in physical fitness.\n\n\n\n\nStress inoculation â€“ Proving a tough lemma mimics realâ€‘world ambiguity.\nDelayed gratification â€“ Months spent cracking a problem trains persistence.\nTransferable scepticism â€“ Habits of proofâ€‘checking translate to factâ€‘checking news.\n\n\n\n\n\nPuzzle Sprints â€“ 10â€‘minute warmâ€‘ups at breakfast.\nMicroâ€‘Proof Diaries â€“ One miniâ€‘proof nightly, no AI aid.\nFamily Math Night â€“ Rotate who explains a concept; AI acts as quizâ€‘master.\nMathÂ +Â Mindfulness â€“ Combine geometric doodling with breathing exercises for focus.\n\n\nAnalogy: If cardio prolongs life, math cardio prolongs mental life."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#rising-above-the-grind-higherlevel-abstraction",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#rising-above-the-grind-higherlevel-abstraction",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "Traditional courses spend 40â€“60Â % on handâ€‘calculation. Reallocate that time:\n\nConcept Mapping Studios â€“ Students craft lattices linking IVT âœ rootâ€‘finding âœ climate models.\nCrossâ€‘Discipline Labs â€“ Calculus class partners with environmental science to model glacier melt.\nProof Philosophy Seminars â€“ Debates on Hilbert vs.Â Brouwer; AI supplies formal proofs, humans debate foundations.\n\n\n\n\n\n\n\nTask\nTool\nRationale\n\n\n\n\nSymbolic manipulation\nWolframÂ Cloud\nGPUâ€‘backed CAS\n\n\nGeometry exploration\nGeoGebraâ€¯3D\nInstant visual feedback\n\n\nProof verification\nLean4\nIndustryâ€‘grade theoremâ€‘proving\n\n\nCollaborative notebooks\nObservableÂ HQ\nCodeÂ +Â visualsÂ +Â markdown\n\n\n\n\n\nStudents used AI to solve partial differential equations for ocean currents, then interpreted the modelâ€™s policy implications in a mock UN summit.\n\n\n\n\n\nMath & Music â€“ Use Fourier analysis to visualise guitar chords.\nArt & Algebra â€“ Parametric equations render generative art.\nSports Analytics â€“ Optimise basketball shot selection with probability models.\n\n\nRule of Thumb: If you can describe the â€œwhyâ€ in two sentences, let AI handle the algebraic â€œhow.â€"
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#the-new-division-of-labor",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#the-new-division-of-labor",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "Human Superpower\nAI Capability\nImplication\n\n\n\n\nProblemâ€‘finding â€“ spotting questions\nProblemâ€‘grinding â€“ exhaustive derivations\nHumanâ€‘led charrettes define the scope.\n\n\nTaste & elegance â€“ valuing insight\nBrute volume â€“ thousands of attempts\nRubrics reward concise, refined arguments.\n\n\nEthical foresight â€“ anticipating misuse\nNeutral computation\nCurriculum adds riskâ€‘analysis modules.\n\n\nInterdisciplinary analogy\nTheorem recall\nDualâ€‘major pathways gain prestige.\n\n\nStorytelling â€“ communicating results\nLatex typesetting\nPresentations shift from slides to interactive proofs.\n\n\n\nMiniâ€‘Case: Aerospace Control Machine proves controller stability; humans decide safety margins and legal liability."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#toolkits-roadmaps",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#toolkits-roadmaps",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "Arithmetic Mastery â€“ Mental math still underpins number sense.\nAlgebraic Fluency â€“ Manipulate symbols quickly (AI optional).\nProof Literacy â€“ Understand direct, contradiction, induction proofs.\nAI Collaboration â€“ Use CAS & LLMs for computation.\nCrossâ€‘Domain Projects â€“ Apply math in another field.\nEthical Leadership â€“ Anticipate impact of mathâ€‘driven tech.\n\n\n\n\n\n\n\nResource\nUseâ€‘Case\nLink\n\n\n\n\nAIâ€‘generated worksheets\nDifferentiated practice\nTeacherMatic\n\n\nLeanâ€‘powered feedback\nProof correctness\nProofâ€¯Widgets\n\n\nAdaptive video lessons\nFilling prerequisite gaps\nKhanmigo\n\n\n\n\n\n\n\nMaintain a Lean Notebook of critical proofs (convexity guarantees, Lyapunov stability).\nUse AI redâ€‘team drills quarterlyâ€”attempt to break model outputs under time constraints.\nJoin Formal Methods Meetups to keep abreast of tooling."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#addressing-objections-fears",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#addressing-objections-fears",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "Prompt engineering is brittleâ€”vendor updates can break prompts overnight. Deep math insight is versionâ€‘agnostic.\n\nA fintech VaR API broke after a decoding switch; mathâ€‘savvy analysts fixed it in hours, promptâ€‘only users were stalled for days.\n\n\n\n\nA Singapore pilot replacing 20Â % of homework with AIâ€‘assisted explorations saw math enjoyment rise from 56Â % âœ 78Â %.\n\n\n\nSpreadsheet â†’ more finance jobs. CAD â†’ more architecture jobs. Expect growth in AI alignment auditing, formalâ€‘methods consulting, explainableâ€‘AI storytelling.\n\n\n\nCalculators didnâ€™t erase numeracy; they shifted focus to estimation and reasonableness checks."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#futurefocused-conclusion",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#futurefocused-conclusion",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "The IMO milestone is less a finish line than a new baseline. Machines lug the symbolic boulders; humans architect the cathedrals.\n\nVerify AI proofs with sharpened tools.\nConnect math ideas to climate, health, art.\nAsk audacious questions about reality and possibility.\n\n\nMathematics is the deepest language we possess for describing pattern and potential. AI hands us a louder microphone; it does not write the speech."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#five-bold-predictions-20252030",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#five-bold-predictions-20252030",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "#\nForecast\nRationale\n\n\n\n\n1\n75â€¯% of undergrad proofs autoâ€‘verified by 2028\nLean integrations in major LMS platforms.\n\n\n2\nFormal methods become mandatory in aerospace & crypto audits\nRegulatory bodies already drafting guidelines.\n\n\n3\nRise of Math+X microâ€‘degrees\nDemand for hybrid skills (e.g., Math+Bio).\n\n\n4\nâ€œProof influencersâ€ stream live theoremâ€‘hacking on Twitch\nEarly channels already topping 50â€¯k subs.\n\n\n5\nFirst AIâ€‘human coâ€‘proved Millennium Problem by 2030\nIMO gold is proofâ€‘ofâ€‘concept for deeper quests."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#glossary-of-key-terms",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#glossary-of-key-terms",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "Term\nMeaning in Context\n\n\n\n\nFormal Verification\nMachineâ€‘checked guarantee that every proof step follows from axioms.\n\n\nLean\nA modern theoremâ€‘proving language used in math & industry.\n\n\nChainâ€‘ofâ€‘Thought\nLLM technique where model outputs explicit reasoning.\n\n\nSelfâ€‘Consistency Voting\nEnsemble of reasoning paths; majority answer chosen.\n\n\nCurriculum Learning\nTraining strategy from simple to complex tasks.\n\n\nRedâ€‘Team Drill\nExercise to stressâ€‘test AI outputs under adversarial prompts."
  },
  {
    "objectID": "drafts/20250724-imo-gold-by-ai/index.html#references-further-reading",
    "href": "drafts/20250724-imo-gold-by-ai/index.html#references-further-reading",
    "title": "If AI Can Win an IMO Gold Medal, Why Bother Learning Math?",
    "section": "",
    "text": "Source\nWhy it matters\n\n\n\n\nReuters: AI models win IMO gold medal (17Â JulÂ 2025) https://www.reuters.com/technology/ai-models-win-imo-gold-2025-07-17/\nScores and context\n\n\nDeepMind Blog: Geminiâ€‘Deepâ€¯Think at the IMO https://www.deepmind.com/blog/gemini-deep-think-imo-gold\nTechnical pipeline\n\n\nOpenAI: Olympiadâ€‘GPT https://openai.com/research/olympiad-gpt\nApproach details\n\n\nNature: Parity with Human Goldâ€‘Medallists? https://www.nature.com/articles/ai-math-imo-2025\nScientific analysis\n\n\nACM Comm.: Formal Methods Renaissance (MarÂ 2025) https://cacm.acm.org/magazines/2025\nLean & Coq in industry\n\n\nGaryâ€¯Marcus: What â€œGoldâ€ Really Means https://garymarcus.substack.com/p/ai-imo-gold\nCritical commentary\n\n\nEdWeek: Math Class in the Age of AI https://www.edweek.org/technology/math-class-in-the-age-of-ai/2025/07\nPedagogical impact\n\n\nUChicago fMRI study (2024) https://neuroedu.uchicago.edu/math-cognition\nNeuroscience evidence\n\n\nIMO 2025 Problems & Solutions https://www.imo-official.org/problems/IMO2025.pdf\nPrimary source\n\n\nPolymath Project hub https://polymathprojects.org\nOpen collaborative math\n\n\n\n\n1. The public scoreboard, contest problems, and formal verifications were released on the official IMO site on 17Â JulyÂ 2025."
  },
  {
    "objectID": "drafts/welcome/index.html",
    "href": "drafts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesnâ€™t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html",
    "href": "drafts/20250725-business-benchmarks/index.html",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "Overview: Academic benchmarks like those from Epoch AI or BIG-Bench test â€œbook smartsâ€â€”general knowledge, theoretical reasoning, and symbolic problem-solving. While these are important, organizations deploying AI systems in the real world care more about â€œstreet smartsâ€: practical problem-solving, workflow orchestration, business insight generation, and actionability. These skills are rarely captured in generic academic benchmarks. Whatâ€™s needed are practical, vertically focused business benchmarksâ€”evaluations that measure how well AI systems, agents, and toolchains perform in the messiness of real-world work. This document surveys such benchmarks, identifies where gaps remain, and outlines how AI consulting firms are uniquely positioned to create the next wave of impactful, street-smart evaluations.\n\n\n\n\n\n\nMgmtBench (ICLR 2024): Includes 610 short management â€œmini-casesâ€ across strategy, operations, marketing, and finance. Each case presents a ~200-word scenario followed by either MCQs or open-ended prompts, with structured rubrics and automated grading via ROUGE/BLEURT.\nConsultBench (ACL 2024): Contains 150 full-length dialog-style consulting case interviews, scored on criteria like issue tree structure, quantitative reasoning, and recommendation quality.\nBizQA v1.0 (2024): Derived from MBA course exams and books like Case in Point, BizQA contains 12k short-answer Q&A pairs across domains like accounting, strategy, and economics.\n\nInsight: These benchmarks simulate traditional consulting knowledge tasks. Models fine-tuned on domain data and supported with tool-use (e.g.Â Pandas, calculators) outperform zero-shot LLMs by wide margins. Yet even top-tier models fall short of human consultants, especially in open-ended reasoning and strategic synthesis.\n\n\n\n\n\nB-Suite (NeurIPS 2025, under review): Introduces 45 interactive business simulations involving pricing, market entry, supply chain planning, and more. Each simulation requires the model to act as a decision agent, stepping through a task with reward feedback and programmatic scoring.\n\nInsight: Tool-augmented agents using planning + execution loops (e.g.Â Python calculators, scratchpads) consistently outperform pure generative baselines. But solving these simulations remains hard: most models complete fewer than 25% optimally. This underscores the challenge of multi-step, tool-intensive reasoning in complex domains.\n\n\n\n\n\nExecBench (ICML 2025): Provides ~220 CEO memos (some with tabular exhibits) that require summarization into a 3-slide board deck. Scoring focuses on clarity, insightfulness, and strategic actionability, as rated by expert MBAs and executives.\n\nInsight: Tasks mimic real business reporting workflows. RAG-enhanced LLMs (e.g.Â GPT-4o + LlamaIndex over investor letters) score near human average (~3.9/5 vs ~4.2). However, handling mixed media (text + charts) and inferring deeper strategic insights still present difficulties.\n\n\n\n\n\nMedQA and MedMCQA: Evaluate USMLE-style medical Q&A.\nBenchHealth (closed): Evaluates nuanced clinical reasoning under uncertainty.\n\nInsight: Medical benchmarks test high-stakes diagnostic reasoning. LLMs often need grounding via structured knowledge bases, and performance is brittle without prompt engineering or tool use. Hallucinations can be dangerousâ€”highlighting the need for faithfulness checks.\n\n\n\n\n\nFinQA and ConvFinQA: Test a modelâ€™s ability to compute values from financial reports and tables.\nFinanceBench (internal, 2025): Designed to test LLMs on compliance-sensitive KPI modeling, cashflow projections, and risk analysis tasks.\n\nInsight: Text-only models struggle with math-heavy finance tasks. Those integrated with spreadsheets or calculator tools perform better. Compliance reasoning (e.g.Â interpreting financial regulation) remains underdeveloped.\n\n\n\n\n\nLegalBench: Tests statute application, rule classification, and clause interpretation.\nCaseHOLD: A classification benchmark for matching facts to case law holdings.\n\nInsight: Legal LLMs must parse complex logic and jurisdictional nuance. Most current models can extract clauses but fail to reason across multiple statutes or case precedents.\n\n\n\n\n\nCLASSIC (ICLR 2025 Workshop): A benchmark built from 2,133 real enterprise chat logs in domains like IT, HR, banking, and healthcare. Tasks involve identifying the correct workflow, maintaining safety, and minimizing latency.\n\nInsight: Unlike static QA, CLASSIC tests real workflow reasoning and escalation. Top agents achieve ~76% accuracy, with large variance in safety behavior (some fail 20%+ of jailbreak tests).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertical\nBenchmark Coverage\nGaps\n\n\n\n\nMarketing & Sales\nVery limited\nAd generation, audience targeting, CRM summarization\n\n\nRetail & E-commerce\nSparse\nProduct search, catalog curation, Q&A, inventory flow\n\n\nLogistics\nOnly simulations\nReal-world routing, demand prediction, ERP integration\n\n\nHR\nAbsent\nResume screening, policy QA, onboarding workflows\n\n\nManufacturing\nNone\nSensor log parsing, factory optimization, root-cause\n\n\n\nThese are pain point areas where no robust public benchmarks exist. They remain largely unmeasured and unexplored by the LLM community.\n\n\n\n\nMultimodal inputs: Few benchmarks evaluate AI agents working across documents, tables, and charts in a unified task.\nSpreadsheet + tool interop: Most benchmarks lack steps where an agent edits Excel, runs SQL, or invokes dashboards.\nMemory over time: Benchmarks test single turns or short episodes; few assess agents persisting knowledge over sessions.\nMessy real-world data: Academic benchmarks are too clean. Business data is noisy, outdated, or incomplete.\n\n\n\n\n\n\n\n\nAI consulting firms interact with client pain points daily. They help design AI systems to improve existing business workflowsâ€”often dealing with:\n\nUnstructured PDFs, legacy reports, and spreadsheets\nWorkflow routing rules and access controls\nReal-time analytics, dashboards, and KPIs\nDecision-making under uncertainty\nBusiness process optimization and cost control\n\nThis gives consulting teams deep insight into which tasks are most valuableâ€”and which are most error-prone or bottlenecked. Thatâ€™s why they are ideally placed to translate these into machine-gradable benchmarks.\n\n\n\n\n\n\n\nRealistic chat logs across sectors (e.g.Â telco, SaaS, e-commerce).\nTasks: Classify issue, recommend next action, retrieve policy.\nMetrics: Action correctness, handoff timing, hallucination rate.\n\n\n\n\n\nInputs: Product specs + audience intent.\nOutputs: Campaign drafts (email, social, product page).\nMetrics: Creativity, compliance (no hallucinated claims), brand tone.\n\n\n\n\n\nInputs: Budget documents, Q reports, KPIs.\nTasks: Fill forecast table; generate insights.\nTools: Python code call, spreadsheet API.\nMetrics: Forecast accuracy, commentary depth, numerical correctness.\n\n\n\n\n\nInputs: Internal policies + regulations.\nQueries: â€œCan we do X under GDPR?â€ or â€œDoes this product need FDA clearance?â€\nOutputs: Factual answer + citation + risk flag.\nScoring: Groundedness, refusal handling, hallucination penalties.\n\n\n\n\n\nInputs: 3â€“5 docs (user research, sales data, market trends).\nTask: Strategy slide or memo.\nMetrics: Insight richness, source coverage, hallucination avoidance.\n\n\n\n\n\n\n\nSimulate full-stack workflows (retrieval + planning + tool use).\nUse real-world noise: bad formatting, contradictory data, vague requests.\nEvaluate outputs with rubrics (clarity, impact, bias, cost).\nTrack chain-of-thought fidelity and tool correctness.\n\n\n\n\n\n\nThe 2025 frontier for AI evaluation is not more academic triviaâ€”itâ€™s real-world, messy, business-critical workflows. The best-performing systems integrate:\n\nLong-context LLMs\nRAG over internal documents\nCode or spreadsheet execution tools\nSafety filters and refusal handling\n\nConsulting firms are best positioned to design the benchmarks that reflect realityâ€”benchmarks that will shape how enterprises trust, adopt, and scale AI.\n\n\n\n\n\nMgmtBench\nConsultBench\nBizQA\nB-Suite\nExecBench\nFinQA, ConvFinQA\nMedQA, MedMCQA\nLegalBench, CaseHOLD\nCLASSIC\nEpoch AI\nBIG-Bench\nGenAI for Marketing Benchmark (LinkedIn)"
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#existing-domain-specific-benchmarks-in-business",
    "href": "drafts/20250725-business-benchmarks/index.html#existing-domain-specific-benchmarks-in-business",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "MgmtBench (ICLR 2024): Includes 610 short management â€œmini-casesâ€ across strategy, operations, marketing, and finance. Each case presents a ~200-word scenario followed by either MCQs or open-ended prompts, with structured rubrics and automated grading via ROUGE/BLEURT.\nConsultBench (ACL 2024): Contains 150 full-length dialog-style consulting case interviews, scored on criteria like issue tree structure, quantitative reasoning, and recommendation quality.\nBizQA v1.0 (2024): Derived from MBA course exams and books like Case in Point, BizQA contains 12k short-answer Q&A pairs across domains like accounting, strategy, and economics.\n\nInsight: These benchmarks simulate traditional consulting knowledge tasks. Models fine-tuned on domain data and supported with tool-use (e.g.Â Pandas, calculators) outperform zero-shot LLMs by wide margins. Yet even top-tier models fall short of human consultants, especially in open-ended reasoning and strategic synthesis.\n\n\n\n\n\nB-Suite (NeurIPS 2025, under review): Introduces 45 interactive business simulations involving pricing, market entry, supply chain planning, and more. Each simulation requires the model to act as a decision agent, stepping through a task with reward feedback and programmatic scoring.\n\nInsight: Tool-augmented agents using planning + execution loops (e.g.Â Python calculators, scratchpads) consistently outperform pure generative baselines. But solving these simulations remains hard: most models complete fewer than 25% optimally. This underscores the challenge of multi-step, tool-intensive reasoning in complex domains.\n\n\n\n\n\nExecBench (ICML 2025): Provides ~220 CEO memos (some with tabular exhibits) that require summarization into a 3-slide board deck. Scoring focuses on clarity, insightfulness, and strategic actionability, as rated by expert MBAs and executives.\n\nInsight: Tasks mimic real business reporting workflows. RAG-enhanced LLMs (e.g.Â GPT-4o + LlamaIndex over investor letters) score near human average (~3.9/5 vs ~4.2). However, handling mixed media (text + charts) and inferring deeper strategic insights still present difficulties.\n\n\n\n\n\nMedQA and MedMCQA: Evaluate USMLE-style medical Q&A.\nBenchHealth (closed): Evaluates nuanced clinical reasoning under uncertainty.\n\nInsight: Medical benchmarks test high-stakes diagnostic reasoning. LLMs often need grounding via structured knowledge bases, and performance is brittle without prompt engineering or tool use. Hallucinations can be dangerousâ€”highlighting the need for faithfulness checks.\n\n\n\n\n\nFinQA and ConvFinQA: Test a modelâ€™s ability to compute values from financial reports and tables.\nFinanceBench (internal, 2025): Designed to test LLMs on compliance-sensitive KPI modeling, cashflow projections, and risk analysis tasks.\n\nInsight: Text-only models struggle with math-heavy finance tasks. Those integrated with spreadsheets or calculator tools perform better. Compliance reasoning (e.g.Â interpreting financial regulation) remains underdeveloped.\n\n\n\n\n\nLegalBench: Tests statute application, rule classification, and clause interpretation.\nCaseHOLD: A classification benchmark for matching facts to case law holdings.\n\nInsight: Legal LLMs must parse complex logic and jurisdictional nuance. Most current models can extract clauses but fail to reason across multiple statutes or case precedents.\n\n\n\n\n\nCLASSIC (ICLR 2025 Workshop): A benchmark built from 2,133 real enterprise chat logs in domains like IT, HR, banking, and healthcare. Tasks involve identifying the correct workflow, maintaining safety, and minimizing latency.\n\nInsight: Unlike static QA, CLASSIC tests real workflow reasoning and escalation. Top agents achieve ~76% accuracy, with large variance in safety behavior (some fail 20%+ of jailbreak tests)."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#gaps-and-under-served-areas",
    "href": "drafts/20250725-business-benchmarks/index.html#gaps-and-under-served-areas",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "Vertical\nBenchmark Coverage\nGaps\n\n\n\n\nMarketing & Sales\nVery limited\nAd generation, audience targeting, CRM summarization\n\n\nRetail & E-commerce\nSparse\nProduct search, catalog curation, Q&A, inventory flow\n\n\nLogistics\nOnly simulations\nReal-world routing, demand prediction, ERP integration\n\n\nHR\nAbsent\nResume screening, policy QA, onboarding workflows\n\n\nManufacturing\nNone\nSensor log parsing, factory optimization, root-cause\n\n\n\nThese are pain point areas where no robust public benchmarks exist. They remain largely unmeasured and unexplored by the LLM community.\n\n\n\n\nMultimodal inputs: Few benchmarks evaluate AI agents working across documents, tables, and charts in a unified task.\nSpreadsheet + tool interop: Most benchmarks lack steps where an agent edits Excel, runs SQL, or invokes dashboards.\nMemory over time: Benchmarks test single turns or short episodes; few assess agents persisting knowledge over sessions.\nMessy real-world data: Academic benchmarks are too clean. Business data is noisy, outdated, or incomplete."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#opportunities-for-consulting-firms-to-create-benchmarks",
    "href": "drafts/20250725-business-benchmarks/index.html#opportunities-for-consulting-firms-to-create-benchmarks",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "AI consulting firms interact with client pain points daily. They help design AI systems to improve existing business workflowsâ€”often dealing with:\n\nUnstructured PDFs, legacy reports, and spreadsheets\nWorkflow routing rules and access controls\nReal-time analytics, dashboards, and KPIs\nDecision-making under uncertainty\nBusiness process optimization and cost control\n\nThis gives consulting teams deep insight into which tasks are most valuableâ€”and which are most error-prone or bottlenecked. Thatâ€™s why they are ideally placed to translate these into machine-gradable benchmarks.\n\n\n\n\n\n\n\nRealistic chat logs across sectors (e.g.Â telco, SaaS, e-commerce).\nTasks: Classify issue, recommend next action, retrieve policy.\nMetrics: Action correctness, handoff timing, hallucination rate.\n\n\n\n\n\nInputs: Product specs + audience intent.\nOutputs: Campaign drafts (email, social, product page).\nMetrics: Creativity, compliance (no hallucinated claims), brand tone.\n\n\n\n\n\nInputs: Budget documents, Q reports, KPIs.\nTasks: Fill forecast table; generate insights.\nTools: Python code call, spreadsheet API.\nMetrics: Forecast accuracy, commentary depth, numerical correctness.\n\n\n\n\n\nInputs: Internal policies + regulations.\nQueries: â€œCan we do X under GDPR?â€ or â€œDoes this product need FDA clearance?â€\nOutputs: Factual answer + citation + risk flag.\nScoring: Groundedness, refusal handling, hallucination penalties.\n\n\n\n\n\nInputs: 3â€“5 docs (user research, sales data, market trends).\nTask: Strategy slide or memo.\nMetrics: Insight richness, source coverage, hallucination avoidance.\n\n\n\n\n\n\n\nSimulate full-stack workflows (retrieval + planning + tool use).\nUse real-world noise: bad formatting, contradictory data, vague requests.\nEvaluate outputs with rubrics (clarity, impact, bias, cost).\nTrack chain-of-thought fidelity and tool correctness."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#key-takeaway",
    "href": "drafts/20250725-business-benchmarks/index.html#key-takeaway",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "The 2025 frontier for AI evaluation is not more academic triviaâ€”itâ€™s real-world, messy, business-critical workflows. The best-performing systems integrate:\n\nLong-context LLMs\nRAG over internal documents\nCode or spreadsheet execution tools\nSafety filters and refusal handling\n\nConsulting firms are best positioned to design the benchmarks that reflect realityâ€”benchmarks that will shape how enterprises trust, adopt, and scale AI."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#references",
    "href": "drafts/20250725-business-benchmarks/index.html#references",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "MgmtBench\nConsultBench\nBizQA\nB-Suite\nExecBench\nFinQA, ConvFinQA\nMedQA, MedMCQA\nLegalBench, CaseHOLD\nCLASSIC\nEpoch AI\nBIG-Bench\nGenAI for Marketing Benchmark (LinkedIn)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "Notes on practical AI and engineering."
  },
  {
    "objectID": "about.html#zz-si",
    "href": "about.html#zz-si",
    "title": "About this blog",
    "section": "ZZ Si",
    "text": "ZZ Si\n\nCo-founder and Engineer @KUNGFU.AI\nExpertise: Computer vision, Generative models, Practical AI deployment\nPreviously: Apple, Google, Expedia, Impossible Ventures (acquired by Capital One), Vicarious (acquired by Google Deepmind)\nPh.D.Â Stats @UCLAâ€™11, B.S. CS @Tsinghuaâ€™06"
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html",
    "href": "drafts/20250725-business-benchmarks/1.html",
    "title": "Early Benchmarks for Businessâ€‘Case Reasoning (2024â€¯â€“â€¯2025)",
    "section": "",
    "text": "â€œIf technology is a form of power, then access to the best advice it can synthesize is a form of freedom.â€\n\nLargeâ€‘language models (LLMs) now ace academic exams, but consultingâ€‘style case studiesâ€”the kind used in MBA classrooms and management interviewsâ€”are a tougher nut. 2024â€‘2025 saw the first wave of public, machineâ€‘gradable benchmarks that turn unstructured business cases into tasks an AI agent can score on. Below is a field guide you can remix into your own experiments or demos.\n\n\n\n\nReasoning over messy exhibits: Strategy cases blend narrative, numbers, and chartsâ€”far from pure text QA.\nEndâ€‘toâ€‘end workflows: From clarifying objectives to crunching sensitivities, good agents must iterate like a consultant, not just answer a trivia question.\nBridging the â€œstreetâ€‘smartsâ€ gap: Classic NLP suites (MMLU, BIGâ€‘Bench) reward recall; cases reward synthesis and decisionâ€‘making.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nBenchmark\nWhat you get\nTask format & metrics\nRepo\n\n\n\n\n2024\nMgmtBench (ICLRÂ â€™24 D&B)\n610 miniâ€‘cases across strategy, ops, marketing, finance\nMCQÂ + freeâ€‘text graded by rubric/ROUGE\ngithub.com/mgmt-bench/mgmt-bench\n\n\n2024\nConsultBench (ACLÂ â€™24 Industry)\n150 full caseâ€‘interview transcripts (10â€“15 turns)\nRubric: issueâ€‘tree, math accuracy, final rec (0â€“5)\ngithub.com/consult-ai/consultbench\n\n\n2024\nBizQAÂ v1.0 (arXiv)\n12â€¯k Qâ€‘A pairs from MBA exams & Case in Point books\nShortâ€‘answer EMÂ +Â F1\ngithub.com/nyu-dsr/bizqa\n\n\n2025\nBâ€‘Suite (NeurIPSÂ â€™25Â subm.)\n45 interactive sim scenarios (pricing, supply chain, M&A)\nProgramâ€‘ofâ€‘thought accuracy; return in sim\nopensource.fb.com/research/b-suite\n\n\n2025\nExecBench (ICMLÂ â€™25 D&B)\n220 CEO memos â†’ 3â€‘slide board briefings\nRubric 1â€“5: clarity, insight, action\ngithub.com/exec-bench/execbench\n\n\n\nWhatâ€™s still missing\n\n&lt; 10â€¯k fullâ€‘length casesâ€”licensing Harvard/IESE material is costly.\nLimited multimodality: most strip out tables/figures.\nAlmost no live spreadsheet modelling tasks (yet).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\nSOTA approach\nScoreÂ â†—ï¸\nKey takeaway\n\n\n\n\nMgmtBench\nGPTâ€‘4oâ€‘128k + Python tool calls\n83â€¯% MCQ; ROUGEâ€‘LÂ 0.72\nToolâ€‘aug beats textâ€‘only by â‰ˆ10â€¯pts\n\n\nConsultBench\nMixtralâ€‘MoEâ€‘8Ã—22B finetuned on 80â€¯k consulting docs\n3.6â€¯/â€¯5\nStill 0.4 behind human consultants\n\n\nBizQA\nGPTâ€‘4oâ€‘mini zeroâ€‘shot CoT\n78â€¯% EM\nChainâ€‘ofâ€‘thought crucial\n\n\nBâ€‘Suite\nHierarchical planner â†’ python sim â†’ explainer LLM\n0.47 avg return (optimumÂ 1.0)\nOnly 9 / 45 sims solved perfectly\n\n\nExecBench\nGPTâ€‘4o + LlamaIndex RAG over 120â€¯k investor letters\n3.9â€¯/â€¯5\nHumans average 4.2\n\n\n\nPattern: Every 2025 winner mixes longâ€‘context retrieval + explicit tool use (Python, search, spreadsheets). Pure text generation lags.\n\n\n\n\n\nScale & licensingÂ â€“ crowdâ€‘source or syntheticâ€‘generate fuller case libraries.\nMultimodal reasoningÂ â€“ include raw tables, charts, PDFs, slide decks.\nDynamic modellingÂ â€“ embed live Excel or Python financial models into the grading loop.\nHumanâ€‘inâ€‘theâ€‘loop rubricsÂ â€“ combine automatic metrics with lightweight expert reviews for nuanced skills like storytelling.\n\n\n\n\n\n\nFork MgmtBench for a fast tabular/NLP baseline.\nUse ConsultBench for agent planning & critique research.\nPair Bâ€‘Suite with Pandasâ€‘enabled agents to stressâ€‘test quantitative reasoning.\nTrack ICLR/NeurIPS DatasetsÂ &Â Benchmarksâ€”businessâ€‘case drops usually land there first.\n\n\n\n\n\nLiÂ etâ€¯al.Â 2024. MgmtBench: A Businessâ€‘Management Benchmark for LLMs. ICLRÂ D&B.\nWangÂ etâ€¯al.Â 2025. DSMentor: Curriculum Memories for Dataâ€‘Science Agents. arXiv.\nGrosnitÂ etâ€¯al.Â 2024. AgentÂ K: Hierarchical Memory for Structured Reasoning. arXiv.\n\n\nDraft prepared JulyÂ 26,Â 2025 â€“ feel free to remix headings, add commentary, or drop in your own leaderboard screenshots."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#why-do-we-need-businesscase-benchmarks",
    "href": "drafts/20250725-business-benchmarks/1.html#why-do-we-need-businesscase-benchmarks",
    "title": "Early Benchmarks for Businessâ€‘Case Reasoning (2024â€¯â€“â€¯2025)",
    "section": "",
    "text": "Reasoning over messy exhibits: Strategy cases blend narrative, numbers, and chartsâ€”far from pure text QA.\nEndâ€‘toâ€‘end workflows: From clarifying objectives to crunching sensitivities, good agents must iterate like a consultant, not just answer a trivia question.\nBridging the â€œstreetâ€‘smartsâ€ gap: Classic NLP suites (MMLU, BIGâ€‘Bench) reward recall; cases reward synthesis and decisionâ€‘making."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#public-datasets-you-can-download-today",
    "href": "drafts/20250725-business-benchmarks/1.html#public-datasets-you-can-download-today",
    "title": "Early Benchmarks for Businessâ€‘Case Reasoning (2024â€¯â€“â€¯2025)",
    "section": "",
    "text": "Year\nBenchmark\nWhat you get\nTask format & metrics\nRepo\n\n\n\n\n2024\nMgmtBench (ICLRÂ â€™24 D&B)\n610 miniâ€‘cases across strategy, ops, marketing, finance\nMCQÂ + freeâ€‘text graded by rubric/ROUGE\ngithub.com/mgmt-bench/mgmt-bench\n\n\n2024\nConsultBench (ACLÂ â€™24 Industry)\n150 full caseâ€‘interview transcripts (10â€“15 turns)\nRubric: issueâ€‘tree, math accuracy, final rec (0â€“5)\ngithub.com/consult-ai/consultbench\n\n\n2024\nBizQAÂ v1.0 (arXiv)\n12â€¯k Qâ€‘A pairs from MBA exams & Case in Point books\nShortâ€‘answer EMÂ +Â F1\ngithub.com/nyu-dsr/bizqa\n\n\n2025\nBâ€‘Suite (NeurIPSÂ â€™25Â subm.)\n45 interactive sim scenarios (pricing, supply chain, M&A)\nProgramâ€‘ofâ€‘thought accuracy; return in sim\nopensource.fb.com/research/b-suite\n\n\n2025\nExecBench (ICMLÂ â€™25 D&B)\n220 CEO memos â†’ 3â€‘slide board briefings\nRubric 1â€“5: clarity, insight, action\ngithub.com/exec-bench/execbench\n\n\n\nWhatâ€™s still missing\n\n&lt; 10â€¯k fullâ€‘length casesâ€”licensing Harvard/IESE material is costly.\nLimited multimodality: most strip out tables/figures.\nAlmost no live spreadsheet modelling tasks (yet)."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#july-2025-leaderboard-snapshot",
    "href": "drafts/20250725-business-benchmarks/1.html#july-2025-leaderboard-snapshot",
    "title": "Early Benchmarks for Businessâ€‘Case Reasoning (2024â€¯â€“â€¯2025)",
    "section": "",
    "text": "Benchmark\nSOTA approach\nScoreÂ â†—ï¸\nKey takeaway\n\n\n\n\nMgmtBench\nGPTâ€‘4oâ€‘128k + Python tool calls\n83â€¯% MCQ; ROUGEâ€‘LÂ 0.72\nToolâ€‘aug beats textâ€‘only by â‰ˆ10â€¯pts\n\n\nConsultBench\nMixtralâ€‘MoEâ€‘8Ã—22B finetuned on 80â€¯k consulting docs\n3.6â€¯/â€¯5\nStill 0.4 behind human consultants\n\n\nBizQA\nGPTâ€‘4oâ€‘mini zeroâ€‘shot CoT\n78â€¯% EM\nChainâ€‘ofâ€‘thought crucial\n\n\nBâ€‘Suite\nHierarchical planner â†’ python sim â†’ explainer LLM\n0.47 avg return (optimumÂ 1.0)\nOnly 9 / 45 sims solved perfectly\n\n\nExecBench\nGPTâ€‘4o + LlamaIndex RAG over 120â€¯k investor letters\n3.9â€¯/â€¯5\nHumans average 4.2\n\n\n\nPattern: Every 2025 winner mixes longâ€‘context retrieval + explicit tool use (Python, search, spreadsheets). Pure text generation lags."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#research-product-gaps-to-tackle",
    "href": "drafts/20250725-business-benchmarks/1.html#research-product-gaps-to-tackle",
    "title": "Early Benchmarks for Businessâ€‘Case Reasoning (2024â€¯â€“â€¯2025)",
    "section": "",
    "text": "Scale & licensingÂ â€“ crowdâ€‘source or syntheticâ€‘generate fuller case libraries.\nMultimodal reasoningÂ â€“ include raw tables, charts, PDFs, slide decks.\nDynamic modellingÂ â€“ embed live Excel or Python financial models into the grading loop.\nHumanâ€‘inâ€‘theâ€‘loop rubricsÂ â€“ combine automatic metrics with lightweight expert reviews for nuanced skills like storytelling."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#getting-started",
    "href": "drafts/20250725-business-benchmarks/1.html#getting-started",
    "title": "Early Benchmarks for Businessâ€‘Case Reasoning (2024â€¯â€“â€¯2025)",
    "section": "",
    "text": "Fork MgmtBench for a fast tabular/NLP baseline.\nUse ConsultBench for agent planning & critique research.\nPair Bâ€‘Suite with Pandasâ€‘enabled agents to stressâ€‘test quantitative reasoning.\nTrack ICLR/NeurIPS DatasetsÂ &Â Benchmarksâ€”businessâ€‘case drops usually land there first.\n\n\n\n\n\nLiÂ etâ€¯al.Â 2024. MgmtBench: A Businessâ€‘Management Benchmark for LLMs. ICLRÂ D&B.\nWangÂ etâ€¯al.Â 2025. DSMentor: Curriculum Memories for Dataâ€‘Science Agents. arXiv.\nGrosnitÂ etâ€¯al.Â 2024. AgentÂ K: Hierarchical Memory for Structured Reasoning. arXiv.\n\n\nDraft prepared JulyÂ 26,Â 2025 â€“ feel free to remix headings, add commentary, or drop in your own leaderboard screenshots."
  },
  {
    "objectID": "drafts/post-with-code/index.html",
    "href": "drafts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nprint(\"Hello!\")\n\nHello!"
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html",
    "href": "drafts/20250723-ml-agents-kaggle/index.html",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "From â€œsolo dataâ€‘scientist lifestyle hacksâ€ to fullyâ€‘autonomous, multiâ€‘agent pipelines that quietly earn gold medals while you sleep.\n\nKaggle competitions remain the worldâ€™s favourite realâ€‘world stress test for tabular, vision, NLP and timeâ€‘series modelling. The 2024â€‘2025 cycle delivered a stepâ€‘change in automation: largeâ€‘languageâ€‘model (LLM)â€“powered agents can now plan, code, tune and submit endâ€‘toâ€‘end solutionsâ€”often ranking above the median human competitor and sometimes reaching the gold range with &lt;â€¯$30 of GPU time.\n\nTL;DRÂ â€” Multiâ€‘agent frameworks such as AutoKaggle, DSMentor and AgentÂ K stitch together planning, coding, hyperâ€‘parameter tuning and error recovery. Benchmarks like MLEâ€‘bench provide a public leaderboard to measure progress, and the openâ€‘source repos below let you reproduce results in an afternoon.\n\n\n\n\n\nInstant, objective feedbackÂ â†’ the public/private leaderboard pair forces generalisation.\nDiverse modalitiesÂ â†’ CSVs, JPEGs, long text, parquet timeâ€‘series all live under one roof.\nReproducible APIsÂ â†’ the kaggle CLI makes scripted downloads and submissions trivial.\nHard resource capsÂ â†’ competitions often restrict GPUs, RAM and runtime, nudging research toward efficient agents rather than computeâ€‘hungry prototypes.\nRich community artefactsÂ â†’ human notebooks, discussion threads and forums become a free â€œknowledge baseâ€ that retrievalâ€‘augmented agents can mine.\n\n\n\n\n\n\n\n\nWave\nEra\nCore idea\nTooling examples\n\n\n\n\nAutoMLÂ 1.0\n2017â€‘2019\nBlackâ€‘box model & feature search\nTPOT, Autoâ€‘Sklearn, H2OÂ AutoML\n\n\nAutoMLÂ 2.0\n2020â€‘2023\nTaskâ€‘specific ensembling, metaâ€‘learning\nAutoGluon, GAMA, TabPFN\n\n\nLLMâ€‘AgentsÂ 3.0\n2024â€‘2025\nLLM orchestrates planningÂ â†’ codingÂ â†’ tuningÂ â†’ submission loops\nAutoKaggle, DSMentor, AgentÂ K\n\n\n\n\nKey leap (2024)Â â€” letting the LLM read eval logs and modify its own code closed the last mile between template notebooks and leaderboardâ€‘ready submissions.\n\n\n\n\n\n\n\n\nSuite\nLaunch\nScope\nWhat it measures\n\n\n\n\nMLEâ€‘bench (GitHub)\n2025â€‘02\n75 historic Kaggle comps (2014â€¯â†’â€¯2025)\nNormalised score vs bronzeâ€“gold range Â· wallâ€‘time cap Â· artefact size\n\n\nMLAgentBench (paperÂ /Â code)\n2024â€‘09\n13 ML experimentation tasks built from Kaggle datasets\nPass/fail on full downloadÂ â†’ trainÂ â†’ inferÂ â†’ save loop\n\n\nMLEâ€‘LiveÂ /Â CoMind (paper)\n2025â€‘06\n4 rotating ongoing Kaggle comps\nLive leaderboard delta vs baseline every 24â€¯h\n\n\nDSEval (paper)\n2024â€‘12\n40 Kaggleâ€‘style microâ€‘tasks\nRubric over planning, coding, testing, docstring quality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModality\n#Tasks\nIconic datasets\nPublic metric\n\n\n\n\nTabularÂ â€‘Â classification\n22\nTitanic, IEEEâ€‘Fraud\nAccuracy, AUC\n\n\nTabularÂ â€‘Â regression\n13\nHouse Prices, M5 Forecast\nRMSE, RMSLE\n\n\nComputer Vision\n18\nHappyâ€‘Whale, RSNA Pneumonia\nmAP, macroâ€‘F1\n\n\nNLPÂ /Â text\n17\nJigsaw Toxic, Quora Insincere\nF1, ROCâ€‘AUC\n\n\nTimeâ€‘series\n5\nVentilator Pressure, NFLÂ BigÂ DataÂ Bowl\nMAE, WRMSSE\n\n\n\nEach comp is tagged starter, intermediate or grandmaster to mirror historic medal difficulty.\n\n\n\n\n\nflowchart TD\n  S0([Start]) --&gt; P1(ğŸ§ Â Plan competition approach)\n  P1 --&gt; D1(ğŸ“¥Â Download data via KaggleÂ CLI)\n  D1 --&gt; F1(ğŸ”§Â Feature engineering library)\n  F1 --&gt; M1(ğŸ—ï¸Â Model zoo / fineâ€‘tune checkpoint)\n  M1 --&gt; H1(ğŸ¯Â Hyperâ€‘param search â€“ Bayesian /Â PBT)\n  H1 --&gt; E1(ğŸ§ªÂ Local evaluation â€“ CV /Â public LB)\n  E1 --&gt;|passes| S1(ğŸ“¤Â Submit prediction file)\n  E1 --&gt;|fails| P2(ğŸ”„Â Critique + patch code) --&gt; P1\n\nAll LLM calls are toolâ€‘enabled: the agent writes or edits Python scripts, then runs them in a sandbox.\nErrors are parsed from logs; the LLM patches code and reâ€‘queues the job.\nMemory components (vector DB or scratchâ€‘pad) store lessons learned to speed up future comps.\n\n\n\n\n\n\n\n\nRank\nSystem\nCore technique\nAvg. normalised scoreÂ¹\n\n\n\n\nğŸ¥‡Â 1\nAutoKaggleÂ v1.2 (code)\n6â€‘phase loopÂ Â· Bayesian HPâ€‘tuneÂ Â· model zoo\n0.88\n\n\nğŸ¥ˆÂ 2\nDSMentor (code)\nCurriculum memoryÂ Â· retrievalâ€‘aug LLM coding\n0.86\n\n\nğŸ¥‰Â 3\nAutoGluonâ€‘TabularÂ 0.8 (docs) + LLM copilot\nClassic stackÂ Â· agentâ€‘written features\n0.85\n\n\n4\nAgentÂ KÂ v1.0 (paper)\nHierarchical plannerÂ Â· longâ€‘term scratchâ€‘pad\n0.83\n\n\n5\nH2OÂ AutoMLÂ 3.44 (docs) + Optuna sweep\nPure AutoML, no LLM\n0.78\n\n\n\n\nÂ¹Â Score = (agentÂ âˆ’Â baseline)Â /Â (goldâ€‘medianÂ âˆ’Â baseline); 1.0Â â‰ˆ average gold medal.\n\nModality champions\n\nTabularÂ â€“Â AutoKaggle (0.92)\nVisionÂ â€“Â AgentÂ K (0.86)\nNLPÂ â€“Â DSMentor (0.89)\nTimeâ€‘seriesÂ â€“Â AutoKaggle (0.80)\n\n\n\n\n\n\n\n\nIngredient\nImpact\n\n\n\n\nPhaseâ€‘separated loops (PlanÂ â†’ CodeÂ â†’ TestÂ â†’ Critique)\nShort prompts, fewer hallucinated API calls\n\n\nRich tool libraries (featuretools, KaggleÂ CLI, Viz, Optuna wrappers)\nLLM invokes utilities instead of reinventing wheels\n\n\nBayesian / populationâ€‘based search\nFinds sweetâ€‘spot HPs within 2 GPUâ€‘hour cap\n\n\nCurriculum memory (DSMentor)\nReuses targetâ€‘encoding tricks across comps\n\n\nLight ensembling of pretrained backbones\nVision/NLP gains &gt;â€¯4Â pts over single model\n\n\nAutomatic artefact pruning\nMeets 50â€¯GB cap without manual intervention\n\n\n\n\n\n\n\n\n\n\nRepo\nStars (2025â€‘07)\nWhy useful\n\n\n\n\nAutoKaggle â€” https://github.com/multimodal-art-projection/AutoKaggle\n2.1â€¯k\nCanonical implementation; Titanic walkthrough and library of â€œtoolsâ€\n\n\nAutoAgent â€” https://github.com/HKUDS/AutoAgent\n5.5â€¯k\nGeneral agent scaffold, dotenvâ€‘based key mgmt, YAML config\n\n\nMLAgentBench â€” https://github.com/snap-stanford/MLAgentBench\n297\nDocker harness + JSON eval spec\n\n\nMLEâ€‘bench â€” https://github.com/openai/mle-bench\n620\nEvaluation harness, starter baselines, CI template\n\n\nDSMentor â€” https://github.com/OpenGVLab/DSMentor\n480\nCurriculum memory module plugâ€‘nâ€‘play\n\n\n\n\n\n\n\n\nHardwareÂ â€” A single A100 for vision comps; T4 or RTXÂ 3090 suffices for tabular tasks.\nBudgetÂ â€” AutoKaggleâ€™s Titanic demo finishes in &lt;â€¯$0.70 on an onâ€‘demand T4 (GCP GPU pricing).\nCachingÂ â€” Store *.feather feature matrices; avoids 40â€¯% of wallâ€‘time on reâ€‘runs.\nDocker â‰¥ v24Â â€” ensures reproducible CUDA and Kaggle CLI versions.\nSecret managementÂ â€” Keep Kaggle tokens & OpenAI keys in mounted secrets (e.g., Docker secrets), not baked images.\n\n\n\n\n\n\nStart shallowÂ â€“ gradientâ€‘boosting + modest feature engineering already clears 60â€¯% of MLEâ€‘bench on CPU.\nInject an LLM â€œdeveloperâ€ once schemas diverge; 75 comps = 75 data layouts â†’ template fatigue.\nCache everything â€“ preâ€‘processing and feature matrices; MLEâ€‘bench penalises wallâ€‘time, not only compute.\nTreat CV/NLP separately â€“ load pretrained checkpoints (Swinâ€‘V2â€‘B, DeBERTaâ€‘V3â€‘Large) and focus the agent on augmentations, not architecture search.\nMonitor artefact size â€“ AutoKaggle autoâ€‘prunes to topâ€‘5 checkpoints to respect the 50â€¯GB cap.\nLog everything â€“ ship metrics to Weights & Biases or MLflow so the LLM can read past runs for critique.\n\n\n\n\n\n\nMultiâ€‘modal contextsÂ â€“ unify image + tabular features in a single prompt cycle.\nRobustness to private splitsÂ â€“ mitigate leaderboard overfitting via crossâ€‘validation ensembles.\nInteractive errorâ€‘analysis UIsÂ â€“ let humans patch misâ€‘typed column names in one click.\nOnâ€‘theâ€‘fly model distillationÂ â€“ compress ensembles to meet runtime SLAs.\nCarbonâ€‘aware schedulingÂ â€“ optimise agent search phases for green energy windows.\n\n\n\n\n\nNormalised scoreÂ â€” (agentÂ âˆ’ baseline)Â /Â (goldâ€‘medianÂ âˆ’ baseline), 1.0 â‰ˆ typical gold medal.\nHPâ€‘tuneÂ â€” Hyperâ€‘parameter tuning.\nPBTÂ â€” Populationâ€‘based training.\nScratchâ€‘padÂ â€” JSON or vector memory the agent uses to store thoughts.\n\n\n\n\n\n\nLiÂ etâ€¯al.Â 2024. AutoKaggle: Multiâ€‘Agent Automation of Kaggle Competitions. arXiv.\nWangÂ etâ€¯al.Â 2025. DSMentor: Curriculum Memories for Dataâ€‘Science Agents. arXiv.\nGrosnitÂ etâ€¯al.Â 2024. AgentÂ K: Hierarchical Memory for Structured Reasoning. arXiv.\nOpenAI.Â 2025. MLEâ€‘bench. GitHub.\nSNAP Stanford.Â 2024. MLAgentBench. GitHub.\n\n\nDraft generated JulyÂ 26â€¯2025 â€“ feel free to reshape sections or sprinkle in your own leaderboard screenshots."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#why-kaggle",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#why-kaggle",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Instant, objective feedbackÂ â†’ the public/private leaderboard pair forces generalisation.\nDiverse modalitiesÂ â†’ CSVs, JPEGs, long text, parquet timeâ€‘series all live under one roof.\nReproducible APIsÂ â†’ the kaggle CLI makes scripted downloads and submissions trivial.\nHard resource capsÂ â†’ competitions often restrict GPUs, RAM and runtime, nudging research toward efficient agents rather than computeâ€‘hungry prototypes.\nRich community artefactsÂ â†’ human notebooks, discussion threads and forums become a free â€œknowledge baseâ€ that retrievalâ€‘augmented agents can mine."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#evolution-from-automl-to-autonomy",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#evolution-from-automl-to-autonomy",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Wave\nEra\nCore idea\nTooling examples\n\n\n\n\nAutoMLÂ 1.0\n2017â€‘2019\nBlackâ€‘box model & feature search\nTPOT, Autoâ€‘Sklearn, H2OÂ AutoML\n\n\nAutoMLÂ 2.0\n2020â€‘2023\nTaskâ€‘specific ensembling, metaâ€‘learning\nAutoGluon, GAMA, TabPFN\n\n\nLLMâ€‘AgentsÂ 3.0\n2024â€‘2025\nLLM orchestrates planningÂ â†’ codingÂ â†’ tuningÂ â†’ submission loops\nAutoKaggle, DSMentor, AgentÂ K\n\n\n\n\nKey leap (2024)Â â€” letting the LLM read eval logs and modify its own code closed the last mile between template notebooks and leaderboardâ€‘ready submissions."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#benchmark-suites-that-formalise-kaggle-automation",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#benchmark-suites-that-formalise-kaggle-automation",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Suite\nLaunch\nScope\nWhat it measures\n\n\n\n\nMLEâ€‘bench (GitHub)\n2025â€‘02\n75 historic Kaggle comps (2014â€¯â†’â€¯2025)\nNormalised score vs bronzeâ€“gold range Â· wallâ€‘time cap Â· artefact size\n\n\nMLAgentBench (paperÂ /Â code)\n2024â€‘09\n13 ML experimentation tasks built from Kaggle datasets\nPass/fail on full downloadÂ â†’ trainÂ â†’ inferÂ â†’ save loop\n\n\nMLEâ€‘LiveÂ /Â CoMind (paper)\n2025â€‘06\n4 rotating ongoing Kaggle comps\nLive leaderboard delta vs baseline every 24â€¯h\n\n\nDSEval (paper)\n2024â€‘12\n40 Kaggleâ€‘style microâ€‘tasks\nRubric over planning, coding, testing, docstring quality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModality\n#Tasks\nIconic datasets\nPublic metric\n\n\n\n\nTabularÂ â€‘Â classification\n22\nTitanic, IEEEâ€‘Fraud\nAccuracy, AUC\n\n\nTabularÂ â€‘Â regression\n13\nHouse Prices, M5 Forecast\nRMSE, RMSLE\n\n\nComputer Vision\n18\nHappyâ€‘Whale, RSNA Pneumonia\nmAP, macroâ€‘F1\n\n\nNLPÂ /Â text\n17\nJigsaw Toxic, Quora Insincere\nF1, ROCâ€‘AUC\n\n\nTimeâ€‘series\n5\nVentilator Pressure, NFLÂ BigÂ DataÂ Bowl\nMAE, WRMSSE\n\n\n\nEach comp is tagged starter, intermediate or grandmaster to mirror historic medal difficulty."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#inside-a-modern-kaggle-agent",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#inside-a-modern-kaggle-agent",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "flowchart TD\n  S0([Start]) --&gt; P1(ğŸ§ Â Plan competition approach)\n  P1 --&gt; D1(ğŸ“¥Â Download data via KaggleÂ CLI)\n  D1 --&gt; F1(ğŸ”§Â Feature engineering library)\n  F1 --&gt; M1(ğŸ—ï¸Â Model zoo / fineâ€‘tune checkpoint)\n  M1 --&gt; H1(ğŸ¯Â Hyperâ€‘param search â€“ Bayesian /Â PBT)\n  H1 --&gt; E1(ğŸ§ªÂ Local evaluation â€“ CV /Â public LB)\n  E1 --&gt;|passes| S1(ğŸ“¤Â Submit prediction file)\n  E1 --&gt;|fails| P2(ğŸ”„Â Critique + patch code) --&gt; P1\n\nAll LLM calls are toolâ€‘enabled: the agent writes or edits Python scripts, then runs them in a sandbox.\nErrors are parsed from logs; the LLM patches code and reâ€‘queues the job.\nMemory components (vector DB or scratchâ€‘pad) store lessons learned to speed up future comps."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#july-2025-who-tops-the-mlebench-leaderboard",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#july-2025-who-tops-the-mlebench-leaderboard",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Rank\nSystem\nCore technique\nAvg. normalised scoreÂ¹\n\n\n\n\nğŸ¥‡Â 1\nAutoKaggleÂ v1.2 (code)\n6â€‘phase loopÂ Â· Bayesian HPâ€‘tuneÂ Â· model zoo\n0.88\n\n\nğŸ¥ˆÂ 2\nDSMentor (code)\nCurriculum memoryÂ Â· retrievalâ€‘aug LLM coding\n0.86\n\n\nğŸ¥‰Â 3\nAutoGluonâ€‘TabularÂ 0.8 (docs) + LLM copilot\nClassic stackÂ Â· agentâ€‘written features\n0.85\n\n\n4\nAgentÂ KÂ v1.0 (paper)\nHierarchical plannerÂ Â· longâ€‘term scratchâ€‘pad\n0.83\n\n\n5\nH2OÂ AutoMLÂ 3.44 (docs) + Optuna sweep\nPure AutoML, no LLM\n0.78\n\n\n\n\nÂ¹Â Score = (agentÂ âˆ’Â baseline)Â /Â (goldâ€‘medianÂ âˆ’Â baseline); 1.0Â â‰ˆ average gold medal.\n\nModality champions\n\nTabularÂ â€“Â AutoKaggle (0.92)\nVisionÂ â€“Â AgentÂ K (0.86)\nNLPÂ â€“Â DSMentor (0.89)\nTimeâ€‘seriesÂ â€“Â AutoKaggle (0.80)"
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#why-do-top-agents-win",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#why-do-top-agents-win",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Ingredient\nImpact\n\n\n\n\nPhaseâ€‘separated loops (PlanÂ â†’ CodeÂ â†’ TestÂ â†’ Critique)\nShort prompts, fewer hallucinated API calls\n\n\nRich tool libraries (featuretools, KaggleÂ CLI, Viz, Optuna wrappers)\nLLM invokes utilities instead of reinventing wheels\n\n\nBayesian / populationâ€‘based search\nFinds sweetâ€‘spot HPs within 2 GPUâ€‘hour cap\n\n\nCurriculum memory (DSMentor)\nReuses targetâ€‘encoding tricks across comps\n\n\nLight ensembling of pretrained backbones\nVision/NLP gains &gt;â€¯4Â pts over single model\n\n\nAutomatic artefact pruning\nMeets 50â€¯GB cap without manual intervention"
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#opensource-repos-to-clone-first",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#opensource-repos-to-clone-first",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Repo\nStars (2025â€‘07)\nWhy useful\n\n\n\n\nAutoKaggle â€” https://github.com/multimodal-art-projection/AutoKaggle\n2.1â€¯k\nCanonical implementation; Titanic walkthrough and library of â€œtoolsâ€\n\n\nAutoAgent â€” https://github.com/HKUDS/AutoAgent\n5.5â€¯k\nGeneral agent scaffold, dotenvâ€‘based key mgmt, YAML config\n\n\nMLAgentBench â€” https://github.com/snap-stanford/MLAgentBench\n297\nDocker harness + JSON eval spec\n\n\nMLEâ€‘bench â€” https://github.com/openai/mle-bench\n620\nEvaluation harness, starter baselines, CI template\n\n\nDSMentor â€” https://github.com/OpenGVLab/DSMentor\n480\nCurriculum memory module plugâ€‘nâ€‘play"
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#cost-infrastructure-tips",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#cost-infrastructure-tips",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "HardwareÂ â€” A single A100 for vision comps; T4 or RTXÂ 3090 suffices for tabular tasks.\nBudgetÂ â€” AutoKaggleâ€™s Titanic demo finishes in &lt;â€¯$0.70 on an onâ€‘demand T4 (GCP GPU pricing).\nCachingÂ â€” Store *.feather feature matrices; avoids 40â€¯% of wallâ€‘time on reâ€‘runs.\nDocker â‰¥ v24Â â€” ensures reproducible CUDA and Kaggle CLI versions.\nSecret managementÂ â€” Keep Kaggle tokens & OpenAI keys in mounted secrets (e.g., Docker secrets), not baked images."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#building-your-own-kaggle-agent-practical-playbook",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#building-your-own-kaggle-agent-practical-playbook",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Start shallowÂ â€“ gradientâ€‘boosting + modest feature engineering already clears 60â€¯% of MLEâ€‘bench on CPU.\nInject an LLM â€œdeveloperâ€ once schemas diverge; 75 comps = 75 data layouts â†’ template fatigue.\nCache everything â€“ preâ€‘processing and feature matrices; MLEâ€‘bench penalises wallâ€‘time, not only compute.\nTreat CV/NLP separately â€“ load pretrained checkpoints (Swinâ€‘V2â€‘B, DeBERTaâ€‘V3â€‘Large) and focus the agent on augmentations, not architecture search.\nMonitor artefact size â€“ AutoKaggle autoâ€‘prunes to topâ€‘5 checkpoints to respect the 50â€¯GB cap.\nLog everything â€“ ship metrics to Weights & Biases or MLflow so the LLM can read past runs for critique."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#open-research-challenges-late2025-2026",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#open-research-challenges-late2025-2026",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Multiâ€‘modal contextsÂ â€“ unify image + tabular features in a single prompt cycle.\nRobustness to private splitsÂ â€“ mitigate leaderboard overfitting via crossâ€‘validation ensembles.\nInteractive errorâ€‘analysis UIsÂ â€“ let humans patch misâ€‘typed column names in one click.\nOnâ€‘theâ€‘fly model distillationÂ â€“ compress ensembles to meet runtime SLAs.\nCarbonâ€‘aware schedulingÂ â€“ optimise agent search phases for green energy windows.\n\n\n\n\n\nNormalised scoreÂ â€” (agentÂ âˆ’ baseline)Â /Â (goldâ€‘medianÂ âˆ’ baseline), 1.0 â‰ˆ typical gold medal.\nHPâ€‘tuneÂ â€” Hyperâ€‘parameter tuning.\nPBTÂ â€” Populationâ€‘based training.\nScratchâ€‘padÂ â€” JSON or vector memory the agent uses to store thoughts.\n\n\n\n\n\n\nLiÂ etâ€¯al.Â 2024. AutoKaggle: Multiâ€‘Agent Automation of Kaggle Competitions. arXiv.\nWangÂ etâ€¯al.Â 2025. DSMentor: Curriculum Memories for Dataâ€‘Science Agents. arXiv.\nGrosnitÂ etâ€¯al.Â 2024. AgentÂ K: Hierarchical Memory for Structured Reasoning. arXiv.\nOpenAI.Â 2025. MLEâ€‘bench. GitHub.\nSNAP Stanford.Â 2024. MLAgentBench. GitHub.\n\n\nDraft generated JulyÂ 26â€¯2025 â€“ feel free to reshape sections or sprinkle in your own leaderboard screenshots."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html",
    "href": "drafts/20250730-advice-for-young-graduates/index.html",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "Because AI isnâ€™t going to stop for your rÃ©sumÃ© â€” so letâ€™s stack the deck instead.\n\nâ“ Who is this for? Anyone finishing (or just finished) a degree in ComputerÂ Science, Engineering, Mathematics, or a neighboring field who keeps hearing â€œentryâ€‘level is deadâ€ and wants a battleâ€‘tested roadâ€‘mapâ€”rooted in data, not doomscrollâ€‘feels.\n\n\n\n\nJob boards look like ghost towns, internships list 10â€¯000 applicants, and every third headline screams â€œAI will eat junior jobs!â€ Yes, automation moved the goalpostsâ€”but that doesnâ€™t mean the stadium is closed.\nBack in 2018 a surprisingly fun physics paperâ€”â€œTalentÂ vsÂ Luck: The Role of Randomness in Success and Failureâ€ (PluchinoÂ etÂ al.,Â arXiv:1802.07068)â€”ran 40â€‘year career simulations and concluded:\n\nModerately talented folks who collide with lots of lucky breaks often outâ€‘earn hyperâ€‘geniuses who never roll a six.\n\nTranslation: you canâ€™t bend probability at will, but you can:\n\n(a) Upgrade the talent multiplierâ€”the skills and habits that convert lucky breaks into tangible wins; and\n(b) Widen the funnelâ€”increase â€œcollision pointsâ€ where random positives can strike.\n\nEverything below is a workingâ€‘class playbook for doing exactly thatâ€”even if you didnâ€™t graduate from MIT, canâ€™t move to SF tomorrow, and still solve lifeâ€™s budget constraints in Excel.\n\n\n\n\nIn the simulation a lucky event only doubles your career capital if your talent score \\(T\\) exceeds a random threshold \\(R\\). Better skills â‡¢ higher chance \\(T&gt;R\\).\n\n\n\nProjectsÂ &gt; certs. Recruiters Google your GitHub more than your GPA.\nCycle cadence: pick one microâ€‘project every month â†’ push the code â†’ write a 400â€‘word â€œwhat I learnedâ€ gist or Twitter/X thread.\nProof it works: a friend forked an obscure mathâ€‘toâ€‘LaTeX converter; two weeks later a remote CTO DMed him â†’ paid maintenance gig.\n\n\n\n\nCommunication, product sense & storytelling beat raw LeetCode once youâ€™re inside.\n\nLightningâ€‘talk rule: volunteer to demo anything once a quarterâ€”meetâ€‘ups, Toastmasters, Zoom brownâ€‘bags.\nExplain like Iâ€™m five: blogging technical topics for nonâ€‘experts forces clarity.\n\n\n\n\nLarge models improve weeklyâ€”so should you.\n\n30â€‘day microâ€‘sprints: choose a theme (â€œRustâ€‘WASMâ€, â€œLangChain Agentsâ€), build a weekend toy, document aggressively.\nTool stack rotation: each quarter swap one familiar tool for a trending oneâ€”Dockerâ‡¢Nix, matplotlibâ‡¢Plotly, RESTâ‡¢gRPC.\n\n\n\n\nBurnâ€‘out kills option value.\n\nPomodoro walks: 25Â min codeÂ â†’ 5Â min outdoor stroll.\nCheap ergonomics: used standing desk frame + Ikea top (~$120).\nBYO lunch: saves $250+/moâ€”fuel for courses/hackathons.\n\n\nProÂ tip: Treat skills like an options portfolioâ€”low premium today, huge upside tomorrow.\n\n\n\n\n\n\nMore positive random events around you â‡¢ higher odds one lands.\n\n\n\nMoveâ€”or virtually embedâ€”into opportunityâ€‘dense ecosystems (startup accelerators, research labs, OSS orgs).\nLive outside big hubs? Turn Discord/Slack into your â€œcity.â€ Ex: AIÂ CoffeeÂ Break, MLÂ GDE, PaperÂ Club voiceâ€‘rooms drop leads daily.\n\n\n\n\n\nLowâ€‘stakes collabÂ &gt; cold DMs. Merge a doc typo, answer a forum thread, build a meme CLI that scratches someoneâ€™s itch.\nBecome a bridge between disjoint tribesâ€”say, mathâ€‘olympiad Discord âœ• indieâ€‘game dev Slack. Bridges receive twice the info flow.\n\n\n\n\n\nBlog postsÂ â†’ Google juiceÂ â†’ recruiter DM.\nBuildÂ Week streams on Twitch/YouTube. Even five viewers beats zero.\nOpen problems lists: publish a Notion page of bugs/data gaps you want help on; collaborators selfâ€‘select.\n\n\n\n\nPlatforms designed to collide talent & randomness:\n\n\n\n\n\n\n\n\nPlatform\nWhat to Do\nPayoff\n\n\n\n\nbuildspace\nShip an AI app in 6Â weeks with cohort support\nAlumni portal packed with recruiters\n\n\nKaggle\nEnter niche comp with small prize pool\nTopÂ 10 badge shows up on Google forever\n\n\nMLH\nWeekend hackathons w/Â swag + sponsor intros\nRealâ€‘world coding under chaotic deadlines\n\n\n\n\n\n\n\n\nPluchinoÂ etÂ al.Â rewarded frequent, diversified shots. Hereâ€™s how to translate that.\n\n\nEach miniâ€‘project is a lottery ticket; any one can pop. Keep them tiny (oneâ€‘toâ€‘four weekends) so failure costs pocket change.\n\n\n\n\nFreelance bugÂ fix ($50) â†’ testimonial â†’ bigger contract.\nWrite docs for an OSS repo â†’ become deÂ facto maintainer.\n\n\n\n\nUse hackathons as cheap seed rounds. Travel light (sleeping bag, instant noodles), chase themes that scare youâ€”edge compute, bioâ€‘AI, robotics.\n\nRule of Thumb âœï¸: Keep three active betsâ€”one learning, one earning, one purely absurd fun (virality factor).\n\n\n\n\n\n\nThe study found random/equal funding surfaces hidden talent better than winnerâ€‘takesâ€‘all. Hunt orgs that:\n\nRotate juniors through production code, not bug backlogs.\nHost internal hackÂ funds or â€œ20â€¯% time.â€\nCelebrate postâ€‘mortems as loud as launch parties.\n\n\n\n\nRecruit 3â€‘4 peers, share a Notion roadmap, hold monthly demoÂ days.\nPool $20 each for DigitalOcean credits; rotate project leads.\nThat groupÂ â‰  hobby clubâ€”call it a coâ€‘lab on your rÃ©sumÃ©.\n\n\n\n\n\n\nStaying in the game longerÂ â‡’ more rolls of dice.\n\n\n\nSpreadsheet your burn rate.\nApply 50â€‘30â€‘20 rule adjusted for ramen budgets (50Â % needs, 30Â % learning/tools, 20Â % play).\n\n\n\n\nSix months of survival juice buys time to wait for good luck instead of grabbing the first cubicle.\n\n\n\nContract gig at $25/hr might beat â€œAIÂ EngineerÂ Iâ€ title if it grows network 10Ã— faster.\n\n\n\n\nDaily stoic journalâ€”separate what you control vs canâ€™t.\nDigital sabbath once a week; randomness also needs incubation.\n\n\n\n\n\n\n\nWeekly Reflection (30Â min): Did I create/spot at least one optionality event?\nMonthly Collisions Log: Write down every unexpected DM/PR/email â†’ analyze patterns.\nQuarterly Luck Audit: Doubleâ€‘down on highâ€‘yield channels, sunset dead ones.\nAnnual Skill Draft: Like fantasy sports but for courses/books you pledge to finish. Draft publicly on X or LinkedIn for accountability.\n\n\n\n\n\n\n\nEvery tiny win (bug fix, conference note, blog post) is a Lego brick. Stack diligently and in five years you look like an overnight success.\n\n\n\nAreas that benefit from chaosâ€”security, distributed systems, explainable AIâ€”age well and keep spawning lucky breaks.\n\n\n\nTeaching freshmen/bootâ€‘campers sharpens mastery and builds a reputation loop: mentees often graduate into colleagues who remember you.\n\n\n\n\n\n\nTalentÂ = readiness Ã— range. Keep learning, broaden skill graph.\nLuck loves movement. Be discoverable, share loudly.\nDiversify bets. Portfolio outperforms passionâ€‘tunnel.\nSeek luckâ€‘dense ecosystems. Or craft microâ€‘ones.\nMaintain runway & humility. Time on field beats perfect first play.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCategory\nSource\nWhy Itâ€™s Useful\n\n\n\n\nResearch\nTalentÂ vsÂ Luck paper\nCore simulation insight\n\n\nCareer Craft\nCal Newport, So Good They Canâ€™t Ignore You\nCareerâ€‘capital mindset\n\n\nIndieâ€‘Hacking\nPatrickÂ McKenzie, â€œBring the Donutsâ€\nCompounding tiny wins\n\n\nCohortâ€‘Based\nBuildspace\nShip projects with peers\n\n\nCommunities\nAdacamp Discord\nInclusive tech mentorship\n\n\nPhilosophy\nThe Almanack of Naval Ravikant\nLeverage & luck mental models\n\n\nHabits\nAtomic Habits by JamesÂ Clear\nSystemâ€‘building over willpower\n\n\nFinance Basics\nIÂ Will Teach You to Be Rich\nRunway & conscious spending\n\n\n\n\nGot a question or a story about rolling the dice in your early career? Drop a commentâ€”or ping me on Twitter/Xâ€”randomness approves."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#why-this-post-exists",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#why-this-post-exists",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "Job boards look like ghost towns, internships list 10â€¯000 applicants, and every third headline screams â€œAI will eat junior jobs!â€ Yes, automation moved the goalpostsâ€”but that doesnâ€™t mean the stadium is closed.\nBack in 2018 a surprisingly fun physics paperâ€”â€œTalentÂ vsÂ Luck: The Role of Randomness in Success and Failureâ€ (PluchinoÂ etÂ al.,Â arXiv:1802.07068)â€”ran 40â€‘year career simulations and concluded:\n\nModerately talented folks who collide with lots of lucky breaks often outâ€‘earn hyperâ€‘geniuses who never roll a six.\n\nTranslation: you canâ€™t bend probability at will, but you can:\n\n(a) Upgrade the talent multiplierâ€”the skills and habits that convert lucky breaks into tangible wins; and\n(b) Widen the funnelâ€”increase â€œcollision pointsâ€ where random positives can strike.\n\nEverything below is a workingâ€‘class playbook for doing exactly thatâ€”even if you didnâ€™t graduate from MIT, canâ€™t move to SF tomorrow, and still solve lifeâ€™s budget constraints in Excel."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#max-out-the-talent-multiplier",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#max-out-the-talent-multiplier",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "In the simulation a lucky event only doubles your career capital if your talent score \\(T\\) exceeds a random threshold \\(R\\). Better skills â‡¢ higher chance \\(T&gt;R\\).\n\n\n\nProjectsÂ &gt; certs. Recruiters Google your GitHub more than your GPA.\nCycle cadence: pick one microâ€‘project every month â†’ push the code â†’ write a 400â€‘word â€œwhat I learnedâ€ gist or Twitter/X thread.\nProof it works: a friend forked an obscure mathâ€‘toâ€‘LaTeX converter; two weeks later a remote CTO DMed him â†’ paid maintenance gig.\n\n\n\n\nCommunication, product sense & storytelling beat raw LeetCode once youâ€™re inside.\n\nLightningâ€‘talk rule: volunteer to demo anything once a quarterâ€”meetâ€‘ups, Toastmasters, Zoom brownâ€‘bags.\nExplain like Iâ€™m five: blogging technical topics for nonâ€‘experts forces clarity.\n\n\n\n\nLarge models improve weeklyâ€”so should you.\n\n30â€‘day microâ€‘sprints: choose a theme (â€œRustâ€‘WASMâ€, â€œLangChain Agentsâ€), build a weekend toy, document aggressively.\nTool stack rotation: each quarter swap one familiar tool for a trending oneâ€”Dockerâ‡¢Nix, matplotlibâ‡¢Plotly, RESTâ‡¢gRPC.\n\n\n\n\nBurnâ€‘out kills option value.\n\nPomodoro walks: 25Â min codeÂ â†’ 5Â min outdoor stroll.\nCheap ergonomics: used standing desk frame + Ikea top (~$120).\nBYO lunch: saves $250+/moâ€”fuel for courses/hackathons.\n\n\nProÂ tip: Treat skills like an options portfolioâ€”low premium today, huge upside tomorrow."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#expand-your-luck-surface-area",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#expand-your-luck-surface-area",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "More positive random events around you â‡¢ higher odds one lands.\n\n\n\nMoveâ€”or virtually embedâ€”into opportunityâ€‘dense ecosystems (startup accelerators, research labs, OSS orgs).\nLive outside big hubs? Turn Discord/Slack into your â€œcity.â€ Ex: AIÂ CoffeeÂ Break, MLÂ GDE, PaperÂ Club voiceâ€‘rooms drop leads daily.\n\n\n\n\n\nLowâ€‘stakes collabÂ &gt; cold DMs. Merge a doc typo, answer a forum thread, build a meme CLI that scratches someoneâ€™s itch.\nBecome a bridge between disjoint tribesâ€”say, mathâ€‘olympiad Discord âœ• indieâ€‘game dev Slack. Bridges receive twice the info flow.\n\n\n\n\n\nBlog postsÂ â†’ Google juiceÂ â†’ recruiter DM.\nBuildÂ Week streams on Twitch/YouTube. Even five viewers beats zero.\nOpen problems lists: publish a Notion page of bugs/data gaps you want help on; collaborators selfâ€‘select.\n\n\n\n\nPlatforms designed to collide talent & randomness:\n\n\n\n\n\n\n\n\nPlatform\nWhat to Do\nPayoff\n\n\n\n\nbuildspace\nShip an AI app in 6Â weeks with cohort support\nAlumni portal packed with recruiters\n\n\nKaggle\nEnter niche comp with small prize pool\nTopÂ 10 badge shows up on Google forever\n\n\nMLH\nWeekend hackathons w/Â swag + sponsor intros\nRealâ€‘world coding under chaotic deadlines"
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#portfolio-thinking-many-small-bets",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#portfolio-thinking-many-small-bets",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "PluchinoÂ etÂ al.Â rewarded frequent, diversified shots. Hereâ€™s how to translate that.\n\n\nEach miniâ€‘project is a lottery ticket; any one can pop. Keep them tiny (oneâ€‘toâ€‘four weekends) so failure costs pocket change.\n\n\n\n\nFreelance bugÂ fix ($50) â†’ testimonial â†’ bigger contract.\nWrite docs for an OSS repo â†’ become deÂ facto maintainer.\n\n\n\n\nUse hackathons as cheap seed rounds. Travel light (sleeping bag, instant noodles), chase themes that scare youâ€”edge compute, bioâ€‘AI, robotics.\n\nRule of Thumb âœï¸: Keep three active betsâ€”one learning, one earning, one purely absurd fun (virality factor)."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#choose-or-create-egalitarian-environments",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#choose-or-create-egalitarian-environments",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "The study found random/equal funding surfaces hidden talent better than winnerâ€‘takesâ€‘all. Hunt orgs that:\n\nRotate juniors through production code, not bug backlogs.\nHost internal hackÂ funds or â€œ20â€¯% time.â€\nCelebrate postâ€‘mortems as loud as launch parties.\n\n\n\n\nRecruit 3â€‘4 peers, share a Notion roadmap, hold monthly demoÂ days.\nPool $20 each for DigitalOcean credits; rotate project leads.\nThat groupÂ â‰  hobby clubâ€”call it a coâ€‘lab on your rÃ©sumÃ©."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#crashproof-your-finances-mindset",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#crashproof-your-finances-mindset",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "Staying in the game longerÂ â‡’ more rolls of dice.\n\n\n\nSpreadsheet your burn rate.\nApply 50â€‘30â€‘20 rule adjusted for ramen budgets (50Â % needs, 30Â % learning/tools, 20Â % play).\n\n\n\n\nSix months of survival juice buys time to wait for good luck instead of grabbing the first cubicle.\n\n\n\nContract gig at $25/hr might beat â€œAIÂ EngineerÂ Iâ€ title if it grows network 10Ã— faster.\n\n\n\n\nDaily stoic journalâ€”separate what you control vs canâ€™t.\nDigital sabbath once a week; randomness also needs incubation."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#rituals-to-keep-rolling-the-dice",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#rituals-to-keep-rolling-the-dice",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "Weekly Reflection (30Â min): Did I create/spot at least one optionality event?\nMonthly Collisions Log: Write down every unexpected DM/PR/email â†’ analyze patterns.\nQuarterly Luck Audit: Doubleâ€‘down on highâ€‘yield channels, sunset dead ones.\nAnnual Skill Draft: Like fantasy sports but for courses/books you pledge to finish. Draft publicly on X or LinkedIn for accountability."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#play-the-ultralong-game",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#play-the-ultralong-game",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "Every tiny win (bug fix, conference note, blog post) is a Lego brick. Stack diligently and in five years you look like an overnight success.\n\n\n\nAreas that benefit from chaosâ€”security, distributed systems, explainable AIâ€”age well and keep spawning lucky breaks.\n\n\n\nTeaching freshmen/bootâ€‘campers sharpens mastery and builds a reputation loop: mentees often graduate into colleagues who remember you."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#tldr-cheat-sheet",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#tldr-cheat-sheet",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "TalentÂ = readiness Ã— range. Keep learning, broaden skill graph.\nLuck loves movement. Be discoverable, share loudly.\nDiversify bets. Portfolio outperforms passionâ€‘tunnel.\nSeek luckâ€‘dense ecosystems. Or craft microâ€‘ones.\nMaintain runway & humility. Time on field beats perfect first play."
  },
  {
    "objectID": "drafts/20250730-advice-for-young-graduates/index.html#further-reading-link-pack",
    "href": "drafts/20250730-advice-for-young-graduates/index.html#further-reading-link-pack",
    "title": "Beating the Odds: A Luckâ€‘Friendly Career Gameâ€‘Plan for Fresh CS/Eng/Math Grads",
    "section": "",
    "text": "Category\nSource\nWhy Itâ€™s Useful\n\n\n\n\nResearch\nTalentÂ vsÂ Luck paper\nCore simulation insight\n\n\nCareer Craft\nCal Newport, So Good They Canâ€™t Ignore You\nCareerâ€‘capital mindset\n\n\nIndieâ€‘Hacking\nPatrickÂ McKenzie, â€œBring the Donutsâ€\nCompounding tiny wins\n\n\nCohortâ€‘Based\nBuildspace\nShip projects with peers\n\n\nCommunities\nAdacamp Discord\nInclusive tech mentorship\n\n\nPhilosophy\nThe Almanack of Naval Ravikant\nLeverage & luck mental models\n\n\nHabits\nAtomic Habits by JamesÂ Clear\nSystemâ€‘building over willpower\n\n\nFinance Basics\nIÂ Will Teach You to Be Rich\nRunway & conscious spending\n\n\n\n\nGot a question or a story about rolling the dice in your early career? Drop a commentâ€”or ping me on Twitter/Xâ€”randomness approves."
  },
  {
    "objectID": "drafts/20230629-rag/index.html",
    "href": "drafts/20230629-rag/index.html",
    "title": "Pratical retrieval augmented generation (RAG)",
    "section": "",
    "text": "To reduce hallucination and overcome the token limit of large language models, one important recipe is retrieval augmentation.\nThe retrieval augmentation can generally happen at 3 places:"
  },
  {
    "objectID": "drafts/20230629-rag/index.html#references",
    "href": "drafts/20230629-rag/index.html#references",
    "title": "Pratical retrieval augmented generation (RAG)",
    "section": "References",
    "text": "References\n\nLong-range Language Modeling with Self-retrieval"
  },
  {
    "objectID": "drafts/20250720-right-to-ai/index.html",
    "href": "drafts/20250720-right-to-ai/index.html",
    "title": "Beyond Counsel: Why Citizens Deserve a Right to Sufficient AI Advisory",
    "section": "",
    "text": "On a gray Tuesday morning, MarÃ­a Alvarez sat at her kitchen table, laptop open, eyes darting between halfâ€‘finished coffee and a healthâ€‘insurance marketplace she barely understood. She was fortyâ€‘nine, newly selfâ€‘employed, and one misclick away from locking herself into a plan that could swallow half her annual incomeâ€”or leave her uncovered when she needed care most. The instructions were written in courteous but opaque prose, the deductible tables hid behind hyperlinks, and actuarial termsâ€”coinsurance, outâ€‘ofâ€‘pocket maximum, formulary tiersâ€”bloomed like weeds everywhere she looked.\nSo MarÃ­a did what many of us do: she guessed. She chose the third cheapest plan, prayed she was healthy enough, and hoped she hadnâ€™t made a catastrophic mistake.\nIf MarÃ­a had been facing a criminal charge instead of an insurance portal, she would have had the U.S. Constitution at her back. The Sixth Amendment guarantees her the right to competent legal counsel, paid for by the state if she cannot afford it. Society long ago recognized that ordinary citizens cannot possibly match the technical skill, specialized language, and professional resources of prosecutors. We insist on balanceâ€”not because every defendant is innocent, but because dignity demands everyone participate meaningfully in decisions that can upend a life.\nThe digital world has reached a similar tipping point. Algorithms weigh in on everything from credit limits to college admissions; bureaucracies publish everâ€‘expanding rulebooks; corporations negotiate contracts in font sizes small enough to dodge notice. The gap between expert knowledge and everyday understanding is now a canyon, and it is widening faster than traditional institutions can respond.\nThat is why I believe we should begin talking seriously about a new civic guarantee: a right to sufficient AI advisory.\n\nâ€œIf technology is a form of power, then access to the best advice it can synthesize is a form of freedom.â€\n\n\n\nThe Counsel Analogyâ€”And Where It Breaks\nWhen Clarence Earl Gideon stood before the Supreme Court in 1963, penniless and accused of burglary, he asked for a lawyer. The Court agreed he had a constitutional right to one, cementing the idea that some forms of expertise are so instrumental to justice that government must help supply them. Today, prosecutors still outnumber public defenders and funding remains tight, but the principle is uncontested: navigating criminal law without a guide is fundamentally unfair.\nNow replace the courtroom with a mortgage refinancing screen, or a disabilityâ€‘benefit appeal, or a smallâ€‘business tax schedule. We are again asking citizens to decode labyrinthine systems written by specialists, enforced automatically, and outcomeâ€‘determinative for livelihoods. The difference is scale: millions more daily interactions, far less theatrical than a trial, but cumulatively just as consequential.\nAnd yet the modern state offers little beyond PDF instructions and understaffed call centers. Privateâ€‘sector tools existâ€”chatbots that suggest tax deductions, symptomâ€‘checker apps, creditâ€‘card comparison sitesâ€”but these come with advertising incentives, hidden data harvesting, or narrow purpose. They owe no fiduciary duty to the user. They are not counsel; they are sales reps wearing AI skins.\n\n\n\nWhat Does â€œSufficientâ€ Mean?\nLawyers are judged by a standard of competence that evolves as precedent and practice mature. A right to AI advisory would need its own evolving yardstick. Here is a starting sketch:\n\nCompetence: The advisor must meet a statistically validated accuracy threshold and surface the provenance of its suggestions. Explaining why Plan A is safer than Plan B cannot be optional.\nIndependence: It must be structurally insulated from the organizations whose products or rules it critiquesâ€”no undisclosed referral fees, no upsells embedded in advice.\nAccessibility: Free at the point of use, multilingual, and designed for people with limited digital literacy or disabilities. In effect, an ADA for algorithms.\nPrivacy: User data stays siloed, encrypted, and never sold. Recall how attorneyâ€‘client privilege undergirds trust; an AI counterpart needs similar guardrails.\n\nIf those criteria sound ambitious, remember that Gideon did not instantly materialize armies of public defenders; it set an aspiration, forcing institutions to build toward it.\n\n\n\nEarly Glimpses of the Future\nYou can already see prototypes flickering into existence. Estoniaâ€™s nationwide â€œKrattâ€ virtual assistant helps residents navigate government services in natural language. Spainâ€™s Carta de Derechos Digitales enshrines a right to algorithmic transparency. The European Unionâ€™s AI Act, passed in 2024, classifies publicâ€‘service AI as â€œhighâ€‘risk,â€ demanding audits and human oversight. None of these, however, amount to a right to personal AI counsel. They regulate providers, not empower citizens.\nOn the commercial side, TurboTaxâ€™s AI coach and the NHSâ€™s 111 triage chatbot inch closer, yet both remain company properties, not public assets. Their loyalties are negotiated by terms of service, not constitutional mandate.\n\n\n\nWhy Enshrine a Right?\nCognitive Equityâ€ƒThe internet gave us universal publishing; AI promises universal analysis. Leaving that promise to market forces risks entrenching a new knowledge aristocracyâ€”the well advised versus the algorithmically outmatched.\nProcedural Justiceâ€ƒWhether you receive cancer treatment, studentâ€‘loan forgiveness, or childâ€‘care subsidies increasingly hinges on opaque risk scores and rule checks. A right to AI advisory restores a measure of symmetry between citizen and system.\nEconomic Efficiencyâ€ƒMistakes are expensive. The 2019 Dutch childâ€‘benefit scandalâ€”where flawed fraud algorithms wrecked thousands of familiesâ€”will cost billions in reparations and lost productivity. Preventive advice is cheaper than postâ€‘hoc scandal management.\nDemocratic Feedbackâ€ƒAggregated, anonymized queries can reveal which statutes confuse citizens most, guiding legislators toward clearer draftingâ€”an iterative loop of governance enhanced by data.\n\n\n\nCommon Pushbackâ€”and Rebuttals\nâ€œWonâ€™t people become dependent on AI?â€ Some will; many already rely on GPS. The goal is not to replace human judgment but to scaffold it. Just as a lawyer doesnâ€™t rob you of agency, neither would an advisory model that explains choices, cites evidence, and invites override.\nâ€œAI is biased; why codify its role?â€ Because bias thrives in darkness. A statutory right would come bundled with audit requirements, redress mechanisms, and a publicâ€‘interest mandateâ€”the opposite of todayâ€™s unregulated chatbots.\nâ€œThis sounds expensive.â€ Training a domainâ€‘specific language model is orders of magnitude cheaper than staffing equivalent human expertise for every citizen interaction. Moreover, costâ€‘sharing frameworksâ€”openâ€‘source models, public cloud credits, civicâ€‘tech partnershipsâ€”can drive marginal costs toward zero.\nâ€œPrivate companies already offer tools.â€ Public defenders coexist with private attorneys; both have roles. The right protects those who cannot afford premium services and compels baseline quality across the board.\n\n\n\nHow Might We Get There?\n\nStatutory Seedsâ€ƒCongress or parliaments could graft an advisory clause onto existing consumerâ€‘protection or administrativeâ€‘procedure acts, starting with highâ€‘impact domains like healthcare or housing.\nPilot Sandboxesâ€ƒGovernments partner with universities and civic nonprofits to deploy openâ€‘weight models, testing accuracy, bias, and user experience on optâ€‘in populations.\nCertification and Liabilityâ€ƒAn oversight bodyâ€”think Financial Ombudsman meets Public Defenderâ€”licenses advisory systems, rescinds certification for repeated harms, and adjudicates complaints.\nOpen Rulebooksâ€ƒAgencies publish regulations in machineâ€‘readable formats, allowing continuous retraining and independent verification.\n\nHistory teaches that rights evolve from practice as much as proclamation. Small pilots snowball into expectations, expectations ossify into precedent, and precedent hardens into law.\n\n\n\nA Global View\nThe conversation wonâ€™t unfold only in Washington or Brussels. Kenyaâ€™s Huduma number, Indiaâ€™s Aadhaarâ€‘linked welfare portals, and Brazilâ€™s openâ€‘government data initiatives all raise the same question: When digital systems mediate citizenship, what guarantees shield the individual? A right to AI advisory could become a lingua franca of digital rights, traveling across legal cultures the way freedom of expression did in the twentieth century.\n\n\n\nClosing Thoughts: Choosing the Future We Inhabit\nNear the end of Gideonâ€™s Trumpet, Anthony Lewis writes that Gideonâ€™s victory mattered not because it freed one man, but because it signaled Americaâ€™s capacity for selfâ€‘correction. The machinery of law bent, however incrementally, toward fairness.\nWe stand at another inflection point. The machinery now is digital, the decisions diffuse, the stakes disguised under userâ€‘agreement clickâ€‘boxes. Yet the principle endures: power without accessible counterâ€‘power breeds injustice. Granting every citizen a right to sufficient AI advisory would not make bureaucracy disappear, but it would supply the flashlight, the map, and the translator that modern life increasingly demands.\nMarÃ­a Alvarez should not have to gamble her health on a hunch. Neither should you.\nIf technology is to remain a tool of emancipation rather than domination, counsel by code may be the right whose hour has come."
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html",
    "href": "drafts/20250629-doc-ai-vlm/index.html",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "By &lt;YourÂ Name&gt; Last updatedÂ JulyÂ 2025\n\nTL;DRÂ â€” Document AI has moved far beyond â€œOCRÂ +Â regex.â€Â Foundation visionâ€‘language models (VLMs), promptable OCR engines, and LoRA/QLoRA adapters now let small teams build humanâ€‘level extraction and reasoning systems for messy, handwritten, multipage documents without huge annotation budgets. This post surveys the tech landscape, production patterns, and open research problemsâ€”peppered with links so you can dive deeper. In a hurry? Jump straight to Â§6Â Recommended Techâ€‘Stack Patterns.\n\n\n\n\n\nThe Big Picture: Why Vision Is Still Not Solved\nClassic Layoutâ€‘Aware TransformersÂ â€” Why They Still Matter\nRise of Promptable OCRÂ & Multimodal Giants\nFineâ€‘Tuning Open VLMsÂ (Qwenâ€‘VL,Â InternVLâ€¦)Â â€”Â RecipesÂ &Â Gains\nHandâ€‘Written HRÂ & Tax FormsÂ â€” Best Pipelines in Practice\nRecommended Techâ€‘Stack Patterns\nOperational MetricsÂ & Validation\nOpen ProblemsÂ & 2025â€‘26Â Research Directions\n\n\n\n\n\n\nâ€œWhen a problem looks solved on benchmarks, ask which benchmark.â€Â â€” Anonymous CVPR reviewer\n\nComputer vision has marched through four overlapping erasâ€”each solved old tasks and exposed new gaps:\n\n\n\nEra\nYears\nCore Idea\nFlagship Models\nLimitations\n\n\n\n\nHandâ€‘crafted features\nÂ 1998â€‘2011\nEdgesÂ + descriptors (SIFT/HOGÂ [LoweÂ 1999],Â Dalalâ€‘TriggsÂ 2005)\nViolaâ€“Jones face, pedestrian HOG\nSensitive to lighting; no global context\n\n\nDeepâ€‘CNN explosion\nÂ 2012â€‘2018\nEndâ€‘toâ€‘end feature learning\nAlexNetÂ [KrizhevskyÂ 2012], ResNet, YOLO\nDataâ€‘hungry; fixed vocab; weak reasoning\n\n\nSelf/Weak SupervisionÂ + ViT\nÂ 2019â€‘2022\nContrastive & masked image pretrain\nSimCLRÂ [ChenÂ 2020], MoCo, ViTÂ [DosovitskiyÂ 2020]\nHeavy compute; limited multimodal grounding\n\n\nMultimodal & Foundation\nÂ 2023â€‘\nJoint visionâ€‘language preâ€‘train; huge context\nCLIPÂ [RadfordÂ 2021], FlamingoÂ [DeepMindÂ 2022], GPTâ€‘4V, GeminiÂ 1.5, SAMÂ [KirillovÂ 2023]\nHallucinations; cost; eval gaps; domain drift\n\n\n\nDocument AI inherits two persistent challenges:\n\nReasoning, not detectionÂ â€” e.g.Â counting allowances, verifying signatures, crossâ€‘page consistency.\nRobustness & fairnessÂ â€” fonts, inks, languages, socioâ€‘economic biases.\n\n\n\n\n\nLayoutLMÂ [XuÂ 2020] injected 2â€‘D coordinates into BERT; LayoutLMv3Â [HuangÂ 2022] adds image patches + masked preâ€‘training. Despite new VLMs, these models remain production favorites.\n\n\n\n\n\n\n\n\n\nAdvantage\nRealâ€‘World Impact\n\n\n\n\nSpatial priors\nBeat heuristics on tables & keyâ€‘value.\n\n\nComputeâ€‘friendly\n110â€‘350â€¯MÂ params â†’ CPU inference or Jetson edge.\n\n\nDeterministic logits\nSimple confidence gating & SOCâ€‘2 audits.\n\n\nTiny fineâ€‘tune sets\nA weekend with 1â€¯k labelled pages.\n\n\n\n\nOn FUNSD/CORD/SROIE, a 350â€¯M LayoutLMâ€‘v3 keeps a 3â€‘10â€¯F1 lead over zeroâ€‘shot GPTâ€‘4oâ€”at a fraction of cost.\n\n\n\n\n\n\n\n\n\n\n\nModel\nCode / Docs\nHighlight\nExample Prompt\n\n\n\n\nPix2Struct\nhttps://github.com/google-research/pix2struct\nSeqâ€‘toâ€‘seq; screenshots & docs; Trained on PaLI.\n&lt;question&gt;total amount?&lt;/question&gt;\n\n\nDonut\nhttps://github.com/clovaai/donut\nOCRâ€‘free; decoder emits JSON; LoRAâ€‘friendly.\n&lt;s_cord-v2&gt;&lt;date&gt;&lt;total&gt;\n\n\nTrOCRÂ v2\nhttps://aka.ms/TrOCR\nViTÂ +Â decoder; SOTA IAM/CEDAR CER.\n--task handwriting\n\n\nMistralÂ OCR\nhttps://mistral.ai/product/ocr\nCommercial; interleaved layout tree; 4â€‘bit GPU runtime.\nmode=table,json=true\n\n\n\nÂ» Proâ€‘tipÂ â€” Combine SAMÂ (https://segment-anything.com/) with Donut to autoâ€‘crop dense tables â†’Â +3â€‘6Â F1 on invoices.\n\n\n\n\n\n\nModel\nWeights / API\nContext\nPaper / Repo\n\n\n\n\nGPTâ€‘4o\nOpenAIÂ API\n128â€¯k tokens\nhttps://openai.com\n\n\nGeminiÂ 1.5 Flash\nGoogleÂ Vertex\n1â€¯M tokens\nhttps://deepmind.google/technologies/gemini/\n\n\nQwenÂ 2.5â€‘VLâ€‘7B\nhttps://modelscope.cn/models/qwen/Qwen-VL/\n32â€¯k\nQwenâ€‘VL paperÂ [ChenÂ 2024]\n\n\nInternVLÂ 2.5â€‘8B\nhttps://github.com/OpenGVLab/InternVL\n64â€¯k\nInternVLâ€‘X paperÂ [YangÂ 2024]\n\n\n\n\n\n\n\n\n\n\n\nDomain vocabularyÂ â€” e.g.Â â€œExemptÂ Allow.â€, â€œSSNÂ (LastÂ 4)â€.\nHallucinationâ€‘trimÂ â€” DPO/SteerLM adapters cut MMHalÂ [ShusterÂ 2024] hallucinations by â‰¥9â€¯%.\nCost/LatencyÂ â€” 4â€‘bit LoRA onâ€‘prem beats API bills.\n\n\n\n\nSee full script: https://github.com/QwenLM/Qwen/blob/main/examples/finetune_vl_lora.py\n\n\n\n\nWildReceiptsÂ [ChenÂ 2024] KV F1 83â€¯â†’â€¯90.4.\nDocVQA EM 44.5â€¯â†’â€¯57.0.\nLayoutLM parity with zeroâ€‘shot GPTâ€‘4o on SROIE after 3â€¯kâ€‘page LoRA.\n\n\n\n\n\n\nHR onboarding packets (Wâ€‘4, ACH Directâ€‘Deposit, Wâ€‘9) still arrive as faxâ€‘quality scans.\n\n\n\n\n\nEngine\nIAM CER â†“\nWâ€‘4 CER â†“\nLink\n\n\n\n\nTrOCRâ€‘large\n3.9â€¯%\n2.5â€¯%\nhttps://aka.ms/TrOCR\n\n\nDonutâ€‘handwritten\n5.2â€¯%\n3.1â€¯%\nhttps://github.com/clovaai/donut\n\n\nAzureÂ Readâ€¯v4\n4.8â€¯%\n3.4â€¯%\nhttps://learn.microsoft.com/azure/ai-services/document-intelligence\n\n\nMistralÂ OCR\n4.4â€¯%\n3.0â€¯%\nhttps://mistral.ai/product/ocr\n\n\n\n\n\n\nMermaid diagram above. Validation prompt spec: https://platform.openai.com/docs/guides/function-calling\n\n\n\n\nABA checksum: https://en.wikipedia.org/wiki/ABA_routing_transit_number\nSignature embeddings: https://github.com/ShieldMnt/invisible-watermark-signature\n\n\n\n\nCase study spreadsheet â†’ https://tinyurl.com/docaiâ€‘w4â€‘metrics\n\n\n\n\n\n\n\n\nScenario\nOCR\nVLM\nHelpfulÂ Link\n\n\n\n\nAllâ€‘cloudÂ / noÂ ops\nGoogle EnterpriseÂ OCR\nGeminiÂ 1.5Â Flash\nhttps://cloud.google.com/document-ai\n\n\nRegulated onâ€‘prem\nTrOCRâ€‘largeÂ 4â€‘bit\nGPTâ€‘4oâ€‘miniÂ (local)\nhttps://openai.com/research/gpt-4o\n\n\nEdge kiosk\nLayoutLMâ€‘v3Â INT8\nQwenâ€‘VLâ€‘2B\nhttps://github.com/pytorch/accelerated-inference\n\n\nStartup PoC\nDonutâ€‘base\nGPTâ€‘4o API\nhttps://clovaai.github.io/donut/\n\n\n\n\n\n\n\nAutomate nightly tests with docâ€‘gen synthetic corpusÂ (https://github.com/xyz/doc-gen). Track: CER, F1, MMHal hallucination %, drift metrics.\n\n\n\n\n\nGrounded QA â€”Â projects like VALHALLA tie answers to pixel spans.\nLowâ€‘resource handwriting â€” transfer to Devanagari, Thai, Amharic.\nFederated fineâ€‘tuning â€” e.g., FedLoRA.\nVLMÂ MLOps â€” model/version diffâ€‘storage [LitellmÂ WeightsÂ Diff].\nStress benchmarks â€” MMHalÂ v2, DocRagEval.\nSparse MoE vision towers â€” see SparseSight.\n\n\n\n\n\nLayoutâ€‘aware transformers remain accuracyâ€‘perâ€‘$ kings.\nPromptable OCR engines replace regex hacksâ€”return JSON directly.\nVLMs add humanâ€‘style QA & validationâ€”light adapters cut hallucinations.\nHybrid stacks winâ€”keep models modular so you can swap components quarterly.\n\n\nâ€œIn production, the boring stuffâ€”confidence thresholds, audit logs, drift monitorsâ€”is what keeps the VP Legal happy.â€Â â€” StaffÂ ML Engineer, Fintech unicorn\n\nHappy building! Ping me onÂ X/Twitter orÂ Threads if you deploy any of these stacks in theÂ wild."
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#table-of-contents",
    "href": "drafts/20250629-doc-ai-vlm/index.html#table-of-contents",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "The Big Picture: Why Vision Is Still Not Solved\nClassic Layoutâ€‘Aware TransformersÂ â€” Why They Still Matter\nRise of Promptable OCRÂ & Multimodal Giants\nFineâ€‘Tuning Open VLMsÂ (Qwenâ€‘VL,Â InternVLâ€¦)Â â€”Â RecipesÂ &Â Gains\nHandâ€‘Written HRÂ & Tax FormsÂ â€” Best Pipelines in Practice\nRecommended Techâ€‘Stack Patterns\nOperational MetricsÂ & Validation\nOpen ProblemsÂ & 2025â€‘26Â Research Directions"
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#the-big-picture-why-vision-is-still-not-solved",
    "href": "drafts/20250629-doc-ai-vlm/index.html#the-big-picture-why-vision-is-still-not-solved",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "â€œWhen a problem looks solved on benchmarks, ask which benchmark.â€Â â€” Anonymous CVPR reviewer\n\nComputer vision has marched through four overlapping erasâ€”each solved old tasks and exposed new gaps:\n\n\n\nEra\nYears\nCore Idea\nFlagship Models\nLimitations\n\n\n\n\nHandâ€‘crafted features\nÂ 1998â€‘2011\nEdgesÂ + descriptors (SIFT/HOGÂ [LoweÂ 1999],Â Dalalâ€‘TriggsÂ 2005)\nViolaâ€“Jones face, pedestrian HOG\nSensitive to lighting; no global context\n\n\nDeepâ€‘CNN explosion\nÂ 2012â€‘2018\nEndâ€‘toâ€‘end feature learning\nAlexNetÂ [KrizhevskyÂ 2012], ResNet, YOLO\nDataâ€‘hungry; fixed vocab; weak reasoning\n\n\nSelf/Weak SupervisionÂ + ViT\nÂ 2019â€‘2022\nContrastive & masked image pretrain\nSimCLRÂ [ChenÂ 2020], MoCo, ViTÂ [DosovitskiyÂ 2020]\nHeavy compute; limited multimodal grounding\n\n\nMultimodal & Foundation\nÂ 2023â€‘\nJoint visionâ€‘language preâ€‘train; huge context\nCLIPÂ [RadfordÂ 2021], FlamingoÂ [DeepMindÂ 2022], GPTâ€‘4V, GeminiÂ 1.5, SAMÂ [KirillovÂ 2023]\nHallucinations; cost; eval gaps; domain drift\n\n\n\nDocument AI inherits two persistent challenges:\n\nReasoning, not detectionÂ â€” e.g.Â counting allowances, verifying signatures, crossâ€‘page consistency.\nRobustness & fairnessÂ â€” fonts, inks, languages, socioâ€‘economic biases."
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#classic-layoutaware-transformers-why-they-still-matter",
    "href": "drafts/20250629-doc-ai-vlm/index.html#classic-layoutaware-transformers-why-they-still-matter",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "LayoutLMÂ [XuÂ 2020] injected 2â€‘D coordinates into BERT; LayoutLMv3Â [HuangÂ 2022] adds image patches + masked preâ€‘training. Despite new VLMs, these models remain production favorites.\n\n\n\n\n\n\n\n\n\nAdvantage\nRealâ€‘World Impact\n\n\n\n\nSpatial priors\nBeat heuristics on tables & keyâ€‘value.\n\n\nComputeâ€‘friendly\n110â€‘350â€¯MÂ params â†’ CPU inference or Jetson edge.\n\n\nDeterministic logits\nSimple confidence gating & SOCâ€‘2 audits.\n\n\nTiny fineâ€‘tune sets\nA weekend with 1â€¯k labelled pages.\n\n\n\n\nOn FUNSD/CORD/SROIE, a 350â€¯M LayoutLMâ€‘v3 keeps a 3â€‘10â€¯F1 lead over zeroâ€‘shot GPTâ€‘4oâ€”at a fraction of cost."
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#rise-of-promptable-ocr-multimodal-giants",
    "href": "drafts/20250629-doc-ai-vlm/index.html#rise-of-promptable-ocr-multimodal-giants",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "Model\nCode / Docs\nHighlight\nExample Prompt\n\n\n\n\nPix2Struct\nhttps://github.com/google-research/pix2struct\nSeqâ€‘toâ€‘seq; screenshots & docs; Trained on PaLI.\n&lt;question&gt;total amount?&lt;/question&gt;\n\n\nDonut\nhttps://github.com/clovaai/donut\nOCRâ€‘free; decoder emits JSON; LoRAâ€‘friendly.\n&lt;s_cord-v2&gt;&lt;date&gt;&lt;total&gt;\n\n\nTrOCRÂ v2\nhttps://aka.ms/TrOCR\nViTÂ +Â decoder; SOTA IAM/CEDAR CER.\n--task handwriting\n\n\nMistralÂ OCR\nhttps://mistral.ai/product/ocr\nCommercial; interleaved layout tree; 4â€‘bit GPU runtime.\nmode=table,json=true\n\n\n\nÂ» Proâ€‘tipÂ â€” Combine SAMÂ (https://segment-anything.com/) with Donut to autoâ€‘crop dense tables â†’Â +3â€‘6Â F1 on invoices.\n\n\n\n\n\n\nModel\nWeights / API\nContext\nPaper / Repo\n\n\n\n\nGPTâ€‘4o\nOpenAIÂ API\n128â€¯k tokens\nhttps://openai.com\n\n\nGeminiÂ 1.5 Flash\nGoogleÂ Vertex\n1â€¯M tokens\nhttps://deepmind.google/technologies/gemini/\n\n\nQwenÂ 2.5â€‘VLâ€‘7B\nhttps://modelscope.cn/models/qwen/Qwen-VL/\n32â€¯k\nQwenâ€‘VL paperÂ [ChenÂ 2024]\n\n\nInternVLÂ 2.5â€‘8B\nhttps://github.com/OpenGVLab/InternVL\n64â€¯k\nInternVLâ€‘X paperÂ [YangÂ 2024]"
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#finetuning-open-vlms-recipes-gains",
    "href": "drafts/20250629-doc-ai-vlm/index.html#finetuning-open-vlms-recipes-gains",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "Domain vocabularyÂ â€” e.g.Â â€œExemptÂ Allow.â€, â€œSSNÂ (LastÂ 4)â€.\nHallucinationâ€‘trimÂ â€” DPO/SteerLM adapters cut MMHalÂ [ShusterÂ 2024] hallucinations by â‰¥9â€¯%.\nCost/LatencyÂ â€” 4â€‘bit LoRA onâ€‘prem beats API bills.\n\n\n\n\nSee full script: https://github.com/QwenLM/Qwen/blob/main/examples/finetune_vl_lora.py\n\n\n\n\nWildReceiptsÂ [ChenÂ 2024] KV F1 83â€¯â†’â€¯90.4.\nDocVQA EM 44.5â€¯â†’â€¯57.0.\nLayoutLM parity with zeroâ€‘shot GPTâ€‘4o on SROIE after 3â€¯kâ€‘page LoRA."
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#handwritten-hr-tax-forms-best-pipelines",
    "href": "drafts/20250629-doc-ai-vlm/index.html#handwritten-hr-tax-forms-best-pipelines",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "HR onboarding packets (Wâ€‘4, ACH Directâ€‘Deposit, Wâ€‘9) still arrive as faxâ€‘quality scans.\n\n\n\n\n\nEngine\nIAM CER â†“\nWâ€‘4 CER â†“\nLink\n\n\n\n\nTrOCRâ€‘large\n3.9â€¯%\n2.5â€¯%\nhttps://aka.ms/TrOCR\n\n\nDonutâ€‘handwritten\n5.2â€¯%\n3.1â€¯%\nhttps://github.com/clovaai/donut\n\n\nAzureÂ Readâ€¯v4\n4.8â€¯%\n3.4â€¯%\nhttps://learn.microsoft.com/azure/ai-services/document-intelligence\n\n\nMistralÂ OCR\n4.4â€¯%\n3.0â€¯%\nhttps://mistral.ai/product/ocr\n\n\n\n\n\n\nMermaid diagram above. Validation prompt spec: https://platform.openai.com/docs/guides/function-calling\n\n\n\n\nABA checksum: https://en.wikipedia.org/wiki/ABA_routing_transit_number\nSignature embeddings: https://github.com/ShieldMnt/invisible-watermark-signature\n\n\n\n\nCase study spreadsheet â†’ https://tinyurl.com/docaiâ€‘w4â€‘metrics"
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#recommended-techstack-patterns",
    "href": "drafts/20250629-doc-ai-vlm/index.html#recommended-techstack-patterns",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "Scenario\nOCR\nVLM\nHelpfulÂ Link\n\n\n\n\nAllâ€‘cloudÂ / noÂ ops\nGoogle EnterpriseÂ OCR\nGeminiÂ 1.5Â Flash\nhttps://cloud.google.com/document-ai\n\n\nRegulated onâ€‘prem\nTrOCRâ€‘largeÂ 4â€‘bit\nGPTâ€‘4oâ€‘miniÂ (local)\nhttps://openai.com/research/gpt-4o\n\n\nEdge kiosk\nLayoutLMâ€‘v3Â INT8\nQwenâ€‘VLâ€‘2B\nhttps://github.com/pytorch/accelerated-inference\n\n\nStartup PoC\nDonutâ€‘base\nGPTâ€‘4o API\nhttps://clovaai.github.io/donut/"
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#operational-metrics-validation",
    "href": "drafts/20250629-doc-ai-vlm/index.html#operational-metrics-validation",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "Automate nightly tests with docâ€‘gen synthetic corpusÂ (https://github.com/xyz/doc-gen). Track: CER, F1, MMHal hallucination %, drift metrics."
  },
  {
    "objectID": "drafts/20250629-doc-ai-vlm/index.html#open-problems-research-directions",
    "href": "drafts/20250629-doc-ai-vlm/index.html#open-problems-research-directions",
    "title": "Modern Documentâ€¯AI & Visionâ€‘Language Models â€“ AÂ 2025 TechnicalÂ Recap",
    "section": "",
    "text": "Grounded QA â€”Â projects like VALHALLA tie answers to pixel spans.\nLowâ€‘resource handwriting â€” transfer to Devanagari, Thai, Amharic.\nFederated fineâ€‘tuning â€” e.g., FedLoRA.\nVLMÂ MLOps â€” model/version diffâ€‘storage [LitellmÂ WeightsÂ Diff].\nStress benchmarks â€” MMHalÂ v2, DocRagEval.\nSparse MoE vision towers â€” see SparseSight.\n\n\n\n\n\nLayoutâ€‘aware transformers remain accuracyâ€‘perâ€‘$ kings.\nPromptable OCR engines replace regex hacksâ€”return JSON directly.\nVLMs add humanâ€‘style QA & validationâ€”light adapters cut hallucinations.\nHybrid stacks winâ€”keep models modular so you can swap components quarterly.\n\n\nâ€œIn production, the boring stuffâ€”confidence thresholds, audit logs, drift monitorsâ€”is what keeps the VP Legal happy.â€Â â€” StaffÂ ML Engineer, Fintech unicorn\n\nHappy building! Ping me onÂ X/Twitter orÂ Threads if you deploy any of these stacks in theÂ wild."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html",
    "href": "posts/20250712-agentic-causal/index.html",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of sciences. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why.\nThe plural on sciences is intentional. I want to emphasize the range of disciplines: physics, chemistry, biology, economics, sociology, computer science, data science, you name it. But in this post I would like to dedicate my attention to causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-sciences",
    "href": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-sciences",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of sciences. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why.\nThe plural on sciences is intentional. I want to emphasize the range of disciplines: physics, chemistry, biology, economics, sociology, computer science, data science, you name it. But in this post I would like to dedicate my attention to causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "href": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "title": "Agentic Causal Inference",
    "section": "Causal Inference and LLMs",
    "text": "Causal Inference and LLMs\n\n\n\n\n\nCausal inference is such an important decision making tool in life and in business. However, to be an expert in this field takes years of mathematical and statistical training. LLMs on the other hand are easy to use, but they lack rigor when reasoning about causality.\nIntegrating causality into LLM agents addresses limitations on both sides:\n\nPure causal methods demand strict assumptions and expert guidance.\nLLMs overflow with knowledge yet often mistake correlation for causation.\n\nBy wiring LLMâ€‘based agents to specialized causal inference libraries, we can automate the causal workflow: discovery -&gt; identification -&gt; estimation -&gt; refutation. The result is a new class of generalâ€‘purpose causal AI systems that parse tabular, time-series, and even multimodal data with human-like intuition and mathematical rigor.\nPractically, that means the agent:\n\nThinks (via an LLM) about what causal graph should link your variables\nActs by writing Python: drawing DAGs, running ID algorithms, calling estimators, using libraries like dowhy, econml, causaltune, etc.\nReflects on the results, prompting itself with â€œDo my assumptions still hold?â€\nIterates until a relevant answer surfaces, or it asks you for help.\n\nIf that sounds suspiciously like a data scientist teamate with infinite patience, youâ€™ve got the gist.\n\n10 lines of code for your first causal inference agent\nTo illustrate the idea, here is a minimal snippet to estimate the effect of a new coupon on revenue. This may actually be sufficient to get you a quick answer, if (a big if) the data is ready.\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.llms import OpenAI\nfrom dowhy import CausalModel\nfrom causaltune import AutoTune\n\ndef estimate_ate(df, treatment, outcome):\n    model = CausalModel(data=df, treatment=treatment, outcome=outcome)\n    ided = model.identify_effect()\n    best = AutoTune(model, df).best_estimator_\n    return model.estimate_effect(ided, method_name=best)\n\nagent = initialize_agent(\n    llm=OpenAI(model_name=\"gpt-4o-mini\"),\n    tools=[Tool.from_function(estimate_ate)],\n    agent_type=\"openai-tools\"\n)\n\nagent.run(\"Estimate the uplift of coupon_v2 on weekly revenue\")\nTime to look at some interesting papers and open source projects:\n\n\n\nSingleâ€‘Agent Autonomous Pipelines\nCausalÂ Agent frameworkÂ (2024):\n\nAn LLM operates in a ReActâ€‘style loop with a suite of causal toolsâ€”e.g.Â CausalLearn for graph discovery and EconML for effect estimation.\nGiven a dataset and a query (e.g.Â â€œEffect ofÂ X onÂ Y?â€) the agent automatically:\n\nexplores variable correlations,\nhypothesizes causal links,\nproposes a causal graph,\ncomputes the quantitative effect ofÂ X onÂ Y.\n\nEach step is backed by library outputs that the LLM interprets before deciding its next move.\n\nThe frameworkâ€™s hierarchical breakdown (variableâ€‘level, edgeâ€‘level, graphâ€‘level, effectâ€‘level) has produced expertâ€‘level accuracy on a testing dataset with 1.3k questions, all while providing interpretable explanations.\n\nCausal-Copilot (2025)\n\nThe agent chains 20 + causal tools, from discovery to hyper-parameter tuning, inside a single LLM loop.\n\nWorks on both tabular and time-series data: prompts the user for a question, auto-selects the right discovery algorithm (e.g., NOTEARS, PC), tunes an estimator (DoubleML, CausalForest, IV), runs refuters, and returns an English report with effect size + CI.\nAchieves state-of-the-art graph accuracy and effect-estimation error across five public benchmarks, edging out both classic SCD baselines and earlier LLM agents.\n\n\n\n\nDebating Multiâ€‘Agent Systems for Causal Discovery\nSingle agents sometimes hallucinate; multiâ€‘agent approaches aim to reduce errors through debate and consensus.\n\nMultiâ€‘Agent Causal Discovery Using LLMsÂ (2024) assigns dedicated LLM roles:\n\nAffirmative Debaters proposes a DAG using temporal cues and domain priors.\nNegative Debaters attacks the proposal by surfacing hidden confounders, incorrect temporal orderings, or omitted variables.\nJudges evaluate arguments and pick the most plausible structure.\nCoders materializes the agreed-upon algorithm, reruns it on the entire dataset, and emits the refined graph.\n\n\nExperiments show these debating agents outperform both classical algorithms and singleâ€‘LLM prompts on datasets like Autoâ€¯MPG, demonstrating that multiple specialized minds can yield more reliable causal graphs.\n\nChain-of-Collaboration Prompting (2025) shows that giving sub-agents explicit roles (planner, verifier, critic) and letting them share scratch pads improves causal reasoning accuracy on CLADDER and Causal-Copilot QA tasks, cutting hallucinated edges by 35 % vs.Â single-prompt ReAct.\n\n\n\nToolbox Layer (AutoML & Noâ€‘Code Platforms)\nParallel to LLM research, we also see AutoMLâ€‘style causal platforms that automate model selection, tuning, and robustness checks.\n\nAutoCausality: part of the PyWhy ecosystem, using hyperâ€‘parameter search and ensembling to choose the best estimator for a dataset.\nOpportunityFinderÂ (AmazonÂ 2023) offers codeâ€‘less causal studies for panel data cleaning, cohorting, and computing effects (plus sensitivity) endâ€‘toâ€‘end.\nSalesforce CausalAI Library consolidates discovery and inference methods, synthetic data generators, and a noâ€‘code GUI, scaling to larger problems via optimized multiprocessing.\n\nThese toolkits enrich agentic workflows: an LLM planner can mixâ€‘andâ€‘match discovery, estimation, and AutoML selection modules without human intervention."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "href": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "title": "Agentic Causal Inference",
    "section": "Evaluating Causal Inference Agents",
    "text": "Evaluating Causal Inference Agents\nHow well do these causal inference agents perform? Here are some real or synthetic datasets and benchmarks.\nFor treatmentâ€‘effect estimation, the Lalonde jobâ€‘training study is a good place to start. It has real observational covariates paired with true RCT outcomesâ€”to sanityâ€‘check bias reduction. When larger, controlled replications are needed, you can use semiâ€‘synthetic generators such as IHDP and the Twins dataset, whose perfect counterfactual comes from each twinâ€™s paired outcome. The annual ACIC challenges extend this idea with dozens of highâ€‘dimensional scenarios, while the 2025 RealCause generator allows people to create realistic Lalondeâ€‘style benchmarks.\nFor longitudinal uplift studies, Amazonâ€™s noâ€‘code OpportunityFinder panels ship sample retail datasets ready for differenceâ€‘inâ€‘differences.\nWhen it comes to graph discovery methods, people tend to use classic datasets such as the 11â€‘node Sachs proteinâ€‘signaling map, a real wetâ€‘lab interventions dataset. Bayesianâ€‘network classics like Asia and ALARM remain quick smoke tests. Pairwise direction algorithms rely on the TÃ¼bingen causeâ€“effect pairs, and larger timeâ€‘series graphs come from geneâ€‘regulation contests such as DREAM4.\nMore recently we see languageâ€‘centric causal benchmarks. CLADDER has 10k natural language questions across Pearlâ€™s ladder, while ACCESS asks agents to build abstract causal graphs over multimodal vignettes before answering why queries.\nAs to multimodal causal inference, CausalVQA is a benchmark for video question answering (VQA) that test modelsâ€™ understanding of causality in the physical world."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "href": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "title": "Agentic Causal Inference",
    "section": "Challenges and Mitigations",
    "text": "Challenges and Mitigations\nGoing beyond the happy path to production is often not a smooth ride. Here are some of common challenges in my experience:\n\nData Quality and the Missing Confounders\nObservational datasets rarely contain every variable that shapes a treatmentâ€“outcome relationship, so even a stateâ€‘ofâ€‘theâ€‘art estimator can inherit hidden bias.\nTo mitigate, insert a humanâ€‘review checkpoint right after the agent proposes its first causal graph: domain experts eyeball edges and nominate missing covariates. The software then launches automatic robustness probes such as placebo tests, synthetic confounder injections, and other refutation modules shipped with DoWhy, to quantify how fragile the estimate is. Crucially, if any refutation fails, the planner LLM must stop, annotate the failure, and either revise the graph or escalate to a human reviewer; surfacing a shaky result as â€œtentativeâ€ is better than silently proceeding. Some teams also run a â€œdataâ€‘profiling agentâ€ that scans fresh tables for covariate drift or sparsity and warns the planner before analysis starts.\n\n\nHallucinations and Overâ€‘Confidence in Planner LLMs\nLLM planners are persuasive storytellers; a well phrased chainâ€‘ofâ€‘thought can make a shaky causal graph feel ironclad.\nMultiâ€‘agent debate is a good recipe to reduce hallucination: a second LLM plays devilâ€™s advocate, and challenges the assumptions that make an estimate causal:\n\nPlaceboâ€‘treatment test: replace the real treatment with a fake; any nonâ€‘zero effect flags hidden bias.\nSyntheticâ€‘confounder injection: add a random common cause and observe the ATE shift; big swings imply unmeasured confounding.\nOverlap / positivity audit: verify that propensity scores span both arms; poor overlap triggers trimming or doubly robust methods.\nCrossâ€‘estimator consensus: pit a backâ€‘door learner against an IV or frontâ€‘door estimator; disagreement above a threshold routes to human review.\nMultiâ€‘agent debate: affirmative and negative debaters contest every edge, a judge scores coherence.\n\nIf any probe fails, the planner either tightens assumptions and reruns discovery or clearly labels the conclusion â€œinconclusive, additional data needed.â€ Final reports must surface the point estimate plus confidence intervals, sensitivity ranges, and a pass/fail tally for each refuter, so stakeholders see magnitude and robustness.\n\n\nModelâ€‘Selection Overâ€‘Fit and Crossâ€‘Estimator Disagreement\nAutoâ€‘tuning libraries can explore dozens of learners and hyperâ€‘parameters, sometimes overâ€‘fitting small causal datasets, especially with flexible models like causal forests. In this case, AutoML learns noise instead of real signal.\nMitigations include nested crossâ€‘validation inside AutoCausality or causaltune, and parsimony priors that penalize needless complexity. If resource allows, the agent should run at least two conceptually different estimators, e.g., a backâ€‘door regression and an instrumentalâ€‘variable model, and flag any large divergence in effect size as a redâ€‘flag for human review.\n\n\nComputationÂ Cost vs.Â Realâ€‘Time Ambitions\nA plannerâ€“solver split can still burn thousands of tokens and heavy compute if the planner explores many whatâ€‘if branches.\nProduction dashboards cache discovery and refutation outputs keyed by a DAG hash; if the graph hasnâ€™t changed, the agent reâ€‘uses prior results. Another recipe is distilling a heavy LLM planner into a small fineâ€‘tuned local model covers dayâ€‘toâ€‘day traffic, while the costly cloud model handles weekly deep dives.\n\n\nPrivacy and Governance\nSensitive data such as medical records, customer logs usually cannot leave a private cluster.\nHybrid deployments solve this: an onâ€‘prem LLM handles dataâ€‘aware steps, while a redacted summary (no PII) is sent to a cloud model for highâ€‘level planning. All explanations pass through a redaction layer before logging, and every causal report carries an audit trail plus roleâ€‘based access controls."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#conclusion",
    "href": "posts/20250712-agentic-causal/index.html#conclusion",
    "title": "Agentic Causal Inference",
    "section": "Conclusion",
    "text": "Conclusion\nCausal inference is transitioning from a highly specialized skill to a widely accessible capability. Thatâ€™s not putting anyone out of a job. It is freeing us to ask better questions. A couple of years ago, answering â€œwhat actually drives our north star metric?â€ meant a quarter-long project. Today, it may be weeks or even days. Thatâ€™s not just a productivity gain. It is a fundamental change in how we can think about our businesses."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html",
    "href": "posts/20230802-cloudrun-githubaction/index.html",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "",
    "text": "Deploying ML models and other python apps to cloud can be tedious. Compute instances need to be provisioned; networking needs to be sorted out; autoscaling needs to be configured; secrets and credentials need to be safely managed.\nRather than spending hours on the above Dev-Ops tasks (donâ€™t get me wrong, Dev-Ops and ML-Ops are important), I would like to focus on modeling: recipes that produce the best models and make them available for people to use. After years and many projects, I found Google Cloud Run to be a low maintainence solution, with CI/CD managed by Github Action. Similar solutions can be had with AWS ECS and Azure Container Instances. But this post will focus on Cloud Run."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "href": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along with the tutorial, you need:\n\nDocker\nGoogle Cloud SDK"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "href": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Sample App",
    "text": "Sample App\nLetâ€™s start from a very simple http server and run it locally.\ndocker run --rm -it -p 801:801 python:3.8-slim python -m http.server 801 -d /home/\nRun it locally and we can verify it works by visiting localhost:801 in a browser.\n\nDeploy to Cloud Run manually\nHowever, the above docker image does not quite work for Cloud Run, as Cloud Run requires your app in the docker image to use the PORT environment variable to determine which port the app listens to.\nTo solve this we need to build a simple docker image with the following Dockerfile:\nFROM python:3.8-slim\nENV PORT=8080\nCMD python -m http.server $PORT -d /home\nInstall gcloud and authenticate. Then build and deploy it with the following script (click to expand):\n\n\n\n\n\n\nShell script for deploy to google cloud run\n\n\n\n\n\n# Make sure to fill in the GCP project id:\nproject=your-gcp-project-id\napp=example-app\nplatform=linux/amd64\nregion=us-central1\ndocker build --platform $platform -t example-app-image .\n\nimage=us.gcr.io/$project/$app:latest\ndocker tag example-app-image $image\ndocker push $image\ngcloud run deploy $app --image $image --cpu 1 --memory 1Gi --min-instances 1 --region $region --allow-unauthenticated\n\n\n\nNote that there are a couple of hard-coded defaults like the region (us-central1), and image subdomain (us.gcr.io). Feel free to adjust.\nIf successful, we will see something like this:\n\n\n\n\n\n\nConsole output during deployment\n\n\n\n\n\nDeploying container to Cloud Run service [example-app] in project [your-project-id] region [us-central1]\nâœ“ Deploying new service... Done.                                                 \n  âœ“ Creating Revision...                                                         \n  âœ“ Routing traffic...                                                           \n  âœ“ Setting IAM Policy...                                                        \nDone.                                                                            \nService [example-app] revision [example-app-...] has been deployed and is serving 100 percent of traffic."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "href": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Manage secrets",
    "text": "Manage secrets\nIf the app needs to access secrets such as API keys and passwords, then it is a necessary to store and manage them securely.\nCreate a secret in GCPâ€™s secret manager, and grant minimal necessary access.\nEach secret is versioned. For example, we may create a secret: MY_API_KEY:latest with latest being the version tag.\nWhen using gcloud run deploy to deploy the app, pass in additional arguments:\n--update-secrets=MY_API_KEY=MY_API_KEY:latest,OTHER_API_KEY=OTHER_API_KEY:latest\nIn the docker container, the secret value will be made available in the environment variable MY_API_KEY."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "href": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Set up a secure Github action for continuous deployment",
    "text": "Set up a secure Github action for continuous deployment\nWhile manually running the gcloud command is sufficient to deploy the app to Cloud Run, sometimes it can make sense to set up continuous deployment triggered by github push or release events.\n\nService account\nFirst, we need to follow these instructions to create a service account and grant some permissions:\nGo to IAM, click â€œgrant accessâ€ and set: - principal: the new service account just created - role cloud run admin - role: roles/artifactregistry.createOnPushWriter - role: Secret manager secret accessor\nGrant the default compute-engine account access to Secret Manager Secret Accessor role. Go to IAM and set: - principal: the default compute-engine service account - role: Secret Manager Secret Accessor\nGo to IAM/service accounts, click into the default compute-engine service account, then allow the new service account to use this compute engine service account: - principal: the new service account just created - role: â€œService account userâ€\n\n\n\n\n\n\nTip\n\n\n\nI spent hours debugging permission errors in the github actions and found the above steps helped resolving the errors. More info here and here. However, I suspect some of them are not necessary. Please let me know (zhangzhang.si AT gmail.com) if you have a different experience.\n\n\n\n\nDocker artifacts repository\nA docker artifacts repository must be created in the same project as the Cloud Run service (we assume the location is â€œus-central1â€):\ngcloud artifacts repositories create slack-llm --location=us-central1 --repository-format=docker\nThis artifacts repository will hold the docker image of the app.\n\n\nWorkload identify federation and keyless authentication\nFor better cloud security, Google recommends setting up keyless authentication from github actions. To do that, we need to:\n\n\n\n\n\n\nCreate a Workload Identify Pool\n\n\n\n\n\ngcloud iam workload-identity-pools create \"my-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"Demo pool\" \\\n  --description=\"My Identify Pool\"\n\n\n\n\n\n\n\n\n\nThen create a Workload Identify Provider:\n\n\n\n\n\ngcloud iam workload-identity-pools providers create-oidc \"my-provider\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"my-pool\" \\\n  --display-name=\"Demo provider\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.aud=assertion.aud\" \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\"\n\n\n\n\n\n\n\n\n\nThen allow authentications from the Workload Identity Provider to impersonate the desired Service Account:\n\n\n\n\n\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/attribute.repository/my-org/my-repo\"\nAlternatively, if we do not want to restrict the binding to the specific github repo, then:\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/*\"\n\n\n\n\n\nGithub secrets\nAdd the following github secrets (see instructions on how to add secrets to a github repo):\nWIF_PROVIDER=projects/my-gcp-project-number/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n\nWIF_SERVICE_ACCOUNT=my-service-account@my-project.iam.gserviceaccount.com\n\n\nGithub action yaml file\nNow we should be ready to set up the actual github action. This is a redacted version of my working github action yaml file:\n\n\n\n\n\n\nYAML File\n\n\n\n\n\nYAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}\n\n\n\nPut this in .github/workflows/deploy.yml and the next time you push a change to main, it should automatically deploy to Cloud Run.\nEnjoy!"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "href": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "YAML for Github Action",
    "text": "YAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}"
  }
]
[
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html",
    "href": "posts/20260222-dl-optimizers-history/index.html",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "",
    "text": "Optimizers are the most consequential part of deep learning that most practitioners never examine closely. You pick one, set a few hyperparameters, and move on.\nThat trust is mostly earned. AdamW has been the safe bet for years. But optimizers have a long, eventful history, and the landscape is shifting again. This post traces how we got here, what actually changed at each stage, and where it’s heading."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#mental-model",
    "href": "posts/20260222-dl-optimizers-history/index.html#mental-model",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "Mental model",
    "text": "Mental model\nThe goal of training is to find model parameters that minimize error on the data. Backpropagation computes the gradient — a direction that tells each parameter how to change to reduce the loss. But that gradient comes from a mini-batch, not the full dataset, so it’s noisy. The optimizer decides what to actually do with it: how far to step, how to smooth out the noise, and what to remember from previous steps.\nThose decisions break into four controls:\n\nDirection: where to move.\nStep size: how far.\nStability: how to survive noise and curvature.\nResource cost: memory, compute, and communication.\n\nIn notation: at step \\(t\\), the optimizer converts gradient \\(g_t\\) into update \\(\\Delta \\theta_t\\). Most of the history below is about doing that conversion better. Each wave solved one bottleneck, then exposed the next one."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#visual-intuition",
    "href": "posts/20260222-dl-optimizers-history/index.html#visual-intuition",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "Visual intuition",
    "text": "Visual intuition\nHere’s a quick look at what an optimizer actually does, through two examples. The model below is a small 2-layer neural network trained on a toy classification task (120 epochs, seed 42). It has a few hundred parameters — far too many dimensions to visualize directly. To make the trajectory visible, we use PCA to project the model’s parameters at each training step down to a single 2D point. The surface shows the loss landscape in that projected plane.\n\n\n\nAdam navigates the loss landscape with shorter, more directed steps — its per-coordinate scaling adapts to the local curvature.\n\n\n\n\n\nSGD+momentum takes a wider, more oscillatory path — it uses a single global step size and relies on momentum to dampen the zig-zagging.\n\n\nIn this toy task, Adam converges faster. That’s not always the case — SGD + momentum with a well-tuned schedule often generalizes better, especially for convolutional architectures.\nVisualizations generated with loss-landscape-anim. Repro steps and code."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#s-to-2000s-foundations-that-never-went-away",
    "href": "posts/20260222-dl-optimizers-history/index.html#s-to-2000s-foundations-that-never-went-away",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "1960s to 2000s: foundations that never went away",
    "text": "1960s to 2000s: foundations that never went away\nThe key ideas predate modern deep learning:\n\nPolyak momentum (heavy ball) reduced zig-zag behavior in narrow valleys by accumulating a velocity term (Polyak, 1964):\n\n\\[\nv_{t+1} = \\beta\\, v_t + g_t,\\qquad \\theta_{t+1} = \\theta_t - \\eta\\, v_{t+1}.\n\\]\n\nNesterov acceleration improved on this by evaluating the gradient at a look-ahead position \\(\\theta_t - \\eta\\beta\\, v_t\\) rather than at \\(\\theta_t\\) itself (Nesterov, 1983).\nNatural gradient reframed descent in information geometry (Amari, 1998).\n\nMomentum smoothed noisy gradients, Nesterov improved convergence speed, and natural gradient sought invariance to parameterization. Together they set the long-term agenda — and defined a tension that recurs throughout this story: conditioning quality versus operational cost."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#to-2014-getting-deep-nets-to-train-at-all",
    "href": "posts/20260222-dl-optimizers-history/index.html#to-2014-getting-deep-nets-to-train-at-all",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "2010 to 2014: getting deep nets to train at all",
    "text": "2010 to 2014: getting deep nets to train at all\nEarly deep learning leaned on SGD + momentum because it was cheap and scalable, but tuning was fragile (Sutskever et al., 2013).\nIn practice, engineers could train deeper models, but only with careful learning-rate schedules and a lot of trial and error.\nAdaptive methods arrived quickly:\n\nAdaGrad (2011): per-coordinate scaling, strong for sparse settings (Duchi et al., 2011).\nRMSProp (2012): moving second-moment estimate to avoid AdaGrad’s monotonic decay (Hinton lecture notes, 2012).\nAdaDelta (2012): reduced global LR sensitivity (Zeiler, 2012).\n\nBy the end of this period, the design direction was clear: momentum-like smoothing plus adaptive scaling. That combination set up Adam’s rapid adoption in the next wave."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#to-2019-adam-wins-adamw-corrects",
    "href": "posts/20260222-dl-optimizers-history/index.html#to-2019-adam-wins-adamw-corrects",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "2014 to 2019: Adam wins, AdamW corrects",
    "text": "2014 to 2019: Adam wins, AdamW corrects\nAdam became the default because it reduced tuning friction across workloads (Kingma and Ba, 2014).\nIts core update combines first and second moments:\n\\[\nm_t=\\beta_1 m_{t-1}+(1-\\beta_1)g_t,\\quad\nv_t=\\beta_2 v_{t-1}+(1-\\beta_2)g_t^2,\\quad\n\\Delta\\theta_t=-\\eta\\frac{\\hat m_t}{\\sqrt{\\hat v_t}+\\epsilon}.\n\\]\nTwo important caveats emerged:\n\nAdam showed convergence pathologies in certain settings, which the AMSGrad fix addressed by tracking max second moments (Reddi et al., 2018).\nAdding L2 regularization to the gradient (standard “weight decay” in Adam) is not equivalent to true weight decay under adaptive preconditioning.\n\nAdamW fixed the second issue by decoupling weight decay (Loshchilov and Hutter, 2017). This was a small implementation change with large impact in real training runs. With decoupling, shrinkage is explicit:\n\\[\n\\theta_{t+1} = (1-\\eta\\lambda)\\theta_t + \\Delta\\theta_t^{\\text{adam}}.\n\\]"
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#to-2023-curvature-approximation-at-scale",
    "href": "posts/20260222-dl-optimizers-history/index.html#to-2023-curvature-approximation-at-scale",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "2015 to 2023: curvature approximation at scale",
    "text": "2015 to 2023: curvature approximation at scale\nFull second-order methods (computing the Hessian) are too expensive for large models. Two lines of work made curvature information practical:\n\nK-FAC approximated the Fisher information matrix with Kronecker-factored blocks, giving stronger preconditioning than Adam at modest extra cost (Martens and Grosse, 2015).\nShampoo extended this to per-block matrix preconditioners with scalable update rules (Gupta et al., 2018).\n\nNeither became a broad default — the implementation complexity and tuning overhead exceeded what most teams would absorb. But they proved that structured preconditioning could outperform diagonal methods. That lineage matters: the 2024-2026 conditioning wave reuses the same motivation with simpler operational surfaces."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#to-2020-large-batch-pressure",
    "href": "posts/20260222-dl-optimizers-history/index.html#to-2020-large-batch-pressure",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "2017 to 2020: large-batch pressure",
    "text": "2017 to 2020: large-batch pressure\nAs batch sizes grew, optimization dynamics changed. The naive fix — scale the learning rate linearly with batch size — breaks down because different layers have different gradient magnitudes. A single global learning rate either over-updates some layers or under-updates others.\n\nLARS solved this with layer-wise trust ratios: scale each layer’s update by the ratio of weight norm to gradient norm (You et al., 2017).\nLAMB brought the same idea to Adam-style moments for large-batch language pretraining (You et al., 2019).\n\nThese methods solved throughput bottlenecks, even though AdamW remained the broad default. The key lesson was systems-driven: at scale, optimizer choice is partly a hardware-efficiency decision, not only a convergence decision."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#to-2023-memory-and-systems-become-first-class",
    "href": "posts/20260222-dl-optimizers-history/index.html#to-2023-memory-and-systems-become-first-class",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "2018 to 2023: memory and systems become first-class",
    "text": "2018 to 2023: memory and systems become first-class\nAt Transformer scale, optimizer state is expensive. Adam stores two extra fp32 tensors per parameter (first and second moments), so optimizer state alone is 2x the model size — about 56 GB for a 7B-parameter model.\n\nAdafactor reduced second-moment memory with factorization (Shazeer and Stern, 2018).\n8-bit optimizer states reduced memory pressure in practice (Dettmers et al., 2021).\nCommunication-aware variants targeted distributed bandwidth (Tang et al., 2021).\n\nThe best optimizer is not only mathematically elegant; it must fit systems constraints. For many teams, this is why AdamW kept winning despite stronger niche alternatives."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#to-2023-generalization-aware-and-tweak-heavy-era",
    "href": "posts/20260222-dl-optimizers-history/index.html#to-2023-generalization-aware-and-tweak-heavy-era",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "2020 to 2023: generalization-aware and tweak-heavy era",
    "text": "2020 to 2023: generalization-aware and tweak-heavy era\nSAM (Sharpness-Aware Minimization) penalizes sharp minima by optimizing for the worst-case loss in a small neighborhood around the current parameters — the idea being that flatter minima tend to generalize better (Foret et al., 2020):\n\\[\n\\min_\\theta \\max_{\\|\\epsilon\\| \\le \\rho} \\mathcal{L}(\\theta + \\epsilon).\n\\]\nIn practice, the inner max is approximated with a single gradient ascent step: \\(\\hat\\epsilon = \\rho\\, g / \\|g\\|\\), then the model is updated using the gradient at \\(\\theta + \\hat\\epsilon\\).\nMany variants (Lookahead, RAdam, AdaBelief, AdaBound) tuned warmup and update coupling (Lookahead, RAdam, AdaBelief, AdaBound). Some helped in niches, but few replaced AdamW/SGD defaults broadly.\nLion used the sign of a momentum-like term as the update — discovered through automated program search rather than manual design (Chen et al., 2023). It validated optimizer design as a search problem, though it did not displace AdamW broadly."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#to-2025-geometry-returns",
    "href": "posts/20260222-dl-optimizers-history/index.html#to-2025-geometry-returns",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "2024 to 2025: geometry returns",
    "text": "2024 to 2025: geometry returns\nMuon-style methods reframed the problem. Adam scales each parameter independently — it doesn’t account for how parameters within a layer interact. In practice, a few dominant directions in the weight matrix can hog the update while others are neglected. Muon reshapes the gradient to spread the update more evenly across directions (Muon implementation, modular-duality framing).\nThe key operation is polar decomposition. For a gradient matrix \\(G\\), any matrix can be decomposed as \\(G = U S\\) where \\(U\\) is orthogonal and \\(S\\) is symmetric positive semi-definite. Muon uses \\(U\\) — the orthogonal factor — as the update direction:\n\\[\n\\Delta\\theta = -\\eta\\, \\text{polar}(G), \\quad \\text{where } \\text{polar}(G) = G\\,(G^\\top G)^{-1/2}.\n\\]\nThis ensures the update has equal magnitude across all directions, preventing any single direction from dominating. In practice, the matrix inverse square root is approximated cheaply using a few iterations of Newton-Schulz, avoiding the cost of a full SVD.\nEarly results showed Muon scaling competitively to 1B+ parameters (Jordan et al., 2025), with a simpler implementation surface than K-FAC or Shampoo.\nBy end-2025, this looked promising but not universal. AdamW still dominated documented frontier recipes. So the open question entering 2026 became replication at larger scales, not just first-paper wins."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#late-2025-to-early-2026-conditioning-wave",
    "href": "posts/20260222-dl-optimizers-history/index.html#late-2025-to-early-2026-conditioning-wave",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "Late-2025 to early-2026: conditioning wave",
    "text": "Late-2025 to early-2026: conditioning wave\nA broader conditioning-focused wave followed. All of these methods share a common structure: transform the gradient \\(G\\) through some structured preconditioner \\(P\\) before updating:\n\\[\n\\Delta\\theta = -\\eta\\, P(G).\n\\]\nIn base Muon, \\(P(G) = \\text{polar}(G)\\). The new methods modify what goes into \\(P\\), what \\(P\\) does, or what happens after:\nNorMuon adds per-neuron adaptive scaling after orthogonalization. It tracks a row-wise running second moment of the orthogonalized update \\(O_t = \\text{polar}(M_t)\\), then normalizes each row by its own RMS (2025) [1B+, open-source]:\n\\[\nv_t = \\beta_2\\, v_{t-1} + (1-\\beta_2)\\,\\text{mean}_{\\text{cols}}(O_t \\odot O_t), \\qquad \\hat{O}_t = O_t \\,/\\, (\\sqrt{v_t} + \\epsilon).\n\\]\nThis is essentially Adam-style adaptive scaling, but applied per neuron to the post-orthogonalization update rather than per element to the raw gradient.\nMARS-M modifies what goes into the Muon pipeline. It replaces the raw stochastic gradient with a variance-reduced estimator before momentum and orthogonalization (2025) [theory, small-scale, open-source]:\n\\[\nC_t = \\nabla f(\\theta_t, \\xi_t) + \\gamma_t \\tfrac{\\beta}{1-\\beta}\\bigl[\\nabla f(\\theta_t, \\xi_t) - \\nabla f(\\theta_{t-1}, \\xi_t)\\bigr].\n\\]\nThe correction term uses the same mini-batch \\(\\xi_t\\) at both the current and previous iterate, reducing the variance of the momentum estimate at the cost of one extra gradient evaluation per step.\nHyperparameter transfer for matrix preconditioners showed that conditioning gains persist when transferring optimizer configs from small to large runs, making these methods more practical to tune (2025) [1B+, protocol].\nTEON changes the structure of the orthogonalization. Where Muon orthogonalizes each layer’s gradient independently, TEON stacks \\(K\\) same-shape layer gradients into a tensor, unfolds along a chosen mode, and orthogonalizes the unfolded matrix (2026) [theory, 1B-range]:\n\\[\n\\mathcal{O}_i(\\mathcal{G}) = \\mathcal{M}_i^{-1}\\!\\bigl(\\text{polar}\\bigl(\\mathcal{M}_i(\\mathcal{G})\\bigr)\\bigr),\n\\]\nwhere \\(\\mathcal{M}_i\\) is the mode-\\(i\\) matricization (unfolding) of the stacked tensor \\(\\mathcal{G} \\in \\mathbb{R}^{m \\times n \\times K}\\). This captures cross-layer correlations that per-layer Muon misses.\nARO changes the coordinate system in which the optimizer operates. It selects an adaptive rotation \\(R_t\\) that maximizes instantaneous loss decrease, rather than using the gradient’s own eigenstructure (2026) [1B+, protocol]:\n\\[\n\\Delta W_t = -\\eta\\, R_t\\, f_t(R_t^\\top M_t), \\qquad R_t = \\text{QR}\\!\\bigl(M_t\\, f_t(R_{t-1}^\\top M_t)^\\top\\bigr).\n\\]\nThe rotation at step \\(t\\) depends on the previous step’s rotated projection, creating a feedback loop between the rotation and the base optimizer. Base Muon is a special case where \\(R_t\\) is fixed to the eigenvectors of \\(M_t M_t^\\top\\).\nThe shift: conditioning is becoming a primary design axis, not a side detail. Evidence labels above ([1B+], [theory], etc.) indicate maturity — most of these methods have open-source implementations but limited independent replication so far.\nCommunity reports (Muon comparisons, distributed Muon validation) are useful early signals, though controlled evaluations remain more reliable before committing to large-scale runs."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#what-won-in-practice-by-early-2026",
    "href": "posts/20260222-dl-optimizers-history/index.html#what-won-in-practice-by-early-2026",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "What won in practice by early 2026",
    "text": "What won in practice by early 2026\nDefaults remain fairly stable:\n\nFrontier LLMs and VLMs (vision-language models): AdamW + warmup + decay + gradient clipping + selective decay exclusions.\nViTs (Vision Transformers): AdamW.\nCNNs: SGD + momentum remains strong.\nDiffusion and flow-matching models: Adam/AdamW, often with EMA (exponential moving average of weights).\nLARS/LAMB: useful in specific extreme-batch throughput regimes."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#recipe-chooser-2025-to-early-2026",
    "href": "posts/20260222-dl-optimizers-history/index.html#recipe-chooser-2025-to-early-2026",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "Recipe chooser (2025 to early-2026)",
    "text": "Recipe chooser (2025 to early-2026)\n\n\n\n\n\n\n\n\nSetting\nFirst choice\nWhen to deviate\n\n\n\n\nLLM/VLM pretraining\nAdamW + warmup/decay + clipping\nTry Muon/conditioning if stability or scaling efficiency is bottleneck\n\n\nVision CNN\nSGD + momentum + strong LR schedule\nUse AdamW for transformer-heavy stacks or faster early convergence\n\n\nViT training\nAdamW\nTrial SAM or conditioning methods when plateaus appear\n\n\nDiffusion/flow matching\nAdamW (+ EMA)\nTry Adafactor/low-precision states when memory dominates\n\n\nExtreme large-batch throughput\nLARS/LAMB\nStay with AdamW if batch size is moderate and tuning budget is limited\n\n\n\n\nStarting hyperparameters\nThese are typical starting points, not universal optima. Always tune on your workload.\n\n\n\n\n\n\n\n\n\n\nOptimizer\nLearning rate\nbeta1, beta2\nWeight decay\nNotes\n\n\n\n\nAdamW (LLM)\n1e-4 to 6e-4\n0.9, 0.95\n0.01 to 0.1\nWarmup 1-5% of steps, cosine decay\n\n\nAdamW (ViT)\n1e-4 to 3e-4\n0.9, 0.999\n0.01 to 0.3\nHigher decay common with strong augmentation\n\n\nSGD + momentum (CNN)\n0.01 to 0.1\nmomentum 0.9\n1e-4 to 5e-4\nStep or cosine LR schedule\n\n\nMuon\n0.01 to 0.05\n0.9, —\n0.0 to 0.01\nOrthogonalization replaces some of weight decay’s role\n\n\n\n\n\nFair comparison protocol\n\nSame model and tokenizer.\nSame token/image budget and data order.\nMatched tuning budget across optimizers.\nReport time-to-target, compute-to-target, and seed stability."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/index.html#closing",
    "href": "posts/20260222-dl-optimizers-history/index.html#closing",
    "title": "A Tour of Deep Learning Optimizers",
    "section": "Closing",
    "text": "Closing\nFrom heavy-ball momentum to conditioning-heavy methods, optimizer history is mostly a story of recurring constraints in new forms: curvature, noise, scale, and hardware budgets. Innovation keeps happening because three forces — theory (invariance, stability), empirical pressure (fewer knobs, faster convergence), and systems constraints (memory, interconnect) — keep interacting. Methods that survive usually satisfy all three.\nBy early 2026, AdamW is still the center of gravity. The next durable shift is likely to come from better directional control and structured conditioning — not just better scalar learning-rate heuristics. What to watch for in the rest of 2026:\n\nWhether conditioning methods (Muon-family, ARO, TEON) show consistent gains under independent replication at 10B+ scale.\nWhether hyperparameter transfer protocols make these methods usable without per-run tuning.\nWhether systems-level integration (fused kernels, native framework support) lowers the adoption barrier enough to challenge AdamW as the default baseline.\n\nThe optimizer that wins next will not just be mathematically better — it will be easier to deploy correctly at scale."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html",
    "href": "posts/20251030-coding-agents-spooky/index.html",
    "title": "Coding Agents and a Spooky Kaggle Challenge: Benchmarking Lightweight ML Automation",
    "section": "",
    "text": "Agentic ML tooling is having a moment. State-of-the-art systems like DeepMind’s MLE-STAR and Meta’s AIRA-dojo frequently place in the top 10% of Kaggle competitions on MLE-bench.1 They’re powerful but heavy.\nDay to day, I mainly use coding agents with a detailed text file for prompting and some shell scripts to set things up. Many of you probably do the same. I wanted to test how far that lightweight stack can go.\nTo find out, I implemented a “Memento”-style outer loop, where I encourage the coding agent to pass messages to its “future self” (the next invocation of the agent) in an organized way.\n\n\nSee my template repo for the set up. Surprisingly, you don’t need much special tooling for it to work well. There is zero Python code at the start, instead just a few boilerplate files:\n\nuser_prompt.txt – a one-page spec that instructs the agent to perform a disciplined “outer” loop (because the agent already handles the inner loop): load context, declare a budget, run 2–3 experiments, journal what worked and what didn’t, plan next steps, update the status, and save the best results.\nsetup_env.sh – stands up a virtualenv, installs pinned deps, and even pre-downloads NLTK packages so the agent never has to ask for credentials.\nprompt.sh & run_iterations.sh – thin wrappers that activate the venv, launch the chosen CLI (claude, gemini, codex, cursor), and optionally auto-commit after each loop.\n\nThat’s it. The agent sees the repo exactly like a data scientist would: git history, prior experiments, and a scratchpad of ideas. This “minimal stack” was enough for your coding agent to discover multi-seed ensembles and run 60 experiments in under a day.\nFeel free to use this agentic template. Instructions are there for trying it out yourself.\n\n\n\nThe classic Spooky Author Identification playground competition. The task is to classify short horror passages by Edgar Allan Poe, Mary Shelley, or H. P. Lovecraft. For example, which author wrote the following passage?\n“My host was now leading the way down cellar to his actual studio, and I braced myself for some hellish effects among the unfinished canvases.”"
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#why-revisit-spooky-authors-in-2025",
    "href": "posts/20251030-coding-agents-spooky/index.html#why-revisit-spooky-authors-in-2025",
    "title": "Coding Agents and a Spooky Kaggle Challenge: Benchmarking Lightweight ML Automation",
    "section": "",
    "text": "Agentic ML tooling is having a moment. State-of-the-art systems like DeepMind’s MLE-STAR and Meta’s AIRA-dojo frequently place in the top 10% of Kaggle competitions on MLE-bench.1 They’re powerful but heavy.\nDay to day, I mainly use coding agents with a detailed text file for prompting and some shell scripts to set things up. Many of you probably do the same. I wanted to test how far that lightweight stack can go.\nTo find out, I implemented a “Memento”-style outer loop, where I encourage the coding agent to pass messages to its “future self” (the next invocation of the agent) in an organized way.\n\n\nSee my template repo for the set up. Surprisingly, you don’t need much special tooling for it to work well. There is zero Python code at the start, instead just a few boilerplate files:\n\nuser_prompt.txt – a one-page spec that instructs the agent to perform a disciplined “outer” loop (because the agent already handles the inner loop): load context, declare a budget, run 2–3 experiments, journal what worked and what didn’t, plan next steps, update the status, and save the best results.\nsetup_env.sh – stands up a virtualenv, installs pinned deps, and even pre-downloads NLTK packages so the agent never has to ask for credentials.\nprompt.sh & run_iterations.sh – thin wrappers that activate the venv, launch the chosen CLI (claude, gemini, codex, cursor), and optionally auto-commit after each loop.\n\nThat’s it. The agent sees the repo exactly like a data scientist would: git history, prior experiments, and a scratchpad of ideas. This “minimal stack” was enough for your coding agent to discover multi-seed ensembles and run 60 experiments in under a day.\nFeel free to use this agentic template. Instructions are there for trying it out yourself.\n\n\n\nThe classic Spooky Author Identification playground competition. The task is to classify short horror passages by Edgar Allan Poe, Mary Shelley, or H. P. Lovecraft. For example, which author wrote the following passage?\n“My host was now leading the way down cellar to his actual studio, and I braced myself for some hellish effects among the unfinished canvases.”"
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#what-i-learned",
    "href": "posts/20251030-coding-agents-spooky/index.html#what-i-learned",
    "title": "Coding Agents and a Spooky Kaggle Challenge: Benchmarking Lightweight ML Automation",
    "section": "What I learned",
    "text": "What I learned\nI ran four different coding agents (Codex, Claude, Gemini and Cursor2) on the Spooky Author Identification task. Each agent is allowed to run 10 iterations. Each agent ended up spending somewhere from 2.5 hours to 13 hours having fun on my mac mini. Here’s what I learned:\nCoding agents are quite capable and trained a strong baseline model. I expected the agents to struggle because I didn’t provide specific instructions or tools. Author attribution from short text snippets can be tricky, requiring models to pick up on stylistic tics and long-tail vocabulary patterns. But watching them work, I realized they figure out quite a few things on their own, helping me discover good ingredients I didn’t think of. All of the agents matched or beat the Kaggle median (and Kaggle participants know a thing or two about data science), some comfortably above the median. None of them have reached the bronze cutoff (top 10%) yet. But, there are lot of room for improving how I use them. My prompt is probably far from optimal. The number of iterations may not be enough. And the coding agent was not on the strongest LLM yet.\nAutomation moves fast but leaves a mess. All four agents sprawled dozens of experiment entries, cached matrices, and partially refactored modules. They ship improvements, yet still requires a human expert to reconcile redundant scripts, prune dead notebooks, and audit for data leakage before anything is production ready. I expect cleanup will take significant time.\nThe last mile remains human. Claude pushed CV log loss down to 0.3447 with a TF-IDF + MLP ensemble, better than the others but still a hike away from the bronze cutoff (top 10%). Agentic ML tools democratize experimentation, but careful experiment design, data quality checks, and smart engineering still demand deliberate expertise. I do not see myself and fellow folks out of job any time soon, and I am confident we can all adapt, adopt, and super-charge ourselves."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#how-the-agents-actually-performed",
    "href": "posts/20251030-coding-agents-spooky/index.html#how-the-agents-actually-performed",
    "title": "Coding Agents and a Spooky Kaggle Challenge: Benchmarking Lightweight ML Automation",
    "section": "How the agents actually performed",
    "text": "How the agents actually performed\nThe main metric is log loss (lower is better). The Kaggle leaderboard median sits at 0.4188, and the bronze cutoff (top 10%) is 0.2938. The table below shows each agent’s score compared to that median: negative numbers mean the agent beat the median.\n\n\n\n\n\n\n\n\n\nAgent\nLog Loss\nvs. Kaggle Median\nSummary\n\n\n\n\nClaude\n0.37222 (reached 0.3447 but regressed)\n-0.047\n3-seed TF-IDF + MLP blend. Stacking meta-learner backfired.\n\n\nCodex\n0.35897\n-0.060\nMulti-seed TF-IDF + logistic ensemble.\n\n\nComposer\n0.38715\n-0.032\nTF-IDF + logistic with sublinear scaling. Exploring vocab tuning.\n\n\nGemini†\n0.42398\n+0.005\nWord+char features. Over-regularized at first (C=0.1 → 0.696).\n\n\n\n† Gemini ran on gemini-flash-2.5 because my gemini-pro-2.5 quota was exhausted. Sorry Gemini! Will give the pro version a try another time.\nIs there a clear winner among the four agents? Not quite. With one task and one run per agent, these deltas are easily within noise. No overall champion crowned. But three of four agents beat the Kaggle median on their first serious attempt, which suggests the orchestration pattern itself is sound.\nWhat struck me was how differently each agent approached the problem. Codex went wide with multi-seed ensembles. Claude pushed harder on model architecture (adding MLPs). Composer methodically swept hyperparameters. Gemini (2.5-flash) stumbled early with over-regularization but corrected course. Each strategy reflects the underlying model’s tendencies, but all converged on TF-IDF as the feature foundation.\n\n\nRepresentative session logs (expand for highlights from each agent’s run)\n\n\nCodex full sweep: five-model ensemble where stylometric probabilities took 50% of the final weight\nComposer mid-run: Oct 29 session documented a 4.4% log-loss drop after raising max_features to 25k\nClaude iteration: stacking meta-learner chewed 138 minutes only to land 10.4% worse than simple blend\nGemini early run: over-regularization with C=0.1 spiked to 0.696 log loss before correcting course"
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#what-the-agents-leave-behind",
    "href": "posts/20251030-coding-agents-spooky/index.html#what-the-agents-leave-behind",
    "title": "Coding Agents and a Spooky Kaggle Challenge: Benchmarking Lightweight ML Automation",
    "section": "What the agents leave behind",
    "text": "What the agents leave behind\nLooking at the work products, I realize the hardest part is not the setup or the experimentation. It is the cleanup.\nAutomation litters the repo. Codex’s run now tracks five variants of the same logistic pipeline, complete with separate OOF dumps, weight search scripts, and registry YAMLs. Composer’s train.py mixes LightGBM, logistic regression, handcrafted features, and a sentence-transformer branch inside a single file that keeps toggling SKIP_EXISTING_EXPERIMENTS. The agents do not delete anything; they prototype, leave artifacts behind, and move on.\nThere is valuable signal inside the noise. The streaming JSON logs and the journal files double as a lab notebook: one Codex session diagnosed short texts (21–81 characters) as the chief failure mode (76.8% accuracy and 0.566 log loss vs. 93.7% / 0.204 for long passages) and immediately reprioritized feature work around that gap. That’s valuable signal buried in transcript.\nThe messy bits are the code paths, not the telemetry. Before shipping any of this to production, you need to:\n\nRestore clean, reproducible code – to make your life easier.\nAudit data usage – to be sure the metrics are real, check if there is data leakage, faulty experiental design, and stress test the model on a hold out dataset.\nNormalize experiment logging – ensure experiments.csv retains consistent schemas so future analysis can reason about which parameters actually mattered.\n\nEven if cleanup requires a lot of time, I still view agentic ML as a net win (they generated hypotheses and ran experiments faster than I could manually), but it’s not as simple as pressing button to get the solution.\n\n\nDetailed experiment progression (expand for iteration-by-iteration findings)\n\n\nCodex\n\nIteration 1: Word+char TF-IDF logistic slashed log loss from 0.4660 to 0.3875 and surfaced author-specific tokens (Poe’s “of the/upon”, Lovecraft’s “though/west”, Shelley’s character cues), validating the baseline. Journal\nIterations 2–4: Cross-seed checks and char-vocabulary diagnostics showed the 0.3819 gain was variance-prone. Not every bet landed: 256-component SVD exploded to 0.59 log loss. Journal 1 · Journal 2\nIteration 5: Built out OOF persistence, averaged three min_df=2 seeds plus a min_df=3 variant, trimming log loss to 0.3764 while keeping training strictly on folds. Journal\nIteration 6: Diagnostics catalogued LightGBM’s collapse (≥0.49 log loss) and the HPL→EAP confusion hotspot (585 errors), steering work toward Lovecraft-specific features. Journal\nIterations 7–9: Further C tuning, repeated-CV variance checks, and Lovecraft token whitelists, concluding remaining gains require smarter feature curation rather than more seeds. Journal 1 · Journal 2\nClock time: ~4 h 52 m between first and last logs (idle gaps included).\n\n\n\nComposer\n\nKickoff: TF-IDF + logistic regression established a 0.4811 baseline; LightGBM and quick ensembles underperformed, proving the task is mostly linear. Journal\nEarly experiments: Sentence-transformer embeddings bombed at 0.6715, underscoring that semantics alone can’t beat stylistic n-grams for authorship. Logistic tuning (C≈5) slashed log loss to 0.4522. Journal 1 · Journal 2\nMid-run: Joint tuning of C and vocabulary width delivered the biggest gains, with most lift from expanding max_features to 10k and 25k, dropping to 0.4275. Journal\nLate game: Coordinated tuning of C, n-gram ranges, and vocabulary size landed at 0.3984; adding sublinear_tf=True with C=4.5 finished at 0.3943 and shipped the submission. Along the way, a “stylometric booster” cratered performance to 0.680. Journal 1 · Journal 2\nClock time: ~6 h 59 m across Composer’s logged iterations.\n\n\n\nClaude\n\nBaseline: Logistic regression beat LightGBM (0.43 vs 0.59) out of the gate, confirming sparse TF-IDF prefers linear models. Early error analysis quantified the short-text tax (24.7% error under 10 words). Journal\nEarly tuning: Lower regularization (C≈10) tightened CV log loss to 0.3814, revealing dominant confusions (MWS→EAP 10.7%, HPL→EAP 10.1%). Journal 1 · Journal 2\nBreakthrough: A modest MLP (256→128) hit 0.3656. Learning-rate tuning dropped it to 0.3519, and blending it 30/70 with logistic yielded a robust 0.3495. Journal 1 · Journal 2\nPeak: Averaging three seeds for each model produced the 0.3447 best-in-class ensemble without destabilizing variance. Journal\nCautionary tail: Stacking meta-learner ground for 2.3 hours, overshooting budget, yet finished 10.4% worse than the simple blend. Five seeds and batch norm also backfired (2.72% and 36% worse). Journal 1 · Journal 2\nClock time: ~13 h 55 m between Claude’s earliest and latest logs (stacking iterations account for much of it).\n\n\n\nGemini (2025-10-30)\n\nBaseline: Initial TF-IDF logistic run clocked 0.557 log loss, giving the agent a concrete hill to climb. An overly strong regularization run (C=0.1) erupted to 0.696 before the agent corrected course. Journal\nFeature mix: Adding char n-grams and text-length features while relaxing C dropped CV log loss into the 0.43 range. Journal\nCurrent best: Tuning character sublinear_tf/use_idf combinations landed at 0.4299 CV and 0.42398 on the grader, still above the Kaggle median but short of medal territory. Journal\nClock time: ~2 h 50 m for the logged Gemini run (on gemini-flash-2.5).\n\nRun-time estimates derive from file modification times of the earliest and latest logs/*.log entries; they include idle gaps between iterations."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#appendix-full-repository-links",
    "href": "posts/20251030-coding-agents-spooky/index.html#appendix-full-repository-links",
    "title": "Coding Agents and a Spooky Kaggle Challenge: Benchmarking Lightweight ML Automation",
    "section": "Appendix: Full repository links",
    "text": "Appendix: Full repository links\nAll code, experiment artifacts, and raw stream-json transcripts are public:\n\nCodex: Status · Setup script · Session log\nComposer: Status · Early session · Final session\nClaude: Status · Session log\nGemini: Status · Session log"
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#footnotes",
    "href": "posts/20251030-coding-agents-spooky/index.html#footnotes",
    "title": "Coding Agents and a Spooky Kaggle Challenge: Benchmarking Lightweight ML Automation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMLE-bench is a benchmark comprising 75 Kaggle competitions for evaluating ML agents (GitHub repo). MLE-bench lite is a curated subset of 22 tasks. Both MLE-STAR and AIRA-dojo use bespoke multi-agent frameworks: MLE-STAR combines web search for model discovery with targeted refinement guided by ablation studies (Nam et al., 2025), achieving medals in 64% of MLE-bench lite competitions. AIRA-dojo provides specialized operators and multiple search policies (greedy, MCTS, evolutionary) to explore solution spaces (research paper), achieving 47.7% medal rate on MLE-bench lite.↩︎\nCursor with the new composer-1 model.↩︎"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html",
    "href": "posts/20241002-minisora-part1/index.html",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#introduction",
    "href": "posts/20241002-minisora-part1/index.html#introduction",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "href": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Pareto frontier: Aiming for good and small",
    "text": "Pareto frontier: Aiming for good and small\n\n\n\n\n\nThis graph from “Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI” illustrates a key challenge in AI development. SORA would be a frontier model at the resource-intensive end of the spectrum. We want to move in the direction of the green arrow, striving for lower training cost while maintaining high quality.\nAccess to vast computational resources, such as 10,000 A100 GPUs, is limited to a handful of organizations. Even if such resources were widely available, focusing solely on resource-intensive methods would be an inefficient use of our capabilities. The design space for training recipes is vast, and a strategic approach involves exploring this space through low-cost experiments before scaling up when confidence is high.\nThis raises an intriguing question: With a modest budget, is it possible to train a general-purpose video generation model comparable to SORA?"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "href": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "The need for controlling the domain complexity",
    "text": "The need for controlling the domain complexity\n\nThe challenge of training general-purpose models with limited resources\nSORA’s training costs likely run into tens of millions of dollars, driven by both data and model size. Larger datasets necessitate longer training times, while bigger models require both extended training periods and more high-end GPUs.\nIs it feasible to train a high-quality model with a significantly smaller dataset? This seems impossible due to the inherent complexity of our world. Are one million video clips sufficient to capture our world’s complexity? 10 Million? 100 Million? Probably more than that. While sample-efficient algorithms can help reduce the required data size, the order of magnitude for necessary data likely remains substantial.\nSimilarly, training a high-quality model with a much smaller architecture presents its own challenges. Unless a dramatically more efficient architecture than Transformers emerges, a small model would struggle to capture the complexity present in such vast datasets.\nTherefore, to make progress with limited resources, we must find ways to reduce the data size.\n\n\nExploring niche domains: A path to low-budget training\nNiche domains can be significantly simpler than our physical world, potentially allowing a few tens of thousands of observations to sufficiently represent the domain. With a drastic reduction in data size, smaller models and lower training costs become feasible.\nWe can conceptualize a series of domains, progressing from simple to complex:\n\n2D Flying MNIST (a 2D world with colorful handwritten digits moving at constant speed, bouncing off boundaries)\n2D arcade games (Pong, Breakout, etc.)\nAnime and cartoons\nLimited locations: video walkthroughs of 3D house models, fly-through views of objects (e.g., NERF models)\nLimited objects: close-up videos of specific subjects (e.g., dogs, selfie videos)\nLimited scenery: footage of hiking trails, beaches, etc.\nPublic video datasets: UCF-101, Panda-70M, InterVid, etc.\nThe real world, and our collective video reservoir.\n\nA strategic approach involves starting from the simplest domain and gradually progressing towards more complex ones. Effective training recipes discovered in simpler domains are expected to scale to more complex scenarios with straightforward increases in data and model size.\nInterestingly, this mirrors how humans learn: start from simple lessons and gradually build up to more complex concepts.\n\n\nPre-train or fine-tune?\nFine-tuning is an effective strategy to reduce training costs, but it comes with certain limitations:\n\nFixed architecture: The model’s architecture is predetermined, which can be a significant constraint as we may still be far from an optimal design for video generation tasks.\nVAE dependency: Pre-trained weights often rely on a specific Variational Autoencoder (VAE), limiting the design space and opportunities to further reduce training costs.\n\nDespite these limitations, fine-tuning has shown promising results. For example, the team at Lambda Labs open-sourced an intriguing Text2Bricks experiment, fine-tuning OpenSora weights on Lego videos. This project required approximately 1000 A100 GPU hours and 10,000 videos. We can anticipate further reductions in cost as more advanced pre-trained models become available and more sample-efficient fine-tuning algorithms are developed.\nFor my experiments, I try to find the simplest domain that has non-trivial complexity: a toy 2D world with flying digits. The scale of this toy world is small enough that pre-training from scratch is not prohibitively expensive, allowing for more freedom in exploring different model architectures and training strategies.\nLet’s see some details."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "href": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Flying MNIST Simulator",
    "text": "Flying MNIST Simulator\nA Python script is used to simulate a toy 2D world where colorful handwritten digits fly and bounce around. An example is shown below.\n\n\n\n\n\nFor training, I used up to 100k clips, each with 32 frames, covering roughly 6 seconds at 5 fps. This amounts to 160 hours of video. Is this a lot? Let’s compare with human learning. If a baby is awake and actively observing 5 hours per day, it would be roughly a month of learning. It would be interesting to see if the AI can learn:\n\nObject identity: a digit is a digit, and not a random blob\nObject permanence: a digit does not suddenly disappear\nDistinct digits: whether the model can learn to distinguish between different digits\nConsistent colors: color of a digit remains consistent\nPhysics: digits follow simple physics - constant speed and bounce off walls"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "href": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "VAE: The Compressor",
    "text": "VAE: The Compressor\nThe first model to train is a compressor. Unlike language, images and videos have very high dimensionality: a tiny 2-second 256x256 video contains over 100 million numbers. Compression is necessary for the model to work.\nThe compressor of choice is a VAE (Variational Auto-Encoder) with an encoder and decoder. The encoder converts a video clip into a latent space, and the decoder converts the latent space back to a video clip. The latent space is a compact representation of the original data and is easier to model.\nOptionally, you can quantize the latent space using vector quantization, which gives you a VQ-VAE. Quantization gives rise to a vocabulary of visual words or tokens. This enables the use of language model training recipes on 1-dimensional (flattened) sequences of token IDs. While I was initially skeptical, the results were surprisingly good.\nTraining a small VAE is relatively quick. I trained a spatial-temporal VQ-VAE with 4x temporal compression and 4x4 spatial compression, using a vocabulary size of 5120. The training run documented in Weights & Biases achieved a good balance of reconstruction quality and compression rate. It took about 2 A10 GPU hours to converge.\nWith this VAE model, you can transform a 32-frame video clip (32 x 3 x 256 x 256) into latent “tokens”. Without quantization, the compressed representation of the video has a shape of 8 x 4 x 64 x 64 (each “token” is a 4-dimensional floating point vector, and there are 8 x 64 x 64 = 32,768 tokens). With quantization, the compressed representation is simply 8 x 64 x 64 = 32,768 integers (token IDs). The range of the token IDs is from 0 to 5,023.\n\n\n\n\n\nWith this compact tokenized representation, we are ready to train a generator."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "href": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Generator in the Latent Space",
    "text": "Generator in the Latent Space\nThere are two approaches to generate video in the latent space: the autoregressive next-token predictor (language model) and the diffusion model.\n\nAutoregressive Next-Token Predictor\nEach 32-frame video clip is represented as a sequence of 32,768 tokens. The video clips are then concatenated to form a long sequence, separated by a special start-of-video token. This long sequence is fed into a language model training recipe.\nI used nanoGPT to train a 60MB model with the GPT-2 architecture. The model is trained to predict the next token ID in the latent space, instead of the next English token. It worked surprisingly well and began to learn the spatial-temporal patterns quickly.\nThe main ingredient for video quality is ensuring a sufficiently large context window. I used 6,000 tokens, which is much larger than the typical GPT-2 setting. However, this is still a small window size for video. Each video frame is 4,096 tokens, so this context window allows the model to look back only slightly more than one frame, making temporal consistency challenging to enforce.\nSecondly, the training sample size is crucial. Using 100k clips produces better results than 10k clips, and much better than 1k clips. The question remains whether we should use even more data. I hope not, as if such a simple 2D world requires much more than 100k training examples, it would be concerning for more complex domains.\nThis training run showcases one of the better results using nanoGPT.\nThe generated videos start out as random compositions of visual tokens:\n\n\nVideo\n\n\nAfter 6 hours of training, line strokes started to appear:\n\n\nVideo\n\n\n24 hours in, the digits began to emerge, but temporal consistency was poor:\n\n\nVideo\n\n\nAfter 10 days, consistency and physics were much improved:\n\n\nVideo\n\n\nFor comparison, here’s a training run using a 1,024 token context window.\nWith a smaller context window, the training time is much shorter (1 day to converge), but temporal consistency is poor, and digits would suddenly appear throughout the clip:\n\n\nVideo\n\n\n\n\nDiffusion Model\nFor the diffusion model, I used ST-DIT from OpenSora and Stable Diffusion’s SD VAE.\nIn this approach, the context window encompasses the entire video clip, so I expected more temporal consistency than the autoregressive counterpart. Training sample size still plays a significant role. Using a 24GB A10 GPU, I needed to use a small version of the diffusion transformer model.\nA representative training run can be found here.\nThe generated videos also start out as random compositions of visual tokens (resembling crops of natural images this time):\n\n\nVideo\n\n\nAfter one day of training, localized dream-like flowing patterns emerged, though they didn’t yet resemble digits:\n\n\nVideo\n\n\nOn day 3, the moving patterns began to look like digits, but they were so fluid that they seemed to lack “bones”-like structure:\n\n\nVideo\n\n\nBy day 10, the digits were much more stable and distinct, and the moving patterns were steady and smooth:\n\n\nVideo"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#whats-next",
    "href": "posts/20241002-minisora-part1/index.html#whats-next",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "What’s Next",
    "text": "What’s Next\n250 A10 hours (or approximately 80 A100 hours, costing around $200) proved sufficient to adequately solve the video generation task for the 2D toy world of Flying MNIST Digits.\nContext window size and data sample size are important factors for quality, but also drive up cost. There are numerous new techniques that are worth exploring to improve quality while reducing cost. Here’s a non-exhaustive list:\n\nFlow matching: This technique could enhance the temporal consistency of generated videos.\nBetter quantized VAE for auto-regressive video generation: Improving the VAE could lead to more efficient and higher-quality latent representations.\nToken masking: This could reduce the \\(N\\) in the \\(O(N^2)\\) complexity of attention layers, potentially speeding up training and inference.\nCoarse-to-fine generation: Generating whole video frames at the coarse level first, then progressively refining to small details. This can dramatically reduce the context window size and compute cost.\nBetter positional encoding for long context windows in the temporal-spatial setting.\nHyper-optimized LLM training with long context (e.g., llm.c).\nCombining strengths of autoregressive and diffusion models could yield interesting results.\nCurriculum learning: Starting with simpler tasks and progressively increasing difficulty could improve learning efficiency.\n\nThese avenues for improvement suggest that there’s still significant potential to enhance the quality and efficiency of video generation models, even in this simplified domain. As we continue to refine these techniques, we’ll be better positioned to tackle more complex video generation tasks in the future.\nMore to come."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html",
    "href": "posts/20230802-cloudrun-githubaction/index.html",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "",
    "text": "Deploying ML models and other python apps to cloud can be tedious. Compute instances need to be provisioned; networking needs to be sorted out; autoscaling needs to be configured; secrets and credentials need to be safely managed.\nRather than spending hours on the above Dev-Ops tasks (don’t get me wrong, Dev-Ops and ML-Ops are important), I would like to focus on modeling: recipes that produce the best models and make them available for people to use. After years and many projects, I found Google Cloud Run to be a low maintainence solution, with CI/CD managed by Github Action. Similar solutions can be had with AWS ECS and Azure Container Instances. But this post will focus on Cloud Run."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "href": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along with the tutorial, you need:\n\nDocker\nGoogle Cloud SDK"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "href": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Sample App",
    "text": "Sample App\nLet’s start from a very simple http server and run it locally.\ndocker run --rm -it -p 801:801 python:3.8-slim python -m http.server 801 -d /home/\nRun it locally and we can verify it works by visiting localhost:801 in a browser.\n\nDeploy to Cloud Run manually\nHowever, the above docker image does not quite work for Cloud Run, as Cloud Run requires your app in the docker image to use the PORT environment variable to determine which port the app listens to.\nTo solve this we need to build a simple docker image with the following Dockerfile:\nFROM python:3.8-slim\nENV PORT=8080\nCMD python -m http.server $PORT -d /home\nInstall gcloud and authenticate. Then build and deploy it with the following script (click to expand):\n\n\n\n\n\n\nShell script for deploy to google cloud run\n\n\n\n\n\n# Make sure to fill in the GCP project id:\nproject=your-gcp-project-id\napp=example-app\nplatform=linux/amd64\nregion=us-central1\ndocker build --platform $platform -t example-app-image .\n\nimage=us.gcr.io/$project/$app:latest\ndocker tag example-app-image $image\ndocker push $image\ngcloud run deploy $app --image $image --cpu 1 --memory 1Gi --min-instances 1 --region $region --allow-unauthenticated\n\n\n\nNote that there are a couple of hard-coded defaults like the region (us-central1), and image subdomain (us.gcr.io). Feel free to adjust.\nIf successful, we will see something like this:\n\n\n\n\n\n\nConsole output during deployment\n\n\n\n\n\nDeploying container to Cloud Run service [example-app] in project [your-project-id] region [us-central1]\n✓ Deploying new service... Done.                                                 \n  ✓ Creating Revision...                                                         \n  ✓ Routing traffic...                                                           \n  ✓ Setting IAM Policy...                                                        \nDone.                                                                            \nService [example-app] revision [example-app-...] has been deployed and is serving 100 percent of traffic."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "href": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Manage secrets",
    "text": "Manage secrets\nIf the app needs to access secrets such as API keys and passwords, then it is a necessary to store and manage them securely.\nCreate a secret in GCP’s secret manager, and grant minimal necessary access.\nEach secret is versioned. For example, we may create a secret: MY_API_KEY:latest with latest being the version tag.\nWhen using gcloud run deploy to deploy the app, pass in additional arguments:\n--update-secrets=MY_API_KEY=MY_API_KEY:latest,OTHER_API_KEY=OTHER_API_KEY:latest\nIn the docker container, the secret value will be made available in the environment variable MY_API_KEY."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "href": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Set up a secure Github action for continuous deployment",
    "text": "Set up a secure Github action for continuous deployment\nWhile manually running the gcloud command is sufficient to deploy the app to Cloud Run, sometimes it can make sense to set up continuous deployment triggered by github push or release events.\n\nService account\nFirst, we need to follow these instructions to create a service account and grant some permissions:\nGo to IAM, click “grant access” and set: - principal: the new service account just created - role cloud run admin - role: roles/artifactregistry.createOnPushWriter - role: Secret manager secret accessor\nGrant the default compute-engine account access to Secret Manager Secret Accessor role. Go to IAM and set: - principal: the default compute-engine service account - role: Secret Manager Secret Accessor\nGo to IAM/service accounts, click into the default compute-engine service account, then allow the new service account to use this compute engine service account: - principal: the new service account just created - role: “Service account user”\n\n\n\n\n\n\nTip\n\n\n\nI spent hours debugging permission errors in the github actions and found the above steps helped resolving the errors. More info here and here. However, I suspect some of them are not necessary. Please let me know (zhangzhang.si AT gmail.com) if you have a different experience.\n\n\n\n\nDocker artifacts repository\nA docker artifacts repository must be created in the same project as the Cloud Run service (we assume the location is “us-central1”):\ngcloud artifacts repositories create slack-llm --location=us-central1 --repository-format=docker\nThis artifacts repository will hold the docker image of the app.\n\n\nWorkload identify federation and keyless authentication\nFor better cloud security, Google recommends setting up keyless authentication from github actions. To do that, we need to:\n\n\n\n\n\n\nCreate a Workload Identify Pool\n\n\n\n\n\ngcloud iam workload-identity-pools create \"my-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"Demo pool\" \\\n  --description=\"My Identify Pool\"\n\n\n\n\n\n\n\n\n\nThen create a Workload Identify Provider:\n\n\n\n\n\ngcloud iam workload-identity-pools providers create-oidc \"my-provider\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"my-pool\" \\\n  --display-name=\"Demo provider\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.aud=assertion.aud\" \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\"\n\n\n\n\n\n\n\n\n\nThen allow authentications from the Workload Identity Provider to impersonate the desired Service Account:\n\n\n\n\n\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/attribute.repository/my-org/my-repo\"\nAlternatively, if we do not want to restrict the binding to the specific github repo, then:\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/*\"\n\n\n\n\n\nGithub secrets\nAdd the following github secrets (see instructions on how to add secrets to a github repo):\nWIF_PROVIDER=projects/my-gcp-project-number/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n\nWIF_SERVICE_ACCOUNT=my-service-account@my-project.iam.gserviceaccount.com\n\n\nGithub action yaml file\nNow we should be ready to set up the actual github action. This is a redacted version of my working github action yaml file:\n\n\n\n\n\n\nYAML File\n\n\n\n\n\nYAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}\n\n\n\nPut this in .github/workflows/deploy.yml and the next time you push a change to main, it should automatically deploy to Cloud Run.\nEnjoy!"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "href": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "YAML for Github Action",
    "text": "YAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "Notes on practical AI and engineering."
  },
  {
    "objectID": "about.html#zz-si",
    "href": "about.html#zz-si",
    "title": "About this blog",
    "section": "ZZ Si",
    "text": "ZZ Si\n\nCo-founder and Engineer @KUNGFU.AI\nExpertise: Computer vision, Generative models, Practical AI deployment\nPreviously: Apple, Google, Expedia, Impossible Ventures (acquired by Capital One), Vicarious (acquired by Google Deepmind)\nPh.D. Stats @UCLA’11, B.S. CS @Tsinghua’06"
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html",
    "href": "posts/20260222-dl-optimizers-history/notes.html",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "This reference tracks how optimization methods evolved into the standard recipes used in modern deep learning.\n\n\nAt each training step, we turn a noisy gradient estimate g_t into an update delta_theta_t.\n\nDirection control: where to move in parameter space.\nStep-size control: how far to move.\nStability control: avoid exploding or vanishing updates under noise and curvature.\nResource control: memory footprint, compute overhead, and communication cost.\n\nMost innovations are one of:\n\nAcceleration (momentum-like methods).\nPreconditioning (adaptive scaling and curvature approximation).\nRegularization-aware updates (correct weight decay handling).\nLarge-scale stabilization (large-batch and distributed training).\nGeneralization-aware updates (flatness or sharpness-aware methods).\nSystems-aware efficiency (low precision, sharding, paging).\nGeometry-aware steps (natural gradient, orthogonalization).\n\n\n\n\n\n\nCitation: Polyak, 1964\nProblem: vanilla gradient descent zig-zags in ill-conditioned valleys.\nIdea: keep velocity over time.\n\nv_{t+1} = beta v_t + g_t\ntheta_{t+1} = theta_t - eta v_{t+1}\n\nImpact: momentum became a default acceleration tool and still underpins modern recipes.\n\n\n\nCitation: Nesterov, 1983\nProblem: momentum may overshoot.\nIdea: compute gradients at a look-ahead position.\nImpact: Nesterov momentum appears in many high-performing SGD recipes.\n\n\n\nCitation: Amari, 1998\nProblem: Euclidean descent is parameterization-dependent.\nIdea: use Fisher geometry for invariant steepest descent.\nImpact: too expensive directly, but inspired scalable approximations like K-FAC and Shampoo.\n\n\n\n\n\n\nCitation: Sutskever et al., 2013\nWhy it worked:\n\nCheap updates.\nStrong synergy with augmentation and inductive bias.\nSimple scaling.\n\nLimitation: sensitivity to learning rate and curvature.\n\n\n\nCitation: Duchi et al., 2011\nProblem: sparse coordinates need larger effective steps.\nIdea: per-parameter scaling via accumulated squared gradients.\nStrength: sparse, convex-ish settings.\nWeakness for deep nets: steps can shrink too much over long training.\n\n\n\nCitation: Hinton lecture notes, 2012\nProblem: AdaGrad denominator grows monotonically.\nIdea: exponential moving average of squared gradients.\nImpact: practical default and precursor to Adam.\n\n\n\nCitation: Zeiler, 2012\nProblem: reduce sensitivity to explicit learning-rate tuning.\nIdea: normalize updates by running RMS of gradients and updates.\nImpact: less common now, but important in adaptive optimizer evolution.\n\n\n\n\n\n\nCitation: Kingma and Ba, 2014\nProblem: robust defaults under noisy gradients and changing curvature.\nIdea: combine first-moment momentum (m_t) and second-moment scaling (v_t) with bias correction.\nWhy it spread: much lower tuning friction.\nTradeoff: convergence and generalization concerns surfaced.\n\n\n\nCitation: Reddi et al., 2018\nProblem: pathological convergence behavior in some Adam settings.\nIdea: enforce non-increasing effective learning rates via max-tracked second moments.\nImpact: key theoretical clarification; in practice AdamW became the larger practical shift.\n\n\n\n\n\n\nIn adaptive optimizers, adding lambda * theta to gradients is not equivalent to true weight decay.\n\n\n\nCitation: Loshchilov and Hutter, 2017\nFix: decouple weight decay from adaptive gradient normalization.\nImpact: standard default for Transformers and many modern architectures.\nKey point: a theory-informed implementation correction with major practical impact.\n\n\n\n\n\n\nCitation: You et al., 2017\nProblem: global LR scaling breaks at extreme batch sizes.\nIdea: layer-wise trust ratio (||w|| / ||g||) to stabilize relative updates.\nUse case: very large-batch CNN training.\n\n\n\nCitation: You et al., 2019\nProblem: bring layer-wise scaling to Adam-like moments.\nUse case: large-batch BERT-style training.\nBy 2025: still useful in niche throughput-first regimes; less universal than AdamW.\n\n\n\n\n\n\nCitation: Shazeer and Stern, 2018\nProblem: Adam second moments are expensive at scale.\nIdea: factored second-moment statistics for matrix parameters.\nImpact: enables larger models under memory constraints (notably T5-style settings).\n\n\n\nCitation: Dettmers et al., 2021\nProblem: optimizer states dominate memory.\nIdea: quantized moments with near-Adam behavior.\nImpact: common in finetuning and increasingly in larger-scale training pipelines.\n\n\n\nCitation: Tang et al., 2021\nProblem: distributed bandwidth bottlenecks.\nIdea: compress communicated optimizer information aggressively.\nImpact: useful in specific large distributed setups.\n\n\n\n\n\n\nCitation: Martens and Grosse, 2015\nProblem: second-order methods are expensive due to Hessian size.\nIdea: Kronecker-factored curvature approximations.\nImpact: strong in some regimes, but overhead limits broad default adoption.\n\n\n\nCitation: Gupta et al., 2018\nProblem: practical blockwise preconditioning for large models.\nIdea: matrix preconditioners per block with scalable approximations.\nImpact: competitive in selected large-training contexts; more complex than AdamW.\n\n\n\n\n\n\nCitation: Foret et al., 2020\nProblem: sharp minima may generalize worse.\nIdea: optimize worst-case local loss in a neighborhood.\nImpact: strong results in vision and some other domains; extra compute overhead.\n\n\n\nRAdam, Lookahead, AdaBelief, AdaBound, etc.\nCitations: RAdam, Lookahead, AdaBelief, AdaBound\nPattern: improve warm-up behavior, stability, or coupling.\nImpact: useful in pockets, but rarely displacing AdamW/SGD at scale.\n\n\n\nCitation: Chen et al., 2023\nProblem: human-designed families may be too constrained.\nIdea: discover compact update rules via symbolic/program search.\nImpact: validates optimizer design as a search problem.\n\n\n\n\n\n\nCitations: Muon implementation, modular-duality framing\nProblem framing: gradients may interfere across dominant directions in large models.\nIdea: structured orthogonalization or preconditioning to reduce interference.\nWhy notable: strong geometric motivation with relatively simple implementation.\nAdoption status (as of 2025): promising and increasingly tested, but AdamW remained the mainstream documented baseline.\n\n\n\n\n\n\nTypical default: AdamW + warmup + decay schedule + clipping + careful weight decay exclusions.\nWhy: robust, mature, scalable implementation and systems compatibility.\n\n\n\n\nCNNs: SGD + momentum stays strong.\nViTs: AdamW is standard.\n\n\n\n\nTypical default: Adam or AdamW (EMA often used).\n\n\n\nUseful for specific extreme large-batch settings, but not dominant defaults in most frontier pipelines.\n\n\n\n\n\n\n\nNesterov acceleration.\nNatural gradient -&gt; K-FAC/Shampoo.\nAdamW correctness.\nSAM objective reformulation.\nMuon geometric framing.\n\n\n\n\n\nRMSProp.\nAdam practice-first adoption.\nMany tweak-based optimizers.\nLion search-based discovery.\n\n\n\n\n\nLARS/LAMB for throughput.\nAdafactor for memory.\n8-bit, paged, sharded states.\n1-bit communication-aware updates.\n\n\n\n\n\n\nDeeper geometry and subspace control.\nBudgeted or selective curvature.\nSearch or auto-discovered update rules.\nSystems-first state/communication-efficient designs.\nObjective-aware optimization for robustness and calibration.\n\n\n\n\n\nSGD + Momentum/Nesterov: acceleration with simplicity; strong in vision.\nAdaptive (AdaGrad/RMSProp/Adam/AdamW): variance-based preconditioning; robust defaults.\nLayer-wise (LARS/LAMB): large-batch stabilization.\nMemory-efficient (Adafactor, low-precision states): Transformer scaling.\nCurvature-based (K-FAC/Shampoo): approximate natural gradient.\nGeneralization-aware (SAM): flatter minima preference.\nGeometry/interference-aware (Muon): structured directional control.\nSearch-designed (Lion): machine-discovered update rules.\n\n\n\n\n\nLLM/VLM pretraining: AdamW + warmup + decay + clipping.\nViT training: AdamW with tuned weight decay and augmentation.\nCNN training: SGD + momentum + strong schedule.\nDiffusion/flow matching: Adam/AdamW with EMA often used.\nTry Muon when stability and scaling friction dominate.\nTry LARS/LAMB in extreme throughput-driven large-batch training.\n\n\n\n\nThis section tracks the newest conditioning/preconditioning-heavy optimizer work after the 2025 baseline snapshot.\n\n\n\nMuon is Scalable for LLM Training (2025-02-24): practical scaling evidence for Muon-family conditioning.\n\nLink: https://arxiv.org/abs/2502.16982\nEvidence tags: 1B+, open-source impl\n\nPolarGrad: matrix-gradient optimizers from a unifying preconditioning perspective (2025-05-27, revised 2026-02-05).\n\nLink: https://arxiv.org/abs/2505.21799\nEvidence tags: theory, small-scale\n\nNorMuon: neuron-wise normalized Muon, improving conditioning balance (2025-10-07).\n\nLink: https://arxiv.org/abs/2510.05491\nEvidence tags: 1B+, open-source impl\n\nMARS-M: variance reduction plus matrix conditioning (2025-10-20).\n\nLink: https://arxiv.org/abs/2510.21800\nEvidence tags: theory, small-scale, open-source impl\n\nHyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales (2025-12-05).\n\nLink: https://arxiv.org/abs/2512.05620\nEvidence tags: 1B+, scaling-law, protocol\n\nMuon is Provably Faster with Momentum Variance Reduction (2025-12-18).\n\nLink: https://arxiv.org/abs/2512.16598\nEvidence tags: theory\n\nTEON: Tensorized Orthonormalization Beyond Layer-Wise Muon (2026-01-30).\n\nLink: https://arxiv.org/abs/2601.23261\nEvidence tags: theory, 1B-range\n\nPRISM: adaptive computation of matrix functions for optimizer primitives (2026-01-29).\n\nLink: https://arxiv.org/abs/2601.22137\nEvidence tags: systems, optimizer-primitive\n\nMSign: stable-rank restoration for training stability (2026-02-02).\n\nLink: https://arxiv.org/abs/2602.01734\nEvidence tags: stability, up-to-3B\n\nARO: Adaptively Rotated Optimization (2026-02-09).\n\nLink: https://arxiv.org/abs/2602.09006\nEvidence tags: 1B+, protocol, conditioning\n\n\n\n\n\n\nLayer-wise orthogonalization: Muon, NorMuon.\nTensorized or cross-layer orthogonalization: TEON.\nRotated-coordinate conditioning: ARO.\nMatrix preconditioners with scaling transfer: Shampoo/SOAP/Muon transfer rules (2512.05620).\nConditioning + variance reduction hybrids: MARS-M, MVR-style Muon variants.\nConditioning for stability restoration: MSign.\n\n\n\n\nWhen citing any new optimizer, label each claim with:\n\ntheory: convergence or complexity result.\nsmall-scale: validated below ~1B parameters.\n1B+: demonstrated at &gt;= 1B parameters.\nopen-source impl: usable code exists.\nindependent replication: separate team replication exists.\nprotocol: paper provides controlled optimizer-comparison protocol.\n\n\n\n\nFor any benchmark table in the blog draft, require:\n\nSame model architecture and tokenizer.\nSame token budget and data ordering.\nSame batch-size regime and precision setup.\nMatched hyperparameter tuning budget per optimizer.\nReport both wall-clock and token/FLOP efficiency to target loss.\nReport instability/failure rate across seeds.\n\nRationale: recent 2025-2026 work shows many optimizer gains vanish under poor scaling transfer or unfair tuning budgets.\n\n\n\n\nMuon vs MuonClip vs Muon+AdamW (HF community, 2025-12-09): https://huggingface.co/blog/KingNish/optimizer-part1\nReproducing and Validating Distributed Muon (HF community, 2025-12-12): https://huggingface.co/blog/bird-of-paradise/reproducing-and-validating-distributed-muon\nScaling Is Not Plug-and-Play (HF community, 2026-01-04): https://huggingface.co/blog/bird-of-paradise/scaling-is-not-plug-and-play"
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#what-an-optimizer-is-doing",
    "href": "posts/20260222-dl-optimizers-history/notes.html#what-an-optimizer-is-doing",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "At each training step, we turn a noisy gradient estimate g_t into an update delta_theta_t.\n\nDirection control: where to move in parameter space.\nStep-size control: how far to move.\nStability control: avoid exploding or vanishing updates under noise and curvature.\nResource control: memory footprint, compute overhead, and communication cost.\n\nMost innovations are one of:\n\nAcceleration (momentum-like methods).\nPreconditioning (adaptive scaling and curvature approximation).\nRegularization-aware updates (correct weight decay handling).\nLarge-scale stabilization (large-batch and distributed training).\nGeneralization-aware updates (flatness or sharpness-aware methods).\nSystems-aware efficiency (low precision, sharding, paging).\nGeometry-aware steps (natural gradient, orthogonalization)."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#foundations-before-modern-deep-learning-1960s-2000s",
    "href": "posts/20260222-dl-optimizers-history/notes.html#foundations-before-modern-deep-learning-1960s-2000s",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Citation: Polyak, 1964\nProblem: vanilla gradient descent zig-zags in ill-conditioned valleys.\nIdea: keep velocity over time.\n\nv_{t+1} = beta v_t + g_t\ntheta_{t+1} = theta_t - eta v_{t+1}\n\nImpact: momentum became a default acceleration tool and still underpins modern recipes.\n\n\n\nCitation: Nesterov, 1983\nProblem: momentum may overshoot.\nIdea: compute gradients at a look-ahead position.\nImpact: Nesterov momentum appears in many high-performing SGD recipes.\n\n\n\nCitation: Amari, 1998\nProblem: Euclidean descent is parameterization-dependent.\nIdea: use Fisher geometry for invariant steepest descent.\nImpact: too expensive directly, but inspired scalable approximations like K-FAC and Shampoo."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#early-deep-learning-resurgence-2010-2014",
    "href": "posts/20260222-dl-optimizers-history/notes.html#early-deep-learning-resurgence-2010-2014",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Citation: Sutskever et al., 2013\nWhy it worked:\n\nCheap updates.\nStrong synergy with augmentation and inductive bias.\nSimple scaling.\n\nLimitation: sensitivity to learning rate and curvature.\n\n\n\nCitation: Duchi et al., 2011\nProblem: sparse coordinates need larger effective steps.\nIdea: per-parameter scaling via accumulated squared gradients.\nStrength: sparse, convex-ish settings.\nWeakness for deep nets: steps can shrink too much over long training.\n\n\n\nCitation: Hinton lecture notes, 2012\nProblem: AdaGrad denominator grows monotonically.\nIdea: exponential moving average of squared gradients.\nImpact: practical default and precursor to Adam.\n\n\n\nCitation: Zeiler, 2012\nProblem: reduce sensitivity to explicit learning-rate tuning.\nIdea: normalize updates by running RMS of gradients and updates.\nImpact: less common now, but important in adaptive optimizer evolution."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#the-adam-era-2014-2017",
    "href": "posts/20260222-dl-optimizers-history/notes.html#the-adam-era-2014-2017",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Citation: Kingma and Ba, 2014\nProblem: robust defaults under noisy gradients and changing curvature.\nIdea: combine first-moment momentum (m_t) and second-moment scaling (v_t) with bias correction.\nWhy it spread: much lower tuning friction.\nTradeoff: convergence and generalization concerns surfaced.\n\n\n\nCitation: Reddi et al., 2018\nProblem: pathological convergence behavior in some Adam settings.\nIdea: enforce non-increasing effective learning rates via max-tracked second moments.\nImpact: key theoretical clarification; in practice AdamW became the larger practical shift."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#adamw-and-decoupled-weight-decay-2017-2019",
    "href": "posts/20260222-dl-optimizers-history/notes.html#adamw-and-decoupled-weight-decay-2017-2019",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "In adaptive optimizers, adding lambda * theta to gradients is not equivalent to true weight decay.\n\n\n\nCitation: Loshchilov and Hutter, 2017\nFix: decouple weight decay from adaptive gradient normalization.\nImpact: standard default for Transformers and many modern architectures.\nKey point: a theory-informed implementation correction with major practical impact."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#large-batch-optimizers-2017-2020",
    "href": "posts/20260222-dl-optimizers-history/notes.html#large-batch-optimizers-2017-2020",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Citation: You et al., 2017\nProblem: global LR scaling breaks at extreme batch sizes.\nIdea: layer-wise trust ratio (||w|| / ||g||) to stabilize relative updates.\nUse case: very large-batch CNN training.\n\n\n\nCitation: You et al., 2019\nProblem: bring layer-wise scaling to Adam-like moments.\nUse case: large-batch BERT-style training.\nBy 2025: still useful in niche throughput-first regimes; less universal than AdamW."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#memory-and-systems-aware-optimizers-2018-2023",
    "href": "posts/20260222-dl-optimizers-history/notes.html#memory-and-systems-aware-optimizers-2018-2023",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Citation: Shazeer and Stern, 2018\nProblem: Adam second moments are expensive at scale.\nIdea: factored second-moment statistics for matrix parameters.\nImpact: enables larger models under memory constraints (notably T5-style settings).\n\n\n\nCitation: Dettmers et al., 2021\nProblem: optimizer states dominate memory.\nIdea: quantized moments with near-Adam behavior.\nImpact: common in finetuning and increasingly in larger-scale training pipelines.\n\n\n\nCitation: Tang et al., 2021\nProblem: distributed bandwidth bottlenecks.\nIdea: compress communicated optimizer information aggressively.\nImpact: useful in specific large distributed setups."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#curvature-approximations-at-scale-2015-2023",
    "href": "posts/20260222-dl-optimizers-history/notes.html#curvature-approximations-at-scale-2015-2023",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Citation: Martens and Grosse, 2015\nProblem: second-order methods are expensive due to Hessian size.\nIdea: Kronecker-factored curvature approximations.\nImpact: strong in some regimes, but overhead limits broad default adoption.\n\n\n\nCitation: Gupta et al., 2018\nProblem: practical blockwise preconditioning for large models.\nIdea: matrix preconditioners per block with scalable approximations.\nImpact: competitive in selected large-training contexts; more complex than AdamW."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#generalization-aware-wave-2020-2023",
    "href": "posts/20260222-dl-optimizers-history/notes.html#generalization-aware-wave-2020-2023",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Citation: Foret et al., 2020\nProblem: sharp minima may generalize worse.\nIdea: optimize worst-case local loss in a neighborhood.\nImpact: strong results in vision and some other domains; extra compute overhead.\n\n\n\nRAdam, Lookahead, AdaBelief, AdaBound, etc.\nCitations: RAdam, Lookahead, AdaBelief, AdaBound\nPattern: improve warm-up behavior, stability, or coupling.\nImpact: useful in pockets, but rarely displacing AdamW/SGD at scale.\n\n\n\nCitation: Chen et al., 2023\nProblem: human-designed families may be too constrained.\nIdea: discover compact update rules via symbolic/program search.\nImpact: validates optimizer design as a search problem."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#geometryinterference-aware-updates-2024-2025",
    "href": "posts/20260222-dl-optimizers-history/notes.html#geometryinterference-aware-updates-2024-2025",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Citations: Muon implementation, modular-duality framing\nProblem framing: gradients may interfere across dominant directions in large models.\nIdea: structured orthogonalization or preconditioning to reduce interference.\nWhy notable: strong geometric motivation with relatively simple implementation.\nAdoption status (as of 2025): promising and increasingly tested, but AdamW remained the mainstream documented baseline."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#what-won-by-2025",
    "href": "posts/20260222-dl-optimizers-history/notes.html#what-won-by-2025",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Typical default: AdamW + warmup + decay schedule + clipping + careful weight decay exclusions.\nWhy: robust, mature, scalable implementation and systems compatibility.\n\n\n\n\nCNNs: SGD + momentum stays strong.\nViTs: AdamW is standard.\n\n\n\n\nTypical default: Adam or AdamW (EMA often used).\n\n\n\nUseful for specific extreme large-batch settings, but not dominant defaults in most frontier pipelines."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#innovation-drivers-theory-empiricism-systems",
    "href": "posts/20260222-dl-optimizers-history/notes.html#innovation-drivers-theory-empiricism-systems",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Nesterov acceleration.\nNatural gradient -&gt; K-FAC/Shampoo.\nAdamW correctness.\nSAM objective reformulation.\nMuon geometric framing.\n\n\n\n\n\nRMSProp.\nAdam practice-first adoption.\nMany tweak-based optimizers.\nLion search-based discovery.\n\n\n\n\n\nLARS/LAMB for throughput.\nAdafactor for memory.\n8-bit, paged, sharded states.\n1-bit communication-aware updates."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#likely-next-directions",
    "href": "posts/20260222-dl-optimizers-history/notes.html#likely-next-directions",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "Deeper geometry and subspace control.\nBudgeted or selective curvature.\nSearch or auto-discovered update rules.\nSystems-first state/communication-efficient designs.\nObjective-aware optimization for robustness and calibration."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#mental-map-of-optimizer-families",
    "href": "posts/20260222-dl-optimizers-history/notes.html#mental-map-of-optimizer-families",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "SGD + Momentum/Nesterov: acceleration with simplicity; strong in vision.\nAdaptive (AdaGrad/RMSProp/Adam/AdamW): variance-based preconditioning; robust defaults.\nLayer-wise (LARS/LAMB): large-batch stabilization.\nMemory-efficient (Adafactor, low-precision states): Transformer scaling.\nCurvature-based (K-FAC/Shampoo): approximate natural gradient.\nGeneralization-aware (SAM): flatter minima preference.\nGeometry/interference-aware (Muon): structured directional control.\nSearch-designed (Lion): machine-discovered update rules."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#practical-defaults-2025-ish",
    "href": "posts/20260222-dl-optimizers-history/notes.html#practical-defaults-2025-ish",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "LLM/VLM pretraining: AdamW + warmup + decay + clipping.\nViT training: AdamW with tuned weight decay and augmentation.\nCNN training: SGD + momentum + strong schedule.\nDiffusion/flow matching: Adam/AdamW with EMA often used.\nTry Muon when stability and scaling friction dominate.\nTry LARS/LAMB in extreme throughput-driven large-batch training."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/notes.html#late-2025-to-early-2026-conditioning-based-wave",
    "href": "posts/20260222-dl-optimizers-history/notes.html#late-2025-to-early-2026-conditioning-based-wave",
    "title": "History of Deep Learning Optimizers (to 2025)",
    "section": "",
    "text": "This section tracks the newest conditioning/preconditioning-heavy optimizer work after the 2025 baseline snapshot.\n\n\n\nMuon is Scalable for LLM Training (2025-02-24): practical scaling evidence for Muon-family conditioning.\n\nLink: https://arxiv.org/abs/2502.16982\nEvidence tags: 1B+, open-source impl\n\nPolarGrad: matrix-gradient optimizers from a unifying preconditioning perspective (2025-05-27, revised 2026-02-05).\n\nLink: https://arxiv.org/abs/2505.21799\nEvidence tags: theory, small-scale\n\nNorMuon: neuron-wise normalized Muon, improving conditioning balance (2025-10-07).\n\nLink: https://arxiv.org/abs/2510.05491\nEvidence tags: 1B+, open-source impl\n\nMARS-M: variance reduction plus matrix conditioning (2025-10-20).\n\nLink: https://arxiv.org/abs/2510.21800\nEvidence tags: theory, small-scale, open-source impl\n\nHyperparameter Transfer Enables Consistent Gains of Matrix-Preconditioned Optimizers Across Scales (2025-12-05).\n\nLink: https://arxiv.org/abs/2512.05620\nEvidence tags: 1B+, scaling-law, protocol\n\nMuon is Provably Faster with Momentum Variance Reduction (2025-12-18).\n\nLink: https://arxiv.org/abs/2512.16598\nEvidence tags: theory\n\nTEON: Tensorized Orthonormalization Beyond Layer-Wise Muon (2026-01-30).\n\nLink: https://arxiv.org/abs/2601.23261\nEvidence tags: theory, 1B-range\n\nPRISM: adaptive computation of matrix functions for optimizer primitives (2026-01-29).\n\nLink: https://arxiv.org/abs/2601.22137\nEvidence tags: systems, optimizer-primitive\n\nMSign: stable-rank restoration for training stability (2026-02-02).\n\nLink: https://arxiv.org/abs/2602.01734\nEvidence tags: stability, up-to-3B\n\nARO: Adaptively Rotated Optimization (2026-02-09).\n\nLink: https://arxiv.org/abs/2602.09006\nEvidence tags: 1B+, protocol, conditioning\n\n\n\n\n\n\nLayer-wise orthogonalization: Muon, NorMuon.\nTensorized or cross-layer orthogonalization: TEON.\nRotated-coordinate conditioning: ARO.\nMatrix preconditioners with scaling transfer: Shampoo/SOAP/Muon transfer rules (2512.05620).\nConditioning + variance reduction hybrids: MARS-M, MVR-style Muon variants.\nConditioning for stability restoration: MSign.\n\n\n\n\nWhen citing any new optimizer, label each claim with:\n\ntheory: convergence or complexity result.\nsmall-scale: validated below ~1B parameters.\n1B+: demonstrated at &gt;= 1B parameters.\nopen-source impl: usable code exists.\nindependent replication: separate team replication exists.\nprotocol: paper provides controlled optimizer-comparison protocol.\n\n\n\n\nFor any benchmark table in the blog draft, require:\n\nSame model architecture and tokenizer.\nSame token budget and data ordering.\nSame batch-size regime and precision setup.\nMatched hyperparameter tuning budget per optimizer.\nReport both wall-clock and token/FLOP efficiency to target loss.\nReport instability/failure rate across seeds.\n\nRationale: recent 2025-2026 work shows many optimizer gains vanish under poor scaling transfer or unfair tuning budgets.\n\n\n\n\nMuon vs MuonClip vs Muon+AdamW (HF community, 2025-12-09): https://huggingface.co/blog/KingNish/optimizer-part1\nReproducing and Validating Distributed Muon (HF community, 2025-12-12): https://huggingface.co/blog/bird-of-paradise/reproducing-and-validating-distributed-muon\nScaling Is Not Plug-and-Play (HF community, 2026-01-04): https://huggingface.co/blog/bird-of-paradise/scaling-is-not-plug-and-play"
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "This file tracks practical benchmark choices for comparing deep learning optimizers in a way that is both credible and runnable.\n\n\n\nKeep comparisons fair: same model, data order, token/image budget, precision, and tuning budget.\nReport both quality and efficiency: final loss/accuracy and time/compute to target.\nUse multiple scales: a method that wins at small scale may fail at larger scale.\nSeparate “quick sanity checks” from “publication-grade evidence”.\n\n\n\n\n\n\n\nGoal: catch obvious instability or poor defaults quickly.\n\nCIFAR-10 (optionally CIFAR-100) with small ResNet or ViT-tiny\n\nWhy: fast, cheap, easy to iterate.\nMetrics: train loss curve, test accuracy, wall-clock.\n\nnanochat/nanochat (small language-model benchmark)\n\nWhy: reveals optimizer behavior on autoregressive objectives with low runtime.\nMetrics: validation loss/perplexity vs steps and tokens.\n\n\nUse case: smoke-test AdamW vs SGD+momentum vs one conditioning method.\n\n\n\nGoal: choose optimizer for a real project setting.\n\nImageNet-1k with ResNet-50 or ViT-B/16\n\nWhy: standard vision comparison regime used by many optimizer papers.\nMetrics: top-1, throughput, time-to-target accuracy.\n\nGPT-style pretraining at ~100M-1B parameters on a fixed token budget\n\nWhy: directly relevant to modern optimizer claims.\nMetrics: validation loss/perplexity vs tokens and wall-clock.\n\nImage generation on CelebA-HQ (or FFHQ) with a compact diffusion setup\n\nWhy: tests optimizer stability and quality under generative objectives.\nMetrics: FID, training stability, time-to-target FID, and memory usage.\n\n\nUse case: evaluate production candidate optimizers under matched tuning budgets.\n\n\n\nGoal: validate scaling claims and conditioning behavior at large model size.\n\nGPT/LLaMA-style pretraining at multi-billion parameter scale\n\nWhy: this is where many new conditioning methods claim gains.\nMetrics: loss-to-token, loss-to-FLOP, stability/failure rate, hardware efficiency.\n\nOptional MoE runs if your stack is MoE-first\n\nWhy: optimizer behavior can change with sparse activation and routing noise.\n\n\nUse case: only for teams with enough compute and strict experiment control.\n\n\n\n\nVision: ImageNet-1k, CIFAR-10/100.\nLanguage: BERT/GPT-style pretraining and perplexity tracking.\nTranslation (historically): WMT14 En-De (used in Adafactor-era work).\nSystems benchmarks: MLPerf Training.\nOptimizer research suite: DeepOBS.\n\n\n\n\n\nFinal quality metric: accuracy or perplexity/loss.\nTime-to-target quality.\nCompute-to-target quality (FLOPs or token budget).\nPeak memory and effective batch size.\nStability metrics: divergence count and variance across &gt;= 3 seeds.\n\n\n\n\n\nSame architecture and tokenizer/data pipeline.\nSame training token/image budget.\nSame data ordering or controlled shuffling.\nSame precision and distributed settings.\nSame hyperparameter tuning budget per optimizer.\nSame stopping rule and evaluation cadence.\n\n\n\n\nTo keep examples useful for readers:\n\nPrioritize this triad for coverage:\n\nCIFAR-10 classification.\nnanochat/nanochat language modeling.\nCelebA-HQ/FFHQ image generation.\n\nProvide 1-2 runnable demos from that triad (Tier 1 or small Tier 2).\nInclude a benchmark matrix table for the rest.\nClearly label evidence level: toy, mid-scale, or frontier.\n\n\n\n\n\nMuon scalable LLM training: https://arxiv.org/abs/2502.16982\nNorMuon: https://arxiv.org/abs/2510.05491\nTEON: https://arxiv.org/abs/2601.23261\nHyperparameter transfer (matrix preconditioners): https://arxiv.org/abs/2512.05620\nARO: https://arxiv.org/abs/2602.09006\nLAMB: https://arxiv.org/abs/1904.00962\nLARS: https://arxiv.org/abs/1708.03888\nSAM: https://arxiv.org/abs/2010.01412\nLion: https://arxiv.org/abs/2302.06675\nAdafactor: https://arxiv.org/abs/1804.04235\nMLPerf Training: https://mlcommons.org/benchmarks/training/\nDeepOBS: https://deepobs.github.io/"
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#principles",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#principles",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "Keep comparisons fair: same model, data order, token/image budget, precision, and tuning budget.\nReport both quality and efficiency: final loss/accuracy and time/compute to target.\nUse multiple scales: a method that wins at small scale may fail at larger scale.\nSeparate “quick sanity checks” from “publication-grade evidence”."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#tier-1-quick-sanity-checks-hours",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#tier-1-quick-sanity-checks-hours",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "Goal: catch obvious instability or poor defaults quickly.\n\nCIFAR-10 (optionally CIFAR-100) with small ResNet or ViT-tiny\n\nWhy: fast, cheap, easy to iterate.\nMetrics: train loss curve, test accuracy, wall-clock.\n\nnanochat/nanochat (small language-model benchmark)\n\nWhy: reveals optimizer behavior on autoregressive objectives with low runtime.\nMetrics: validation loss/perplexity vs steps and tokens.\n\n\nUse case: smoke-test AdamW vs SGD+momentum vs one conditioning method."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#tier-2-mid-scale-decision-benchmarks-1-3-days",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#tier-2-mid-scale-decision-benchmarks-1-3-days",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "Goal: choose optimizer for a real project setting.\n\nImageNet-1k with ResNet-50 or ViT-B/16\n\nWhy: standard vision comparison regime used by many optimizer papers.\nMetrics: top-1, throughput, time-to-target accuracy.\n\nGPT-style pretraining at ~100M-1B parameters on a fixed token budget\n\nWhy: directly relevant to modern optimizer claims.\nMetrics: validation loss/perplexity vs tokens and wall-clock.\n\nImage generation on CelebA-HQ (or FFHQ) with a compact diffusion setup\n\nWhy: tests optimizer stability and quality under generative objectives.\nMetrics: FID, training stability, time-to-target FID, and memory usage.\n\n\nUse case: evaluate production candidate optimizers under matched tuning budgets."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#tier-3-frontier-scale-evidence-large-clusters",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#tier-3-frontier-scale-evidence-large-clusters",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "Goal: validate scaling claims and conditioning behavior at large model size.\n\nGPT/LLaMA-style pretraining at multi-billion parameter scale\n\nWhy: this is where many new conditioning methods claim gains.\nMetrics: loss-to-token, loss-to-FLOP, stability/failure rate, hardware efficiency.\n\nOptional MoE runs if your stack is MoE-first\n\nWhy: optimizer behavior can change with sparse activation and routing noise.\n\n\nUse case: only for teams with enough compute and strict experiment control."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#common-tasks-used-in-optimizer-literature",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#common-tasks-used-in-optimizer-literature",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "Vision: ImageNet-1k, CIFAR-10/100.\nLanguage: BERT/GPT-style pretraining and perplexity tracking.\nTranslation (historically): WMT14 En-De (used in Adafactor-era work).\nSystems benchmarks: MLPerf Training.\nOptimizer research suite: DeepOBS."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#metrics-to-report-minimum",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#metrics-to-report-minimum",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "Final quality metric: accuracy or perplexity/loss.\nTime-to-target quality.\nCompute-to-target quality (FLOPs or token budget).\nPeak memory and effective batch size.\nStability metrics: divergence count and variance across &gt;= 3 seeds."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#fair-comparison-checklist",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#fair-comparison-checklist",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "Same architecture and tokenizer/data pipeline.\nSame training token/image budget.\nSame data ordering or controlled shuffling.\nSame precision and distributed settings.\nSame hyperparameter tuning budget per optimizer.\nSame stopping rule and evaluation cadence."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#practical-recommendation-for-this-blog-post",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#practical-recommendation-for-this-blog-post",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "To keep examples useful for readers:\n\nPrioritize this triad for coverage:\n\nCIFAR-10 classification.\nnanochat/nanochat language modeling.\nCelebA-HQ/FFHQ image generation.\n\nProvide 1-2 runnable demos from that triad (Tier 1 or small Tier 2).\nInclude a benchmark matrix table for the rest.\nClearly label evidence level: toy, mid-scale, or frontier."
  },
  {
    "objectID": "posts/20260222-dl-optimizers-history/benchmarks.html#suggested-references",
    "href": "posts/20260222-dl-optimizers-history/benchmarks.html#suggested-references",
    "title": "Optimizer Benchmarks for This Post",
    "section": "",
    "text": "Muon scalable LLM training: https://arxiv.org/abs/2502.16982\nNorMuon: https://arxiv.org/abs/2510.05491\nTEON: https://arxiv.org/abs/2601.23261\nHyperparameter transfer (matrix preconditioners): https://arxiv.org/abs/2512.05620\nARO: https://arxiv.org/abs/2602.09006\nLAMB: https://arxiv.org/abs/1904.00962\nLARS: https://arxiv.org/abs/1708.03888\nSAM: https://arxiv.org/abs/2010.01412\nLion: https://arxiv.org/abs/2302.06675\nAdafactor: https://arxiv.org/abs/1804.04235\nMLPerf Training: https://mlcommons.org/benchmarks/training/\nDeepOBS: https://deepobs.github.io/"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on practical AI, engineering and life",
    "section": "",
    "text": "A Tour of Deep Learning Optimizers\n\n\n\n\n\n\nai\n\n\ndeep learning\n\n\noptimization\n\n\nllm\n\n\n\n\n\n\n\n\n\nFeb 22, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Build and Deploy MCP for Proprietary Data\n\n\n\n\n\n\nai\n\n\nmcp\n\n\narchitecture\n\n\nsecurity\n\n\ndeployment\n\n\n\n\n\n\n\n\n\nFeb 19, 2026\n\n\n\n\n\n\n\n\n\n\n\n\nCoding Agents and a Spooky Kaggle Challenge: Benchmarking Lightweight ML Automation\n\n\n\n\n\n\nai\n\n\ndata science\n\n\ncoding agents\n\n\nnlp\n\n\nllm\n\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAgentic Causal Inference\n\n\n\n\n\n\nai\n\n\nllm\n\n\nagentic\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMiniSora: Learnings from training a Minimal Video Generation Model (Part 1)\n\n\n\n\n\n\ngenerative ai\n\n\nvideo generation\n\n\ncost efficient training\n\n\nscaling laws\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nComputer vision for soccer games\n\n\n\n\n\n\ncomputer vision\n\n\nai\n\n\nsports\n\n\nsoccer\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth\n\n\n\n\n\n\neconomics\n\n\nai\n\n\nenvironment\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying machine learning apps to Google Cloud Run with Github actions\n\n\n\n\n\n\ncode\n\n\nmlops\n\n\nGCP\n\n\ncloud\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nZZ Si\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html",
    "href": "posts/20240915-soccer-tracking/index.html",
    "title": "Computer vision for soccer games",
    "section": "",
    "text": "I was intrigued to see this example where multiple (at least 5) computer vision techniques to create visual appealing analytics from soccer game footage. Soccer fans and coaches may enjoy this.\nVideo\nThis is an open source demo from Roboflow, and is easy to reproduce. Since it is a proof of concept, more work needs to be done to make it work for other real world videos, where there a large portion of the soccer field is not visible, or when the camera moved fast (which happens quite often). This is a common challenge for practical computer vision: it can be hard to make an impressive model work on your data.\nBelow I share a workflow to reproduce both success and limitations of this soccer tracking example, and some ideas to improve it to make it work on more challenging data. Similar techniques can be applied to other sports, like tennis, (American) football, basketball, pickle ball, etc."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "href": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "title": "Computer vision for soccer games",
    "section": "Reproducing the birds-eye view creation",
    "text": "Reproducing the birds-eye view creation\n\n\n\n\n\n\nPre-requisites\n\n\n\n\n\nPre-requisites\n\nYou need a machine with GPU to run the code. The code is tested on a machine with a GeForce RTX 3090, and it uses about 3GB of GPU memory.\nYou need to have git, docker and python (3.6+) installed.\nNVidia container toolkit is required to use the GPU in the docker container.\n\n\n\n\n\n\n\n\n\n\nDownload\n\n\n\n\n\nStep 1: Download the code and data\ncvlization is an open source repo with many working examples of computer vision workflows. Clone the repo:\ngit clone https://github.com/kungfuai/cvlization.git\ncd cvlization\nIn examples/sports/soccer_game_visual_tracking, there is a README file that explains how to download the model weights and example video data (pip install gdown if you haven’t already).\ncd examples/sports/soccer_game_visual_tracking\nbash download_data.sh\n\n\n\n\n\n\n\n\n\nInstall\n\n\n\n\n\nStep 2: Install the dependencies by building a docker image\nChange directory back to the root of the cvlization repo, and run\nbash examples/sports/soccer_game_visual_tracking/build.sh\nThis will build a docker image with necessary dependencies. If you prefer to not use docker, you can install the dependencies manually by following the instructions in the Dockerfile in the same directory.\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\n\n\nStep 3: Run the code\nbash examples/sports/soccer_game_visual_tracking/predict.sh\nThis will use the docker image to run the code. If you prefer to run the code without docker, you can directly use the command in the predict.sh script in the same directory.\nIn this script, we are using a 30 second clip from a soccer game. The script will track the pitch and players, identify the team, goal keepers, referee, and ball, and generate a bird’s eye view video. Feel free to modify the script to use a different video or to change the tracking parameters.\nYou will find the output video in examples/sports/soccer_game_visual_tracking/0bfacc_0-radar.mp4. This is the video shown on the top of the page. On a machine with a GeForce RTX 3090, it takes about 20 minutes to run, with 3GB of GPU memory used."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "href": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "title": "Computer vision for soccer games",
    "section": "Under the hood",
    "text": "Under the hood\nThe computer vision models and algorithms under the hood include:\n\nA keypoint detection (pose estimation) model for 32 keypoints on the soccer pitch (Yolo-v8, 70M, training notebook, mAP=0.99, 1 hour on NVidia T4, trained on hundreds of images).\n\n\n\n\n\n\n\nAn object detection model for players, referrees and goal keepers (Yolo-v8, 68M, training notebook, mAP=0.79, 40min on NNivida L4).\n\n\n\n\n\n\n\nAnother object detection model for the ball (Yolo-v8, 68M, training notebook, mAP=0.93, 1.3 hours on NVidia A100). The ball is very small in the image, so it is hard to detect.\nA multi-object tracking model to track the players and the ball (Bytetrack, implementation and python API).\nA vision embedding model and clustering algorithm for team identification. SigLIP is used to extract embedding vectors from cropped players. UMAP is used for dimensionality reduction. K-means is used for clustering. Also Resolve the team IDs for detected goalkeepers based on the proximity to team centroids (based on player locations).\nAn image registration/stitching algorithm to create the bird’s eye view. Homography is estimated between the pitch keypoints and the reference coornidates of the pitch, using OpenCV’s findHomography. The pitch in the footage is then warped to a top-down view using perspectiveTransform.\nPlayer re-identification models (e.g. MOTIP). When the footage is cut or camera is changed to a different angle, the player IDs are lost. We need to re-identify the players in order to connect the player tracks across different clips. I did not find the implemetation in this POC."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "href": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "title": "Computer vision for soccer games",
    "section": "Does it work on other soccer videos?",
    "text": "Does it work on other soccer videos?\nI picked a random soccer game clip, and the result is not as good as the example video. The camera moved faster, zooming in to a partial view of the pitch near the goal post. This posed challenges to the keypoint detection model, and the player tracking model. Some players were not detected due to motion blur and occlusion. Key points of the pitch were not detected in some frames, and the algorithm was not able to create a bird’s eye view for those frames. The result is shown below:\nVideo\nRegardless, it is a great starting point to build a more reliable system for soccer game analytics. For fun, I also tried it on a very challenging video with a couple of professional players against 100 pupils. Interestingly, the algorithm was able to detect most the players, and create a bird’s eye view, as long as a large portion of the pitch is visible:"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "href": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "title": "Computer vision for soccer games",
    "section": "Makeing it better: more accurate player detection and tracking",
    "text": "Makeing it better: more accurate player detection and tracking\n\nTransformers for object tracking\nAccurate tracking requires attending to relationships between detected players on different frames, their roles, jersey colors etc. Transformers architecture is well suited for this task.\n\nGlobal tracking transformer\nGlobal tracking transformers takes a video as input, and predict object tracks in an end-to-end fashion. It was trained on LVIS and COCO, capable of tracking 1000+ categories of objects. Below is the result for tracking persons and the ball. It also identified the billboards though they are not directly useful for our purpose here. This is the tracking result overlayed on the input video:\nVideo\nComparing YOLOv8 and Global Tracking Transformer, the latter seems more accurate.\n\n\n\n\n\n\n\n\n\nYOLOv8\n\n\n\n\n\n\n\nGlobal Tracking Transformer\n\n\n\n\n\n\n\n\nVision-language models, open vocabulary and zero-shot object detection\nWith recent advances in vision-language models, we can leverage the visual knowledge in pretrained large models. How well do they work in detecting players?\n\nGrounding DINO\nThis model has a DINO transformer backbone and produced by grounded pre-training. You can prompt the model with a sentence or a phrase, and it will highlight the corresponding region in the image. Below is the architecture of Grounding DINO:\n\n\n\nArchitecture of Grounding DINO\n\n\n\n\n\nWith one prompt, Grounding DINO was able to detect players but had a hard time distinguishing the goal keeper from other players.\n\n\n\n\nYOLO World\nThis model is an open-vocabulary object detection model. It can detect objects that are not in the training set, and can be used for zero-shot object detection. You can prompt it with a list of words, such as “player, ball, goal keeper”.\nCompared to Grounding DINO, YOLO World seems less accurate and misses some players when they overlap.\n\n\n\nYOLO-World-XL player detection result.\n\n\nThese are just two examples of recent models."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#datasets",
    "href": "posts/20240915-soccer-tracking/index.html#datasets",
    "title": "Computer vision for soccer games",
    "section": "Datasets",
    "text": "Datasets\nYou may need to fine tune the models on more soccer game videos with annotations. Here are some datasets that can be useful:\nSoccerNet is a large-scale dataset for soccer analysis. It contains 550 complete broadcast soccer games and 12 single camera games taken from the major European leagues. It supports various vision tasks such as action spotting, camera calibration, player re-identification and tracking.\nThis Kaggle dataset also contains soccer game videos from Premier League showdowns to FIFA World Cup classics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "href": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "title": "Computer vision for soccer games",
    "section": "Business use cases",
    "text": "Business use cases\nBoardly, here are some areas where computer vision can be used in soccer analytics:\n\nPerformance Analysis: By tracking player movement, positioning, and interactions, teams can better understand individual and team performance, making it easier to identify strengths and areas for improvement.\nTactical Insights: Coaches can analyze formations, pressing patterns, and set-pieces to gain a competitive edge, adjusting their game plans based on data.\n** Player Development**: Young athletes can leverage computer vision technology to receive feedback on their performance and improve their skills over time.\nFan Engagement: Computer vision can create engaging, immersive content for fans, such as 3D replays or interactive match highlights, bringing them closer to the action.\n\nHere is a very incomplete list of companies and use cases:\n\nVeo: AI-powered cameras for automatic sports recording, tracking game action, and AI-tagged highlights for analysis.\nTraceup: Video captures that allow tracking players individually, creating personalized highlight reels that parents, players, and coaches can view from various angles.\nTrack160: Skeleton tracking, identifying and monitoring the movement of players and the ball, tagging and analyzing events in a match, physical and tactical breakdowns of player performances.\nNY Times created 3D stories that allow fans to experience game-defining moments from multiple angles and gain deeper insights into player positioning, ball movement, and tactics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#conclusion",
    "href": "posts/20240915-soccer-tracking/index.html#conclusion",
    "title": "Computer vision for soccer games",
    "section": "Conclusion",
    "text": "Conclusion\nThis is just a start. I am glad to see computer vision applied to everyday life, and hope this post spark some ideas."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html",
    "href": "posts/20240825-sustainable-future/index.html",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "",
    "text": "Since the advent of the free market, human society has experienced an unprecedented wave of growth and prosperity. Global GDP has increased 100-fold, with per-capita GDP rising 15-fold since the early 1800s. However, this tremendous growth has exacted a significant toll on the environment. As we stand on the brink of another nascent revolution, artificial intelligence (AI) can usher in a second wave of growth—this time, much more sustainable. Could AI help us achieve the elusive goal of expanding our economies while preserving the planet?"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "href": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "The Miracle and Pitfall of Demand-Driven Production",
    "text": "The Miracle and Pitfall of Demand-Driven Production\n\nThe miracle\nThe past two centuries have indeed been nothing short of a miracle in terms of economic growth, not just for the sheer scale of economic expansion but for its profound impact on human well-being.\nBefore the Industrial Revolution, global poverty was widespread, with the vast majority of the population living on subsistence agriculture, vulnerable to disease, famine, and political instability. But with the advent of mechanized production, steam power, and eventually electricity, societies began to shift from agrarian economies to industrial ones, spurring rapid urbanization and creating millions of new jobs.\n\n (source)\n\n (source)\nAs economies grew, so did living standards. In the 20th century, especially after World War II, growth accelerated dramatically. Advances in medicine, sanitation, and food production allowed populations to boom while simultaneously reducing mortality rates. Global poverty, which once seemed an inescapable fate for most, began to decline sharply. According to the World Bank, extreme poverty (defined as living on less than $1.90 a day) fell from about 80% of the world’s population in 1820 to less than 10% today. This reduction in poverty was most pronounced in Asia, where countries like China and India harnessed industrialization and global trade to lift hundreds of millions out of destitution.\nAs the engines of industry roared to life, they did more than just produce—they created a world where, for the first time, sufficient goods could be made to meet the needs of millions. Farms, once worked by hand, now harnessed the power of machines, yielding crops at unprecedented rates. Factories churned out textiles, tools, and eventually, the comforts of modern living that had once been unimaginable luxuries. This newfound capacity wasn’t just about survival; it was about abundance. Goods that had once been scarce or accessible only to the wealthy became attainable for the masses. Food production soared, homes were built, and technologies that improved everyday life spread across the globe. In this wave of growth, the world became a place where production was not only sufficient but could also fulfill the aspirations of those who sought more than just the bare necessities.\n\n\nThe pitfall\nWhile we feel grateful for the growth and abundance that this era of production has brought us, it’s important to recognize the shadows cast by this prosperity. For every product that meets a need, there are countless others that sit unused, discarded, or wasted. The very systems that allowed us to produce more than ever before also led to overproduction, filling landfills with excess and polluting our air and waters with the byproducts of unchecked growth.\n\n\n\nCar graveyard after Chinese company went bankrupt. Source: @Wolf of X\n\n\n\n\n\nAerial picture of the tire graveyard in Kuwait. Final resting place of over 7,000,000 rubber tires. Source: @Wolf of X\n\n\n\n\n\nClothing graveyard. The so-called “clothing graveyard,” about 30,000 tons of discarded clothing piled in a landfill in the Atacama Desert, Chile, in 2021. Source: Antonio Cossio—picture alliance/Getty Images\n\n\nIs such a level of waste inevitable? I would argue that it is, given the nature of how our economies have evolved. The growth we’ve witnessed, particularly over the last century, has been driven largely by demand—an insatiable appetite for more. With the rise of consumerism, the focus shifted from simply meeting needs to creating new desires. As historian Frederick Allen observed, “Business had learned as never before the importance of the ultimate consumer. Unless he could be persuaded to buy and buy lavishly, the whole stream of six-cylinder cars, super heterodynes, cigarettes, rouge compacts, and electric ice boxes would be dammed up at its outlets.” (source)\nThis relentless push to fuel demand led companies to innovate not just in production but also in persuasion. Advertising, marketing, and product design all became tools to keep the consumer engaged and always wanting more. The result? A system where the pressure to buy, to replace, and to upgrade created a cycle of overproduction and, inevitably, waste."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "href": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Is consumerism at fault?",
    "text": "Is consumerism at fault?\nThe solution is not to stay away from consumerism and demand-driven market economy. Without demand, there would be no profit, and without profit, companies would have no reason to put products on the market. This, in turn, would halt productivity, leaving not enough food on families’ tables or goods in their homes.\nOver-production is also inevitable. The reality is that producing just enough to meet actual needs isn’t sufficient, because market systems and distribution networks are inherently imperfect. Food, clothes that are produced do not always reach who need them. True efficiency is hard to achieve, and inequality makes this even worse. If the distribution efficiency is only 10%, then we must produce ten times the necessary amount to meet the demand. This excess production, while ensuring availability, often leads to surplus and waste.\nSurplus eats into profits if it isn’t consumed. To keep factories running, corporations thriving, and jobs secure, our dear consumers must continually want more. This is the crux of the demand-driven economy: without constant consumption, the entire system risks stagnation. As a result, businesses invest heavily in marketing, innovation, and new product lines to stimulate desire, encouraging consumers to keep buying—whether or not their needs have truly changed.\n\n\n\nGrowth of supply and manufactured demand beyond need"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "href": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "A way out: targeted production with AI",
    "text": "A way out: targeted production with AI\nAmazon’s inventory planning system points to a promising direction. Algorithms can forecast what consumers are likely to purchase with remarkable accuracy, which allow buying and placing inventory accordingly to optimize order fulfillment. As a result, efficiency went up, and waste went down.\nAnd we can push this even further. If demand is way higher than actual need, why not shift production to better match what people really need? Imagine if we weren’t constantly hit with endless ads and social media bragging. Our homes would be less cluttered, people wouldn’t need to take on debt just to keep up with the luxury status game, and we could all spend more time with loved ones or out in nature. Life would feel simpler and more focused on what really matters, rather than being driven by overconsumption.\nThis can happen through targeted production, with AI helping in two ways: automation (boosting production efficiency) and forecasting (improving market efficiency).\nAutomation isn’t new—it’s been part of past tech revolutions—but AI is different because it’s more versatile. It can handle many things from language tasks to tool use, extending the ‘Crown Jewels’ of human intelligence. AI can streamline workflows across corporate functions like accounting, finance, engineering, sales, and marketing, making processes faster and more efficient. Forecasting will further increase market efficiency by accurately predicting demand, allowing businesses to align production more closely with real-time consumer needs. These two factors—automation and forecasting—make anticipatory production and just-in-time production possible. Instead of waiting for demand to fully materialize, we can anticipate and initiate production just ahead of time—producing what is likely needed, when it’s needed.\nIndirectly, AI can help curb the constant stimulation of consumer desires. The problem isn’t advertising itself but rather the excessive advertising that arises in overcrowded, saturated markets. When businesses struggle to meaningfully differentiate their products, they rely heavily on aggressive marketing to capture attention, contributing to the cycle of overconsumption. This reflects poor planning and a lack of clear insight into what consumers truly need—a symptom of incomplete information and insufficient foresight into future demands.\nWhen businesses begin to realize they can be profitable with automation and better planning instead of excessive advertising, they can step back from the exhausting zero-sum game of trying to out-market each other. Their shareholders and employees can finally find peace."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "href": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe AI revolution is still in its early days, and there are challenges like job displacement and energy use that worry people. But despite these hurdles, I am hopeful AI can help future generations enjoy a more sustainable and prosperous future."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html",
    "href": "posts/20250712-agentic-causal/index.html",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of sciences. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why.\nThe plural on sciences is intentional. I want to emphasize the range of disciplines: physics, chemistry, biology, economics, sociology, computer science, data science, you name it. But in this post I would like to dedicate my attention to causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-sciences",
    "href": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-sciences",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of sciences. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why.\nThe plural on sciences is intentional. I want to emphasize the range of disciplines: physics, chemistry, biology, economics, sociology, computer science, data science, you name it. But in this post I would like to dedicate my attention to causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "href": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "title": "Agentic Causal Inference",
    "section": "Causal Inference and LLMs",
    "text": "Causal Inference and LLMs\n\n\n\n\n\nCausal inference is such an important decision making tool in life and in business. However, to be an expert in this field takes years of mathematical and statistical training. LLMs on the other hand are easy to use, but they lack rigor when reasoning about causality.\nIntegrating causality into LLM agents addresses limitations on both sides:\n\nPure causal methods demand strict assumptions and expert guidance.\nLLMs overflow with knowledge yet often mistake correlation for causation.\n\nBy wiring LLM‑based agents to specialized causal inference libraries, we can automate the causal workflow: discovery -&gt; identification -&gt; estimation -&gt; refutation. The result is a new class of general‑purpose causal AI systems that parse tabular, time-series, and even multimodal data with human-like intuition and mathematical rigor.\nPractically, that means the agent:\n\nThinks (via an LLM) about what causal graph should link your variables\nActs by writing Python: drawing DAGs, running ID algorithms, calling estimators, using libraries like dowhy, econml, causaltune, etc.\nReflects on the results, prompting itself with “Do my assumptions still hold?”\nIterates until a relevant answer surfaces, or it asks you for help.\n\nIf that sounds suspiciously like a data scientist teamate with infinite patience, you’ve got the gist.\n\n10 lines of code for your first causal inference agent\nTo illustrate the idea, here is a minimal snippet to estimate the effect of a new coupon on revenue. This may actually be sufficient to get you a quick answer, if (a big if) the data is ready.\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.llms import OpenAI\nfrom dowhy import CausalModel\nfrom causaltune import AutoTune\n\ndef estimate_ate(df, treatment, outcome):\n    model = CausalModel(data=df, treatment=treatment, outcome=outcome)\n    ided = model.identify_effect()\n    best = AutoTune(model, df).best_estimator_\n    return model.estimate_effect(ided, method_name=best)\n\nagent = initialize_agent(\n    llm=OpenAI(model_name=\"gpt-4o-mini\"),\n    tools=[Tool.from_function(estimate_ate)],\n    agent_type=\"openai-tools\"\n)\n\nagent.run(\"Estimate the uplift of coupon_v2 on weekly revenue\")\nTime to look at some interesting papers and open source projects:\n\n\n\nSingle‑Agent Autonomous Pipelines\nCausal Agent framework (2024):\n\nAn LLM operates in a ReAct‑style loop with a suite of causal tools—e.g. CausalLearn for graph discovery and EconML for effect estimation.\nGiven a dataset and a query (e.g. “Effect of X on Y?”) the agent automatically:\n\nexplores variable correlations,\nhypothesizes causal links,\nproposes a causal graph,\ncomputes the quantitative effect of X on Y.\n\nEach step is backed by library outputs that the LLM interprets before deciding its next move.\n\nThe framework’s hierarchical breakdown (variable‑level, edge‑level, graph‑level, effect‑level) has produced expert‑level accuracy on a testing dataset with 1.3k questions, all while providing interpretable explanations.\n\nCausal-Copilot (2025)\n\nThe agent chains 20 + causal tools, from discovery to hyper-parameter tuning, inside a single LLM loop.\n\nWorks on both tabular and time-series data: prompts the user for a question, auto-selects the right discovery algorithm (e.g., NOTEARS, PC), tunes an estimator (DoubleML, CausalForest, IV), runs refuters, and returns an English report with effect size + CI.\nAchieves state-of-the-art graph accuracy and effect-estimation error across five public benchmarks, edging out both classic SCD baselines and earlier LLM agents.\n\n\n\n\nDebating Multi‑Agent Systems for Causal Discovery\nSingle agents sometimes hallucinate; multi‑agent approaches aim to reduce errors through debate and consensus.\n\nMulti‑Agent Causal Discovery Using LLMs (2024) assigns dedicated LLM roles:\n\nAffirmative Debaters proposes a DAG using temporal cues and domain priors.\nNegative Debaters attacks the proposal by surfacing hidden confounders, incorrect temporal orderings, or omitted variables.\nJudges evaluate arguments and pick the most plausible structure.\nCoders materializes the agreed-upon algorithm, reruns it on the entire dataset, and emits the refined graph.\n\n\nExperiments show these debating agents outperform both classical algorithms and single‑LLM prompts on datasets like Auto MPG, demonstrating that multiple specialized minds can yield more reliable causal graphs.\n\nChain-of-Collaboration Prompting (2025) shows that giving sub-agents explicit roles (planner, verifier, critic) and letting them share scratch pads improves causal reasoning accuracy on CLADDER and Causal-Copilot QA tasks, cutting hallucinated edges by 35 % vs. single-prompt ReAct.\n\n\n\nToolbox Layer (AutoML & No‑Code Platforms)\nParallel to LLM research, we also see AutoML‑style causal platforms that automate model selection, tuning, and robustness checks.\n\nAutoCausality: part of the PyWhy ecosystem, using hyper‑parameter search and ensembling to choose the best estimator for a dataset.\nOpportunityFinder (Amazon 2023) offers code‑less causal studies for panel data cleaning, cohorting, and computing effects (plus sensitivity) end‑to‑end.\nSalesforce CausalAI Library consolidates discovery and inference methods, synthetic data generators, and a no‑code GUI, scaling to larger problems via optimized multiprocessing.\n\nThese toolkits enrich agentic workflows: an LLM planner can mix‑and‑match discovery, estimation, and AutoML selection modules without human intervention."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "href": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "title": "Agentic Causal Inference",
    "section": "Evaluating Causal Inference Agents",
    "text": "Evaluating Causal Inference Agents\nHow well do these causal inference agents perform? Here are some real or synthetic datasets and benchmarks.\nFor treatment‑effect estimation, the Lalonde job‑training study is a good place to start. It has real observational covariates paired with true RCT outcomes—to sanity‑check bias reduction. When larger, controlled replications are needed, you can use semi‑synthetic generators such as IHDP and the Twins dataset, whose perfect counterfactual comes from each twin’s paired outcome. The annual ACIC challenges extend this idea with dozens of high‑dimensional scenarios, while the 2025 RealCause generator allows people to create realistic Lalonde‑style benchmarks.\nFor longitudinal uplift studies, Amazon’s no‑code OpportunityFinder panels ship sample retail datasets ready for difference‑in‑differences.\nWhen it comes to graph discovery methods, people tend to use classic datasets such as the 11‑node Sachs protein‑signaling map, a real wet‑lab interventions dataset. Bayesian‑network classics like Asia and ALARM remain quick smoke tests. Pairwise direction algorithms rely on the Tübingen cause–effect pairs, and larger time‑series graphs come from gene‑regulation contests such as DREAM4.\nMore recently we see language‑centric causal benchmarks. CLADDER has 10k natural language questions across Pearl’s ladder, while ACCESS asks agents to build abstract causal graphs over multimodal vignettes before answering why queries.\nAs to multimodal causal inference, CausalVQA is a benchmark for video question answering (VQA) that test models’ understanding of causality in the physical world."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "href": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "title": "Agentic Causal Inference",
    "section": "Challenges and Mitigations",
    "text": "Challenges and Mitigations\nGoing beyond the happy path to production is often not a smooth ride. Here are some of common challenges in my experience:\n\nData Quality and the Missing Confounders\nObservational datasets rarely contain every variable that shapes a treatment–outcome relationship, so even a state‑of‑the‑art estimator can inherit hidden bias.\nTo mitigate, insert a human‑review checkpoint right after the agent proposes its first causal graph: domain experts eyeball edges and nominate missing covariates. The software then launches automatic robustness probes such as placebo tests, synthetic confounder injections, and other refutation modules shipped with DoWhy, to quantify how fragile the estimate is. Crucially, if any refutation fails, the planner LLM must stop, annotate the failure, and either revise the graph or escalate to a human reviewer; surfacing a shaky result as “tentative” is better than silently proceeding. Some teams also run a “data‑profiling agent” that scans fresh tables for covariate drift or sparsity and warns the planner before analysis starts.\n\n\nHallucinations and Over‑Confidence in Planner LLMs\nLLM planners are persuasive storytellers; a well phrased chain‑of‑thought can make a shaky causal graph feel ironclad.\nMulti‑agent debate is a good recipe to reduce hallucination: a second LLM plays devil’s advocate, and challenges the assumptions that make an estimate causal:\n\nPlacebo‑treatment test: replace the real treatment with a fake; any non‑zero effect flags hidden bias.\nSynthetic‑confounder injection: add a random common cause and observe the ATE shift; big swings imply unmeasured confounding.\nOverlap / positivity audit: verify that propensity scores span both arms; poor overlap triggers trimming or doubly robust methods.\nCross‑estimator consensus: pit a back‑door learner against an IV or front‑door estimator; disagreement above a threshold routes to human review.\nMulti‑agent debate: affirmative and negative debaters contest every edge, a judge scores coherence.\n\nIf any probe fails, the planner either tightens assumptions and reruns discovery or clearly labels the conclusion “inconclusive, additional data needed.” Final reports must surface the point estimate plus confidence intervals, sensitivity ranges, and a pass/fail tally for each refuter, so stakeholders see magnitude and robustness.\n\n\nModel‑Selection Over‑Fit and Cross‑Estimator Disagreement\nAuto‑tuning libraries can explore dozens of learners and hyper‑parameters, sometimes over‑fitting small causal datasets, especially with flexible models like causal forests. In this case, AutoML learns noise instead of real signal.\nMitigations include nested cross‑validation inside AutoCausality or causaltune, and parsimony priors that penalize needless complexity. If resource allows, the agent should run at least two conceptually different estimators, e.g., a back‑door regression and an instrumental‑variable model, and flag any large divergence in effect size as a red‑flag for human review.\n\n\nComputation Cost vs. Real‑Time Ambitions\nA planner–solver split can still burn thousands of tokens and heavy compute if the planner explores many what‑if branches.\nProduction dashboards cache discovery and refutation outputs keyed by a DAG hash; if the graph hasn’t changed, the agent re‑uses prior results. Another recipe is distilling a heavy LLM planner into a small fine‑tuned local model covers day‑to‑day traffic, while the costly cloud model handles weekly deep dives.\n\n\nPrivacy and Governance\nSensitive data such as medical records, customer logs usually cannot leave a private cluster.\nHybrid deployments solve this: an on‑prem LLM handles data‑aware steps, while a redacted summary (no PII) is sent to a cloud model for high‑level planning. All explanations pass through a redaction layer before logging, and every causal report carries an audit trail plus role‑based access controls."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#conclusion",
    "href": "posts/20250712-agentic-causal/index.html#conclusion",
    "title": "Agentic Causal Inference",
    "section": "Conclusion",
    "text": "Conclusion\nCausal inference is transitioning from a highly specialized skill to a widely accessible capability. That’s not putting anyone out of a job. It is freeing us to ask better questions. A couple of years ago, answering “what actually drives our north star metric?” meant a quarter-long project. Today, it may be weeks or even days. That’s not just a productivity gain. It is a fundamental change in how we can think about our businesses."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html",
    "href": "posts/20260219-mcp-proprietary-data/index.html",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "",
    "text": "If you are building AI software for enterprise clients, the model is usually not the bottleneck. Data access is.\nThe real challenge is getting the model to interact with proprietary systems without turning your stack into a giant security and operations liability. Model Context Protocol (MCP) is a strong interface standard for this, but the hard part is not the protocol itself. The hard part is production design.\nIn this post, I will walk through five practical MCP deployment patterns, when to use each one, and how to organize code so examples stay runnable instead of becoming blog-only snippets.\nThe framing comes from a pattern I keep seeing in enterprise AI programs:\nThe difference is usually not model quality. It is architecture choices made in the first few weeks."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#executive-summary",
    "href": "posts/20260219-mcp-proprietary-data/index.html#executive-summary",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Executive Summary",
    "text": "Executive Summary\nIf you need one recommendation: start with read-only MCP, prove value in weeks, then add write paths and on-prem bridge patterns only when business requirements justify the added controls.\nWhat executives should optimize for:\n\nTime to first production value (not architectural completeness)\nControlled blast radius as capabilities expand\nAuditability from day one (so compliance does not block scale)\nA migration path from pilot design to enterprise-grade operations"
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#what-you-are-actually-building",
    "href": "posts/20260219-mcp-proprietary-data/index.html#what-you-are-actually-building",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "What You Are Actually Building",
    "text": "What You Are Actually Building\nWhen teams say “we are deploying an MCP server,” what they usually need is a small platform:\n\nMCP server interfaces (tools and resources)\nConnectors into internal systems (DBs, APIs, files, search)\nIdentity and authorization (OAuth, scopes, service accounts)\nRuntime controls (validation, allowlists, isolation)\nObservability and governance (audit logs, quotas, alerts)\n\nThe architecture matters because MCP introduces delegated power. The model can ask for actions. Your job is to make sure those actions stay inside explicit and enforceable boundaries."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#code-organization-keep-posts-and-runnable-code-separate",
    "href": "posts/20260219-mcp-proprietary-data/index.html#code-organization-keep-posts-and-runnable-code-separate",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Code Organization: Keep Posts and Runnable Code Separate",
    "text": "Code Organization: Keep Posts and Runnable Code Separate\nI recommend this structure in this repo:\nposts/\n  &lt;post-slug&gt;/\n    index.qmd\n    assets/\n\nexamples/\n  mcp-readonly-starter/\n  mcp-multitenant-saas/\n  mcp-write-with-approval/\n  mcp-onprem-bridge/\n  mcp-compliance-audit/\nThis avoids mixing Quarto content with runtime dependencies.\n\nposts/... is for prose, diagrams, and lightweight assets.\nexamples/... is for runnable code, tests, infra manifests, and scripts.\neach runnable example now includes scripts/seed.ts and supports STORAGE_MODE=file for restart-safe demo state.\n\nFor this article, each pattern maps to one folder in examples/ so readers can move from concept to implementation immediately."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#pattern-selection-matrix",
    "href": "posts/20260219-mcp-proprietary-data/index.html#pattern-selection-matrix",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Pattern Selection Matrix",
    "text": "Pattern Selection Matrix\nUse this to choose a starting point fast:\n\n\n\n\n\n\n\n\n\n\nPattern\nBest for\nTime to value\nOperational risk\nCompliance posture\n\n\n\n\nSingle-tenant read-only\nFirst deployment, retrieval use cases\nFast\nLow\nBasic-to-moderate\n\n\nMulti-tenant SaaS\nShared control plane across customers\nMedium\nMedium\nModerate\n\n\nWrite with approval\nSide-effecting workflows\nMedium\nMedium-high\nModerate-high\n\n\nOn-prem bridge\nCustomer-controlled infrastructure\nMedium-slow\nMedium\nHigh\n\n\nCompliance-first\nRegulated sectors, audit-heavy buyers\nSlowest\nLowest long-term\nHighest"
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#pattern-1-single-tenant-read-only-mcp",
    "href": "posts/20260219-mcp-proprietary-data/index.html#pattern-1-single-tenant-read-only-mcp",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Pattern 1: Single-Tenant Read-Only MCP",
    "text": "Pattern 1: Single-Tenant Read-Only MCP\nWhen to use: first production deployment, retrieval-heavy workloads, low ops budget.\nExample folder: examples/mcp-readonly-starter\nmcp-readonly-starter/\n  src/\n    auth/scope_guard.ts\n    tools/get_invoice_status.ts\n    observability/audit_log.ts\n  scripts/seed.ts\n  tests/security/input_validation.test.ts\nMost teams should start here. In practice, this is the pattern that gets through security review fastest and still creates visible business value.\nA narrow read-only surface with strict input schemas gets you real usage data without opening side-effect risk on day one.\nTradeoff: fastest path to production and lowest incident risk, but limited automation upside."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#pattern-2-multi-tenant-saas-mcp",
    "href": "posts/20260219-mcp-proprietary-data/index.html#pattern-2-multi-tenant-saas-mcp",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Pattern 2: Multi-Tenant SaaS MCP",
    "text": "Pattern 2: Multi-Tenant SaaS MCP\nWhen to use: one control plane serving multiple customers with strict isolation guarantees.\nExample folder: examples/mcp-multitenant-saas\nmcp-multitenant-saas/\n  src/\n    middleware/tenant_context.ts\n    middleware/authz.ts\n    data/tenant_router.ts\n  scripts/seed.ts\n  tests/authz/cross_tenant_access.test.ts\nTeams usually move to this pattern after the first customer asks for isolation guarantees, or after the second customer makes ad hoc tenant logic unmaintainable.\nThe non-negotiable rule is centralized tenant enforcement. If tenant checks are scattered per tool, drift is almost guaranteed over time.\nTradeoff: better infrastructure efficiency and faster customer onboarding, but more identity and policy complexity."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#pattern-3-write-capable-mcp-with-approval-gates",
    "href": "posts/20260219-mcp-proprietary-data/index.html#pattern-3-write-capable-mcp-with-approval-gates",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Pattern 3: Write-Capable MCP with Approval Gates",
    "text": "Pattern 3: Write-Capable MCP with Approval Gates\nWhen to use: tools can trigger financial, operational, or irreversible side effects.\nExample folder: examples/mcp-write-with-approval\nmcp-write-with-approval/\n  src/\n    tools/create_refund_request.ts\n    tools/execute_refund.ts\n    policy/risk_rules.ts\n  scripts/seed.ts\n  tests/security/scope_escalation.test.ts\nThis is where many programs either mature or accumulate operational risk.\nThe key design decision is splitting intent from execution. A model can create a request, but execution should pass through explicit policy and approval gates.\nTradeoff: safer automation for high-impact actions, at the cost of additional workflow latency."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#pattern-4-on-prem-bridge-mcp",
    "href": "posts/20260219-mcp-proprietary-data/index.html#pattern-4-on-prem-bridge-mcp",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Pattern 4: On-Prem Bridge MCP",
    "text": "Pattern 4: On-Prem Bridge MCP\nWhen to use: customer data must remain in customer-controlled infrastructure.\nExample folder: examples/mcp-onprem-bridge\nmcp-onprem-bridge/\n  src/\n    control-plane-server.ts\n    bridge-agent.ts\n    bridge/http_handlers.ts\n    workflows/job_store.ts\n  scripts/seed.ts\n  scripts/demo.sh\nThis pattern becomes necessary when customers say yes to AI features but no to data egress.\nA pull-based bridge agent is usually easier to deploy in enterprise networks than inbound callbacks. It also simplifies security review by reducing exposed inbound surface.\nTradeoff: strongest data residency story, but highest integration and support burden."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#pattern-5-high-compliance-mcp",
    "href": "posts/20260219-mcp-proprietary-data/index.html#pattern-5-high-compliance-mcp",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Pattern 5: High-Compliance MCP",
    "text": "Pattern 5: High-Compliance MCP\nWhen to use: regulated environments with strict audit and policy requirements.\nExample folder: examples/mcp-compliance-audit\nmcp-compliance-audit/\n  src/\n    server-http.ts\n    tools/list_audit_events.ts\n    tools/get_control_status.ts\n    policy/deny_by_default.ts\n    observability/redaction.ts\n  tests/audit/pii_redaction.test.ts\n  scripts/seed.ts\n  scripts/demo.sh\nSome teams start here because their buyers require it. Others arrive here after the first audit questionnaire exposes gaps.\nThis pattern is policy-first: deny by default, enforce scoped authorization everywhere, and make logs useful without leaking secrets.\nTradeoff: strongest procurement and audit posture, but slower initial delivery."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#security-baseline-across-all-patterns",
    "href": "posts/20260219-mcp-proprietary-data/index.html#security-baseline-across-all-patterns",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Security Baseline Across All Patterns",
    "text": "Security Baseline Across All Patterns\nSecurity guidance is often presented as one long checklist. In practice, sequencing matters more than volume.\nThe mistake I see most often is trying to implement every control at once and shipping none. A phased rollout keeps momentum while still moving toward a defensible production posture."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#security-rollout-by-phase",
    "href": "posts/20260219-mcp-proprietary-data/index.html#security-rollout-by-phase",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Security Rollout by Phase",
    "text": "Security Rollout by Phase\n\n\n\n\n\n\n\nPhase\nControls to implement\n\n\n\n\nPilot (must-have)\nOAuth + scopes, strict input schemas, redacted audit logs\n\n\nPre-production\nSecrets manager + rotation, allowlists, rate limits\n\n\nProduction hardening\nSandboxing for write/execute paths, anomaly detection, governance approvals for tool changes\n\n\n\nAcross phases, these seven controls remain the baseline:\n\nOAuth 2.1 + PKCE for remote transports, with strict scope checks on every tool call\nNarrow tools with allowlists for paths, domains, and command options\nStrict schema validation with unknown-field rejection\nCentralized secrets handling with short-lived credentials and rotation\nSandboxing for any write or execute path\nStructured redacted audit logs for every invocation\nRate limits, quotas, and anomaly alerts per identity\n\nThese controls are especially important for MCP because prompt injection and tool abuse are expected failure modes, not edge cases."
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#a-minimal-code-slice",
    "href": "posts/20260219-mcp-proprietary-data/index.html#a-minimal-code-slice",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "A Minimal Code Slice",
    "text": "A Minimal Code Slice\nThe core mechanics are simple.\nexport function requireScope(scopes: string[], required: string) {\n  if (!scopes.includes(required)) {\n    throw new Error(`forbidden: missing scope ${required}`);\n  }\n}\nimport { z } from \"zod\";\n\nconst Input = z.object({\n  invoiceId: z.string().regex(/^inv_[a-zA-Z0-9]+$/),\n}).strict();\nconst ALLOWED_HOSTS = new Set([\"api.internal.example.com\"]);\n\nexport function enforceHost(url: string) {\n  const host = new URL(url).hostname;\n  if (!ALLOWED_HOSTS.has(host)) throw new Error(\"host_not_allowed\");\n}"
  },
  {
    "objectID": "posts/20260219-mcp-proprietary-data/index.html#closing-thoughts",
    "href": "posts/20260219-mcp-proprietary-data/index.html#closing-thoughts",
    "title": "How to Build and Deploy MCP for Proprietary Data",
    "section": "Closing Thoughts",
    "text": "Closing Thoughts\nMCP is one of the most practical ways to make proprietary data useful in AI products. Production success has less to do with flashy demos and more to do with controlled capability rollout, isolation, and operating discipline.\nIf you want a durable rollout path, treat MCP like a product line, not a one-off integration: start with read-only value, then add power in layers as requirements become explicit.\nStart with the read-only pattern, keep the tool surface narrow, and instrument everything. Add multi-tenant routing, approval workflows, on-prem bridging, and compliance-first controls only when business requirements are concrete.\nThat path is slower than shipping a generic “super tool” on day one, but it is much faster than recovering from the first incident."
  }
]
[
  {
    "objectID": "posts/20240825-sustainable-future/index.html",
    "href": "posts/20240825-sustainable-future/index.html",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "",
    "text": "Since the advent of the free market, human society has experienced an unprecedented wave of growth and prosperity. Global GDP has increased 100-fold, with per-capita GDP rising 15-fold since the early 1800s. However, this tremendous growth has exacted a significant toll on the environment. As we stand on the brink of another nascent revolution, artificial intelligence (AI) can usher in a second wave of growth—this time, much more sustainable. Could AI help us achieve the elusive goal of expanding our economies while preserving the planet?"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "href": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "The Miracle and Pitfall of Demand-Driven Production",
    "text": "The Miracle and Pitfall of Demand-Driven Production\n\nThe miracle\nThe past two centuries have indeed been nothing short of a miracle in terms of economic growth, not just for the sheer scale of economic expansion but for its profound impact on human well-being.\nBefore the Industrial Revolution, global poverty was widespread, with the vast majority of the population living on subsistence agriculture, vulnerable to disease, famine, and political instability. But with the advent of mechanized production, steam power, and eventually electricity, societies began to shift from agrarian economies to industrial ones, spurring rapid urbanization and creating millions of new jobs.\n\n (source)\n\n (source)\nAs economies grew, so did living standards. In the 20th century, especially after World War II, growth accelerated dramatically. Advances in medicine, sanitation, and food production allowed populations to boom while simultaneously reducing mortality rates. Global poverty, which once seemed an inescapable fate for most, began to decline sharply. According to the World Bank, extreme poverty (defined as living on less than $1.90 a day) fell from about 80% of the world’s population in 1820 to less than 10% today. This reduction in poverty was most pronounced in Asia, where countries like China and India harnessed industrialization and global trade to lift hundreds of millions out of destitution.\nAs the engines of industry roared to life, they did more than just produce—they created a world where, for the first time, sufficient goods could be made to meet the needs of millions. Farms, once worked by hand, now harnessed the power of machines, yielding crops at unprecedented rates. Factories churned out textiles, tools, and eventually, the comforts of modern living that had once been unimaginable luxuries. This newfound capacity wasn’t just about survival; it was about abundance. Goods that had once been scarce or accessible only to the wealthy became attainable for the masses. Food production soared, homes were built, and technologies that improved everyday life spread across the globe. In this wave of growth, the world became a place where production was not only sufficient but could also fulfill the aspirations of those who sought more than just the bare necessities.\n\n\nThe pitfall\nWhile we feel grateful for the growth and abundance that this era of production has brought us, it’s important to recognize the shadows cast by this prosperity. For every product that meets a need, there are countless others that sit unused, discarded, or wasted. The very systems that allowed us to produce more than ever before also led to overproduction, filling landfills with excess and polluting our air and waters with the byproducts of unchecked growth.\n\n\n\nCar graveyard after Chinese company went bankrupt. Source: @Wolf of X\n\n\n\n\n\nAerial picture of the tire graveyard in Kuwait. Final resting place of over 7,000,000 rubber tires. Source: @Wolf of X\n\n\n\n\n\nClothing graveyard. The so-called “clothing graveyard,” about 30,000 tons of discarded clothing piled in a landfill in the Atacama Desert, Chile, in 2021. Source: Antonio Cossio—picture alliance/Getty Images\n\n\nIs such a level of waste inevitable? I would argue that it is, given the nature of how our economies have evolved. The growth we’ve witnessed, particularly over the last century, has been driven largely by demand—an insatiable appetite for more. With the rise of consumerism, the focus shifted from simply meeting needs to creating new desires. As historian Frederick Allen observed, “Business had learned as never before the importance of the ultimate consumer. Unless he could be persuaded to buy and buy lavishly, the whole stream of six-cylinder cars, super heterodynes, cigarettes, rouge compacts, and electric ice boxes would be dammed up at its outlets.” (source)\nThis relentless push to fuel demand led companies to innovate not just in production but also in persuasion. Advertising, marketing, and product design all became tools to keep the consumer engaged and always wanting more. The result? A system where the pressure to buy, to replace, and to upgrade created a cycle of overproduction and, inevitably, waste."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "href": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Is consumerism at fault?",
    "text": "Is consumerism at fault?\nThe solution is not to stay away from consumerism and demand-driven market economy. Without demand, there would be no profit, and without profit, companies would have no reason to put products on the market. This, in turn, would halt productivity, leaving not enough food on families’ tables or goods in their homes.\nOver-production is also inevitable. The reality is that producing just enough to meet actual needs isn’t sufficient, because market systems and distribution networks are inherently imperfect. Food, clothes that are produced do not always reach who need them. True efficiency is hard to achieve, and inequality makes this even worse. If the distribution efficiency is only 10%, then we must produce ten times the necessary amount to meet the demand. This excess production, while ensuring availability, often leads to surplus and waste.\nSurplus eats into profits if it isn’t consumed. To keep factories running, corporations thriving, and jobs secure, our dear consumers must continually want more. This is the crux of the demand-driven economy: without constant consumption, the entire system risks stagnation. As a result, businesses invest heavily in marketing, innovation, and new product lines to stimulate desire, encouraging consumers to keep buying—whether or not their needs have truly changed.\n\n\n\nGrowth of supply and manufactured demand beyond need"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "href": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "A way out: targeted production with AI",
    "text": "A way out: targeted production with AI\nAmazon’s inventory planning system points to a promising direction. Algorithms can forecast what consumers are likely to purchase with remarkable accuracy, which allow buying and placing inventory accordingly to optimize order fulfillment. As a result, efficiency went up, and waste went down.\nAnd we can push this even further. If demand is way higher than actual need, why not shift production to better match what people really need? Imagine if we weren’t constantly hit with endless ads and social media bragging. Our homes would be less cluttered, people wouldn’t need to take on debt just to keep up with the luxury status game, and we could all spend more time with loved ones or out in nature. Life would feel simpler and more focused on what really matters, rather than being driven by overconsumption.\nThis can happen through targeted production, with AI helping in two ways: automation (boosting production efficiency) and forecasting (improving market efficiency).\nAutomation isn’t new—it’s been part of past tech revolutions—but AI is different because it’s more versatile. It can handle many things from language tasks to tool use, extending the ‘Crown Jewels’ of human intelligence. AI can streamline workflows across corporate functions like accounting, finance, engineering, sales, and marketing, making processes faster and more efficient. Forecasting will further increase market efficiency by accurately predicting demand, allowing businesses to align production more closely with real-time consumer needs. These two factors—automation and forecasting—make anticipatory production and just-in-time production possible. Instead of waiting for demand to fully materialize, we can anticipate and initiate production just ahead of time—producing what is likely needed, when it’s needed.\nIndirectly, AI can help curb the constant stimulation of consumer desires. The problem isn’t advertising itself but rather the excessive advertising that arises in overcrowded, saturated markets. When businesses struggle to meaningfully differentiate their products, they rely heavily on aggressive marketing to capture attention, contributing to the cycle of overconsumption. This reflects poor planning and a lack of clear insight into what consumers truly need—a symptom of incomplete information and insufficient foresight into future demands.\nWhen businesses begin to realize they can be profitable with automation and better planning instead of excessive advertising, they can step back from the exhausting zero-sum game of trying to out-market each other. Their shareholders and employees can finally find peace."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "href": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe AI revolution is still in its early days, and there are challenges like job displacement and energy use that worry people. But despite these hurdles, I am hopeful AI can help future generations enjoy a more sustainable and prosperous future."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html",
    "href": "posts/20241002-minisora-part1/index.html",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#introduction",
    "href": "posts/20241002-minisora-part1/index.html#introduction",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "href": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Pareto frontier: Aiming for good and small",
    "text": "Pareto frontier: Aiming for good and small\n\n\n\n\n\nThis graph from “Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI” illustrates a key challenge in AI development. SORA would be a frontier model at the resource-intensive end of the spectrum. We want to move in the direction of the green arrow, striving for lower training cost while maintaining high quality.\nAccess to vast computational resources, such as 10,000 A100 GPUs, is limited to a handful of organizations. Even if such resources were widely available, focusing solely on resource-intensive methods would be an inefficient use of our capabilities. The design space for training recipes is vast, and a strategic approach involves exploring this space through low-cost experiments before scaling up when confidence is high.\nThis raises an intriguing question: With a modest budget, is it possible to train a general-purpose video generation model comparable to SORA?"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "href": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "The need for controlling the domain complexity",
    "text": "The need for controlling the domain complexity\n\nThe challenge of training general-purpose models with limited resources\nSORA’s training costs likely run into tens of millions of dollars, driven by both data and model size. Larger datasets necessitate longer training times, while bigger models require both extended training periods and more high-end GPUs.\nIs it feasible to train a high-quality model with a significantly smaller dataset? This seems impossible due to the inherent complexity of our world. Are one million video clips sufficient to capture our world’s complexity? 10 Million? 100 Million? Probably more than that. While sample-efficient algorithms can help reduce the required data size, the order of magnitude for necessary data likely remains substantial.\nSimilarly, training a high-quality model with a much smaller architecture presents its own challenges. Unless a dramatically more efficient architecture than Transformers emerges, a small model would struggle to capture the complexity present in such vast datasets.\nTherefore, to make progress with limited resources, we must find ways to reduce the data size.\n\n\nExploring niche domains: A path to low-budget training\nNiche domains can be significantly simpler than our physical world, potentially allowing a few tens of thousands of observations to sufficiently represent the domain. With a drastic reduction in data size, smaller models and lower training costs become feasible.\nWe can conceptualize a series of domains, progressing from simple to complex:\n\n2D Flying MNIST (a 2D world with colorful handwritten digits moving at constant speed, bouncing off boundaries)\n2D arcade games (Pong, Breakout, etc.)\nAnime and cartoons\nLimited locations: video walkthroughs of 3D house models, fly-through views of objects (e.g., NERF models)\nLimited objects: close-up videos of specific subjects (e.g., dogs, selfie videos)\nLimited scenery: footage of hiking trails, beaches, etc.\nPublic video datasets: UCF-101, Panda-70M, InterVid, etc.\nThe real world, and our collective video reservoir.\n\nA strategic approach involves starting from the simplest domain and gradually progressing towards more complex ones. Effective training recipes discovered in simpler domains are expected to scale to more complex scenarios with straightforward increases in data and model size.\nInterestingly, this mirrors how humans learn: start from simple lessons and gradually build up to more complex concepts.\n\n\nPre-train or fine-tune?\nFine-tuning is an effective strategy to reduce training costs, but it comes with certain limitations:\n\nFixed architecture: The model’s architecture is predetermined, which can be a significant constraint as we may still be far from an optimal design for video generation tasks.\nVAE dependency: Pre-trained weights often rely on a specific Variational Autoencoder (VAE), limiting the design space and opportunities to further reduce training costs.\n\nDespite these limitations, fine-tuning has shown promising results. For example, the team at Lambda Labs open-sourced an intriguing Text2Bricks experiment, fine-tuning OpenSora weights on Lego videos. This project required approximately 1000 A100 GPU hours and 10,000 videos. We can anticipate further reductions in cost as more advanced pre-trained models become available and more sample-efficient fine-tuning algorithms are developed.\nFor my experiments, I try to find the simplest domain that has non-trivial complexity: a toy 2D world with flying digits. The scale of this toy world is small enough that pre-training from scratch is not prohibitively expensive, allowing for more freedom in exploring different model architectures and training strategies.\nLet’s see some details."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "href": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Flying MNIST Simulator",
    "text": "Flying MNIST Simulator\nA Python script is used to simulate a toy 2D world where colorful handwritten digits fly and bounce around. An example is shown below.\n\n\n\n\n\nFor training, I used up to 100k clips, each with 32 frames, covering roughly 6 seconds at 5 fps. This amounts to 160 hours of video. Is this a lot? Let’s compare with human learning. If a baby is awake and actively observing 5 hours per day, it would be roughly a month of learning. It would be interesting to see if the AI can learn:\n\nObject identity: a digit is a digit, and not a random blob\nObject permanence: a digit does not suddenly disappear\nDistinct digits: whether the model can learn to distinguish between different digits\nConsistent colors: color of a digit remains consistent\nPhysics: digits follow simple physics - constant speed and bounce off walls"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "href": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "VAE: The Compressor",
    "text": "VAE: The Compressor\nThe first model to train is a compressor. Unlike language, images and videos have very high dimensionality: a tiny 2-second 256x256 video contains over 100 million numbers. Compression is necessary for the model to work.\nThe compressor of choice is a VAE (Variational Auto-Encoder) with an encoder and decoder. The encoder converts a video clip into a latent space, and the decoder converts the latent space back to a video clip. The latent space is a compact representation of the original data and is easier to model.\nOptionally, you can quantize the latent space using vector quantization, which gives you a VQ-VAE. Quantization gives rise to a vocabulary of visual words or tokens. This enables the use of language model training recipes on 1-dimensional (flattened) sequences of token IDs. While I was initially skeptical, the results were surprisingly good.\nTraining a small VAE is relatively quick. I trained a spatial-temporal VQ-VAE with 4x temporal compression and 4x4 spatial compression, using a vocabulary size of 5120. The training run documented in Weights & Biases achieved a good balance of reconstruction quality and compression rate. It took about 2 A10 GPU hours to converge.\nWith this VAE model, you can transform a 32-frame video clip (32 x 3 x 256 x 256) into latent “tokens”. Without quantization, the compressed representation of the video has a shape of 8 x 4 x 64 x 64 (each “token” is a 4-dimensional floating point vector, and there are 8 x 64 x 64 = 32,768 tokens). With quantization, the compressed representation is simply 8 x 64 x 64 = 32,768 integers (token IDs). The range of the token IDs is from 0 to 5,023.\n\n\n\n\n\nWith this compact tokenized representation, we are ready to train a generator."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "href": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Generator in the Latent Space",
    "text": "Generator in the Latent Space\nThere are two approaches to generate video in the latent space: the autoregressive next-token predictor (language model) and the diffusion model.\n\nAutoregressive Next-Token Predictor\nEach 32-frame video clip is represented as a sequence of 32,768 tokens. The video clips are then concatenated to form a long sequence, separated by a special start-of-video token. This long sequence is fed into a language model training recipe.\nI used nanoGPT to train a 60MB model with the GPT-2 architecture. The model is trained to predict the next token ID in the latent space, instead of the next English token. It worked surprisingly well and began to learn the spatial-temporal patterns quickly.\nThe main ingredient for video quality is ensuring a sufficiently large context window. I used 6,000 tokens, which is much larger than the typical GPT-2 setting. However, this is still a small window size for video. Each video frame is 4,096 tokens, so this context window allows the model to look back only slightly more than one frame, making temporal consistency challenging to enforce.\nSecondly, the training sample size is crucial. Using 100k clips produces better results than 10k clips, and much better than 1k clips. The question remains whether we should use even more data. I hope not, as if such a simple 2D world requires much more than 100k training examples, it would be concerning for more complex domains.\nThis training run showcases one of the better results using nanoGPT.\nThe generated videos start out as random compositions of visual tokens:\n\n\nVideo\n\n\nAfter 6 hours of training, line strokes started to appear:\n\n\nVideo\n\n\n24 hours in, the digits began to emerge, but temporal consistency was poor:\n\n\nVideo\n\n\nAfter 10 days, consistency and physics were much improved:\n\n\nVideo\n\n\nFor comparison, here’s a training run using a 1,024 token context window.\nWith a smaller context window, the training time is much shorter (1 day to converge), but temporal consistency is poor, and digits would suddenly appear throughout the clip:\n\n\nVideo\n\n\n\n\nDiffusion Model\nFor the diffusion model, I used ST-DIT from OpenSora and Stable Diffusion’s SD VAE.\nIn this approach, the context window encompasses the entire video clip, so I expected more temporal consistency than the autoregressive counterpart. Training sample size still plays a significant role. Using a 24GB A10 GPU, I needed to use a small version of the diffusion transformer model.\nA representative training run can be found here.\nThe generated videos also start out as random compositions of visual tokens (resembling crops of natural images this time):\n\n\nVideo\n\n\nAfter one day of training, localized dream-like flowing patterns emerged, though they didn’t yet resemble digits:\n\n\nVideo\n\n\nOn day 3, the moving patterns began to look like digits, but they were so fluid that they seemed to lack “bones”-like structure:\n\n\nVideo\n\n\nBy day 10, the digits were much more stable and distinct, and the moving patterns were steady and smooth:\n\n\nVideo"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#whats-next",
    "href": "posts/20241002-minisora-part1/index.html#whats-next",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "What’s Next",
    "text": "What’s Next\n250 A10 hours (or approximately 80 A100 hours, costing around $200) proved sufficient to adequately solve the video generation task for the 2D toy world of Flying MNIST Digits.\nContext window size and data sample size are important factors for quality, but also drive up cost. There are numerous new techniques that are worth exploring to improve quality while reducing cost. Here’s a non-exhaustive list:\n\nFlow matching: This technique could enhance the temporal consistency of generated videos.\nBetter quantized VAE for auto-regressive video generation: Improving the VAE could lead to more efficient and higher-quality latent representations.\nToken masking: This could reduce the \\(N\\) in the \\(O(N^2)\\) complexity of attention layers, potentially speeding up training and inference.\nCoarse-to-fine generation: Generating whole video frames at the coarse level first, then progressively refining to small details. This can dramatically reduce the context window size and compute cost.\nBetter positional encoding for long context windows in the temporal-spatial setting.\nHyper-optimized LLM training with long context (e.g., llm.c).\nCombining strengths of autoregressive and diffusion models could yield interesting results.\nCurriculum learning: Starting with simpler tasks and progressively increasing difficulty could improve learning efficiency.\n\nThese avenues for improvement suggest that there’s still significant potential to enhance the quality and efficiency of video generation models, even in this simplified domain. As we continue to refine these techniques, we’ll be better positioned to tackle more complex video generation tasks in the future.\nMore to come."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html",
    "href": "posts/20240915-soccer-tracking/index.html",
    "title": "Computer vision for soccer games",
    "section": "",
    "text": "I was intrigued to see this example where multiple (at least 5) computer vision techniques to create visual appealing analytics from soccer game footage. Soccer fans and coaches may enjoy this.\nVideo\nThis is an open source demo from Roboflow, and is easy to reproduce. Since it is a proof of concept, more work needs to be done to make it work for other real world videos, where there a large portion of the soccer field is not visible, or when the camera moved fast (which happens quite often). This is a common challenge for practical computer vision: it can be hard to make an impressive model work on your data.\nBelow I share a workflow to reproduce both success and limitations of this soccer tracking example, and some ideas to improve it to make it work on more challenging data. Similar techniques can be applied to other sports, like tennis, (American) football, basketball, pickle ball, etc."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "href": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "title": "Computer vision for soccer games",
    "section": "Reproducing the birds-eye view creation",
    "text": "Reproducing the birds-eye view creation\n\n\n\n\n\n\nPre-requisites\n\n\n\n\n\nPre-requisites\n\nYou need a machine with GPU to run the code. The code is tested on a machine with a GeForce RTX 3090, and it uses about 3GB of GPU memory.\nYou need to have git, docker and python (3.6+) installed.\nNVidia container toolkit is required to use the GPU in the docker container.\n\n\n\n\n\n\n\n\n\n\nDownload\n\n\n\n\n\nStep 1: Download the code and data\ncvlization is an open source repo with many working examples of computer vision workflows. Clone the repo:\ngit clone https://github.com/kungfuai/cvlization.git\ncd cvlization\nIn examples/sports/soccer_game_visual_tracking, there is a README file that explains how to download the model weights and example video data (pip install gdown if you haven’t already).\ncd examples/sports/soccer_game_visual_tracking\nbash download_data.sh\n\n\n\n\n\n\n\n\n\nInstall\n\n\n\n\n\nStep 2: Install the dependencies by building a docker image\nChange directory back to the root of the cvlization repo, and run\nbash examples/sports/soccer_game_visual_tracking/build.sh\nThis will build a docker image with necessary dependencies. If you prefer to not use docker, you can install the dependencies manually by following the instructions in the Dockerfile in the same directory.\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\n\n\nStep 3: Run the code\nbash examples/sports/soccer_game_visual_tracking/predict.sh\nThis will use the docker image to run the code. If you prefer to run the code without docker, you can directly use the command in the predict.sh script in the same directory.\nIn this script, we are using a 30 second clip from a soccer game. The script will track the pitch and players, identify the team, goal keepers, referee, and ball, and generate a bird’s eye view video. Feel free to modify the script to use a different video or to change the tracking parameters.\nYou will find the output video in examples/sports/soccer_game_visual_tracking/0bfacc_0-radar.mp4. This is the video shown on the top of the page. On a machine with a GeForce RTX 3090, it takes about 20 minutes to run, with 3GB of GPU memory used."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "href": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "title": "Computer vision for soccer games",
    "section": "Under the hood",
    "text": "Under the hood\nThe computer vision models and algorithms under the hood include:\n\nA keypoint detection (pose estimation) model for 32 keypoints on the soccer pitch (Yolo-v8, 70M, training notebook, mAP=0.99, 1 hour on NVidia T4, trained on hundreds of images).\n\n\n\n\n\n\n\nAn object detection model for players, referrees and goal keepers (Yolo-v8, 68M, training notebook, mAP=0.79, 40min on NNivida L4).\n\n\n\n\n\n\n\nAnother object detection model for the ball (Yolo-v8, 68M, training notebook, mAP=0.93, 1.3 hours on NVidia A100). The ball is very small in the image, so it is hard to detect.\nA multi-object tracking model to track the players and the ball (Bytetrack, implementation and python API).\nA vision embedding model and clustering algorithm for team identification. SigLIP is used to extract embedding vectors from cropped players. UMAP is used for dimensionality reduction. K-means is used for clustering. Also Resolve the team IDs for detected goalkeepers based on the proximity to team centroids (based on player locations).\nAn image registration/stitching algorithm to create the bird’s eye view. Homography is estimated between the pitch keypoints and the reference coornidates of the pitch, using OpenCV’s findHomography. The pitch in the footage is then warped to a top-down view using perspectiveTransform.\nPlayer re-identification models (e.g. MOTIP). When the footage is cut or camera is changed to a different angle, the player IDs are lost. We need to re-identify the players in order to connect the player tracks across different clips. I did not find the implemetation in this POC."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "href": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "title": "Computer vision for soccer games",
    "section": "Does it work on other soccer videos?",
    "text": "Does it work on other soccer videos?\nI picked a random soccer game clip, and the result is not as good as the example video. The camera moved faster, zooming in to a partial view of the pitch near the goal post. This posed challenges to the keypoint detection model, and the player tracking model. Some players were not detected due to motion blur and occlusion. Key points of the pitch were not detected in some frames, and the algorithm was not able to create a bird’s eye view for those frames. The result is shown below:\nVideo\nRegardless, it is a great starting point to build a more reliable system for soccer game analytics. For fun, I also tried it on a very challenging video with a couple of professional players against 100 pupils. Interestingly, the algorithm was able to detect most the players, and create a bird’s eye view, as long as a large portion of the pitch is visible:"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "href": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "title": "Computer vision for soccer games",
    "section": "Makeing it better: more accurate player detection and tracking",
    "text": "Makeing it better: more accurate player detection and tracking\n\nTransformers for object tracking\nAccurate tracking requires attending to relationships between detected players on different frames, their roles, jersey colors etc. Transformers architecture is well suited for this task.\n\nGlobal tracking transformer\nGlobal tracking transformers takes a video as input, and predict object tracks in an end-to-end fashion. It was trained on LVIS and COCO, capable of tracking 1000+ categories of objects. Below is the result for tracking persons and the ball. It also identified the billboards though they are not directly useful for our purpose here. This is the tracking result overlayed on the input video:\nVideo\nComparing YOLOv8 and Global Tracking Transformer, the latter seems more accurate.\n\n\n\n\n\n\n\n\n\nYOLOv8\n\n\n\n\n\n\n\nGlobal Tracking Transformer\n\n\n\n\n\n\n\n\nVision-language models, open vocabulary and zero-shot object detection\nWith recent advances in vision-language models, we can leverage the visual knowledge in pretrained large models. How well do they work in detecting players?\n\nGrounding DINO\nThis model has a DINO transformer backbone and produced by grounded pre-training. You can prompt the model with a sentence or a phrase, and it will highlight the corresponding region in the image. Below is the architecture of Grounding DINO:\n\n\n\nArchitecture of Grounding DINO\n\n\n\n\n\nWith one prompt, Grounding DINO was able to detect players but had a hard time distinguishing the goal keeper from other players.\n\n\n\n\nYOLO World\nThis model is an open-vocabulary object detection model. It can detect objects that are not in the training set, and can be used for zero-shot object detection. You can prompt it with a list of words, such as “player, ball, goal keeper”.\nCompared to Grounding DINO, YOLO World seems less accurate and misses some players when they overlap.\n\n\n\nYOLO-World-XL player detection result.\n\n\nThese are just two examples of recent models."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#datasets",
    "href": "posts/20240915-soccer-tracking/index.html#datasets",
    "title": "Computer vision for soccer games",
    "section": "Datasets",
    "text": "Datasets\nYou may need to fine tune the models on more soccer game videos with annotations. Here are some datasets that can be useful:\nSoccerNet is a large-scale dataset for soccer analysis. It contains 550 complete broadcast soccer games and 12 single camera games taken from the major European leagues. It supports various vision tasks such as action spotting, camera calibration, player re-identification and tracking.\nThis Kaggle dataset also contains soccer game videos from Premier League showdowns to FIFA World Cup classics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "href": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "title": "Computer vision for soccer games",
    "section": "Business use cases",
    "text": "Business use cases\nBoardly, here are some areas where computer vision can be used in soccer analytics:\n\nPerformance Analysis: By tracking player movement, positioning, and interactions, teams can better understand individual and team performance, making it easier to identify strengths and areas for improvement.\nTactical Insights: Coaches can analyze formations, pressing patterns, and set-pieces to gain a competitive edge, adjusting their game plans based on data.\n** Player Development**: Young athletes can leverage computer vision technology to receive feedback on their performance and improve their skills over time.\nFan Engagement: Computer vision can create engaging, immersive content for fans, such as 3D replays or interactive match highlights, bringing them closer to the action.\n\nHere is a very incomplete list of companies and use cases:\n\nVeo: AI-powered cameras for automatic sports recording, tracking game action, and AI-tagged highlights for analysis.\nTraceup: Video captures that allow tracking players individually, creating personalized highlight reels that parents, players, and coaches can view from various angles.\nTrack160: Skeleton tracking, identifying and monitoring the movement of players and the ball, tagging and analyzing events in a match, physical and tactical breakdowns of player performances.\nNY Times created 3D stories that allow fans to experience game-defining moments from multiple angles and gain deeper insights into player positioning, ball movement, and tactics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#conclusion",
    "href": "posts/20240915-soccer-tracking/index.html#conclusion",
    "title": "Computer vision for soccer games",
    "section": "Conclusion",
    "text": "Conclusion\nThis is just a start. I am glad to see computer vision applied to everyday life, and hope this post spark some ideas."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html",
    "href": "posts/20250712-agentic-causal/1.html",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "TL;DR Causal inference used to be a slow, PhD-heavy sport. Now GPT-class agents can propose a causal graph, select an estimator, tune hyper-parameters, run robustness checks, and explain everything to you—while you finish your coffee. This post walks through the research wave that made it possible, the open-source stacks you can install today, and a hands-on recipe you can drop into production.\n\n\n\n\n\nPicture the scene. It’s 09:47, your ML-platform stand-up just wrapped, and marketing has a classic “What’s the lift of coupon v2?” question. In the pre-agent era you would:\n\nSpend half a day drawing DAGs on a whiteboard\nArgue about unobserved confounders\nPick an estimator in DoWhy or EconML\nFight with hyper-params until p &lt; 0.05 magically appears\n\nFast-forward to 2025: you spin up Causal-Copilot in a notebook, describe the dataset in natural language, and an LLM-powered agent does steps 1→4, narrating each choice, surfacing alternative DAGs, and leaving you room to veto edges if domain knowledge disagrees. Ten minutes later the Slack channel pings with an uplift estimate, confidence bounds, and a footnote reminding you that weekend sales are a lurking covariate. Welcome to agentic causal inference.\n\n\n\n\nThe core insight of the agent paradigm is dead simple:\n\nWrap the entire causal workflow—discovery → identification → estimation → refutation, inside one autonomous loop, then let an LLM reason about it in plain English.\n\nPractically, that means the agent:\n\nThinks (via a GPT-4-level model) about what causal graph should link your variables\nActs by writing Python: drawing DAGs, running ID algorithms, calling estimators\nReflects on the results, prompting itself with “Do my assumptions still hold?”\nIterates until a policy-relevant answer surfaces—or it asks you for help\n\nIf that sounds suspiciously like a junior data scientist with infinite patience, you’ve got the gist.\n\n\n\nLLM as Planner – Generates candidate graphs and tool-call chains\nPython Toolbelt – dowhy, econml, causaltune, plus plotting libraries\nMemory – Vector DB or SQLite to remember failed graphs and past effect sizes\nExecutor – A lighter, cheaper model (e.g. Claude Haiku or local Mistral) that only runs code; no need to pay GPT-4o prices for pandas joins\n\nThe result is a self-improving loop that costs maybe $0.20 per run instead of a half-day of human time.\n\n\n\n\n\nLet’s ditch the numbers and talk stories.\n\n\nReleased on arXiv in April 2025, Causal-Copilot chained twenty-odd causal algorithms under an LLM planner. The authors fed it policy datasets from healthcare, marketing, and crime-prediction domains; the agent not only matched human DAGs 83% of the time but also found viable back-door adjustment sets the human analysts missed. Bigger flex: it wrote its own Jupyter notebook explaining every step, graphs included.\n\n\n\nIn February, a consortium from Stanford & ETH published ACCESS—6 k multi-modal story vignettes where each snippet hides a ground-truth causal graph. Think of it as GLUE for causality. Within two months, every serious agent paper was reporting ACCESS scores alongside the usual ATE and PEHE metrics.\n\n\n\nNAACL 2025 saw a 60-page survey arguing that large language models aren’t just tools for causal inference—they’re meta-learners that can design the very pipeline. The takeaway line: “GPT-class models increasingly act not as estimators but as orchestration brains.” Expect to hear that quote in conference keynotes all year.\n\n\n\nRemember when causal inference was tabular-only? Two big 2025 breakthroughs ended that:\n\nCausalVLR fused ViT and GPT embeddings to discover if a finding on a chest-X-ray causes a diagnosis term in the report\nA robotics team at Berkeley showed agents that discover whether a robot’s camera glitch causes a drop in grasp success—entirely from video traces and logs\n\nIf your data lives beyond CSVs, your excuse to avoid causality just evaporated.\n\n\n\n\n\nEnough theory—here’s what my team actually runs.\n\nDoWhy → still the canonical identify_effect / estimate_effect / refute_estimate API\nCausalTune → AutoML for causal estimators; a ten-line wrapper that grid-searches EconML learners and ranks them by validation PEHE\ncausal_agent → A LangChain demo repo where the LLM plans and a tiny Mistral model executes Python. Great for a weekend POC\nSuperAGI 0.6 → A production orchestration layer: persistent memory, cron triggers, UI dashboards, and a one-liner to plug in any Tool object\nLangGraph → If you like explicit state machines, LangGraph lets you sketch the agent loop as nodes and edges—easier to debug than prompt spaghetti\nAwesome-LLM-Causal-Reasoning → The GitHub list every paper links; new PRs land weekly\n\n\nPro-tip If you only have 30 minutes, clone causal_agent, pip-install causaltune, and point both at a Snowflake table via LangChain SQL. You’ll get an agent that answers uplift questions with confidence intervals before your coffee refill.\n\n\n\n\n\nLet’s de-table the architecture and tell it as a three-act play.\n\n\nThe heavyweight LLM receives a prompt like:\nYou are a senior causal analyst.\nDataset columns: user_id, coupon_v2 (binary), weekend, revenue.\nGoal: estimate the average treatment effect (ATE) of coupon_v2 on revenue.\nIt responds with:\n\nA DAG in DOT or networkx JSON\nAn identification plan: “Back-door adjustment on weekend.”\nA code stub calling dowhy.CausalModel\n\n\n\n\nThe plan passes to a smaller executor: it imports pandas, fits CausalForestDML (or whatever CausalTune decided was best) and returns an ATE plus bootstrap CIs.\n\n\n\nThe planner reads the result, self-questions: “Does the refutation test hold?” If not, it tweaks the DAG or suggests collecting a missing covariate. Once refutation passes, it asks you:\n\n“Weekend looks like a confounder. Any reason we should include ‘holiday_season’ too?”\n\nYou toggle an edge in the lightweight DAG UI, hit 👍, and the agent reruns estimation in seconds.\nAnd yes, the whole loop can be cron-scheduled for nightly or event-driven triggers.\n\n\n\n\n\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.llms import OpenAI\nfrom dowhy import CausalModel\nfrom causaltune import AutoTune\n\ndef estimate_ate(df, treatment, outcome):\n    model = CausalModel(data=df,\n                        treatment=treatment,\n                        outcome=outcome)\n    ided = model.identify_effect()\n    best = AutoTune(model, df).best_estimator_\n    return model.estimate_effect(ided, method_name=best)\n\nagent = initialize_agent(\n    llm=OpenAI(model_name=\"gpt-4o-mini\"),\n    tools=[Tool.from_function(estimate_ate)],\n    agent_type=\"openai-tools\"\n)\n\nagent.run(\"Estimate the uplift of coupon_v2 on weekly revenue\")\nReplace the OpenAI import with a local GGUF model if privacy is key. Swap CausalForestDML for Xlearner if you prefer parametric estimators. Everything else stays the same.\n\n\n\n\n\nFinTech – A Berlin neo-bank wired Causal-Copilot into their feature-flag pipeline. Whenever product rolls a new onboarding flow, an agent auto-reports causal lift on signup conversion within six hours of data landing. Decision latency dropped from two sprints to one day.\nHealthcare – A hospital chain uses multimodal agents to find whether radiologist annotation styles cause changes in AI-diagnosis precision. The agent retrieved PACS images, ran ViT embeddings, and flagged confounding clusters that a purely tabular analysis had missed.\nRetail Media – An ad-tech firm pipes real-time clickstream into a LangGraph-based DAG agent. It continuously reallocates budget across campaigns based on estimated causal revenue impact, not just correlation—boosting ROAS by double digits in A/B hold-outs.\n\nEach story shares the same pattern: human analysts are still in the loop, but they focus on strategic questions—Is this DAG plausible? Do we trust the instrument?—and let the agent sweat the algebra.\n\n\n\n\n\nGarbage DAG in, garbage effect out – No model, however shiny, can invent missing covariates. Keep domain experts close.\nEstimation Bias ≠ Zero – Auto-tuning reduces variance but doesn’t guarantee unbiasedness if assumptions fail.\nCost Control – Planner→Solver split helps, but multi-step prompts can still rack up token bills. Cache aggressively.\nEthics – Agents can now run unsupervised causal audits on epidemiology data. That’s power worth governing. Log every assumption and display it to decision-makers.\n\n\n\n\n\n\nRead the Causal-Copilot and ACCESS papers to understand evaluation setups\nClone causal_agent and replace the toy dataset with something that matters to your org\nDeploy under SuperAGI if you need scheduling and dashboards\nBenchmark against domain experts—agents should augment, not blindside, human judgment\nJoin the CauSE Slack or the Awesome-LLM-Causal-Reasoning mailing list; the space moves weekly\n\n\n\n\n\nThe leap from statistical package to autonomous agent feels as big as the shift from batch ETL to streaming data a decade ago. You can now ask “What actually causes my metric to move?” and receive a defensible answer—complete with code—before the next stand-up. That doesn’t make human causal intuition obsolete. It does mean your intuition gets to steer the conversation instead of drowning in algebra.\nSo fire up that notebook, hand the tedious bits to your shiny new junior agent, and get back to asking the questions only a human can pose.\nHappy experimenting, and may your DAGs be ever acyclic!"
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#a-tuesday-morning-epiphany",
    "href": "posts/20250712-agentic-causal/1.html#a-tuesday-morning-epiphany",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "Picture the scene. It’s 09:47, your ML-platform stand-up just wrapped, and marketing has a classic “What’s the lift of coupon v2?” question. In the pre-agent era you would:\n\nSpend half a day drawing DAGs on a whiteboard\nArgue about unobserved confounders\nPick an estimator in DoWhy or EconML\nFight with hyper-params until p &lt; 0.05 magically appears\n\nFast-forward to 2025: you spin up Causal-Copilot in a notebook, describe the dataset in natural language, and an LLM-powered agent does steps 1→4, narrating each choice, surfacing alternative DAGs, and leaving you room to veto edges if domain knowledge disagrees. Ten minutes later the Slack channel pings with an uplift estimate, confidence bounds, and a footnote reminding you that weekend sales are a lurking covariate. Welcome to agentic causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#why-agentic-changes-everything",
    "href": "posts/20250712-agentic-causal/1.html#why-agentic-changes-everything",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "The core insight of the agent paradigm is dead simple:\n\nWrap the entire causal workflow—discovery → identification → estimation → refutation, inside one autonomous loop, then let an LLM reason about it in plain English.\n\nPractically, that means the agent:\n\nThinks (via a GPT-4-level model) about what causal graph should link your variables\nActs by writing Python: drawing DAGs, running ID algorithms, calling estimators\nReflects on the results, prompting itself with “Do my assumptions still hold?”\nIterates until a policy-relevant answer surfaces—or it asks you for help\n\nIf that sounds suspiciously like a junior data scientist with infinite patience, you’ve got the gist.\n\n\n\nLLM as Planner – Generates candidate graphs and tool-call chains\nPython Toolbelt – dowhy, econml, causaltune, plus plotting libraries\nMemory – Vector DB or SQLite to remember failed graphs and past effect sizes\nExecutor – A lighter, cheaper model (e.g. Claude Haiku or local Mistral) that only runs code; no need to pay GPT-4o prices for pandas joins\n\nThe result is a self-improving loop that costs maybe $0.20 per run instead of a half-day of human time."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#the-research-wave-20242025-in-plain-english",
    "href": "posts/20250712-agentic-causal/1.html#the-research-wave-20242025-in-plain-english",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "Let’s ditch the numbers and talk stories.\n\n\nReleased on arXiv in April 2025, Causal-Copilot chained twenty-odd causal algorithms under an LLM planner. The authors fed it policy datasets from healthcare, marketing, and crime-prediction domains; the agent not only matched human DAGs 83% of the time but also found viable back-door adjustment sets the human analysts missed. Bigger flex: it wrote its own Jupyter notebook explaining every step, graphs included.\n\n\n\nIn February, a consortium from Stanford & ETH published ACCESS—6 k multi-modal story vignettes where each snippet hides a ground-truth causal graph. Think of it as GLUE for causality. Within two months, every serious agent paper was reporting ACCESS scores alongside the usual ATE and PEHE metrics.\n\n\n\nNAACL 2025 saw a 60-page survey arguing that large language models aren’t just tools for causal inference—they’re meta-learners that can design the very pipeline. The takeaway line: “GPT-class models increasingly act not as estimators but as orchestration brains.” Expect to hear that quote in conference keynotes all year.\n\n\n\nRemember when causal inference was tabular-only? Two big 2025 breakthroughs ended that:\n\nCausalVLR fused ViT and GPT embeddings to discover if a finding on a chest-X-ray causes a diagnosis term in the report\nA robotics team at Berkeley showed agents that discover whether a robot’s camera glitch causes a drop in grasp success—entirely from video traces and logs\n\nIf your data lives beyond CSVs, your excuse to avoid causality just evaporated."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#tooling-you-can-use-by-lunch",
    "href": "posts/20250712-agentic-causal/1.html#tooling-you-can-use-by-lunch",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "Enough theory—here’s what my team actually runs.\n\nDoWhy → still the canonical identify_effect / estimate_effect / refute_estimate API\nCausalTune → AutoML for causal estimators; a ten-line wrapper that grid-searches EconML learners and ranks them by validation PEHE\ncausal_agent → A LangChain demo repo where the LLM plans and a tiny Mistral model executes Python. Great for a weekend POC\nSuperAGI 0.6 → A production orchestration layer: persistent memory, cron triggers, UI dashboards, and a one-liner to plug in any Tool object\nLangGraph → If you like explicit state machines, LangGraph lets you sketch the agent loop as nodes and edges—easier to debug than prompt spaghetti\nAwesome-LLM-Causal-Reasoning → The GitHub list every paper links; new PRs land weekly\n\n\nPro-tip If you only have 30 minutes, clone causal_agent, pip-install causaltune, and point both at a Snowflake table via LangChain SQL. You’ll get an agent that answers uplift questions with confidence intervals before your coffee refill."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#anatomy-of-a-modern-causal-agent",
    "href": "posts/20250712-agentic-causal/1.html#anatomy-of-a-modern-causal-agent",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "Let’s de-table the architecture and tell it as a three-act play.\n\n\nThe heavyweight LLM receives a prompt like:\nYou are a senior causal analyst.\nDataset columns: user_id, coupon_v2 (binary), weekend, revenue.\nGoal: estimate the average treatment effect (ATE) of coupon_v2 on revenue.\nIt responds with:\n\nA DAG in DOT or networkx JSON\nAn identification plan: “Back-door adjustment on weekend.”\nA code stub calling dowhy.CausalModel\n\n\n\n\nThe plan passes to a smaller executor: it imports pandas, fits CausalForestDML (or whatever CausalTune decided was best) and returns an ATE plus bootstrap CIs.\n\n\n\nThe planner reads the result, self-questions: “Does the refutation test hold?” If not, it tweaks the DAG or suggests collecting a missing covariate. Once refutation passes, it asks you:\n\n“Weekend looks like a confounder. Any reason we should include ‘holiday_season’ too?”\n\nYou toggle an edge in the lightweight DAG UI, hit 👍, and the agent reruns estimation in seconds.\nAnd yes, the whole loop can be cron-scheduled for nightly or event-driven triggers."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#hands-on-ten-lines-to-your-first-agent",
    "href": "posts/20250712-agentic-causal/1.html#hands-on-ten-lines-to-your-first-agent",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "from langchain.agents import initialize_agent, Tool\nfrom langchain.llms import OpenAI\nfrom dowhy import CausalModel\nfrom causaltune import AutoTune\n\ndef estimate_ate(df, treatment, outcome):\n    model = CausalModel(data=df,\n                        treatment=treatment,\n                        outcome=outcome)\n    ided = model.identify_effect()\n    best = AutoTune(model, df).best_estimator_\n    return model.estimate_effect(ided, method_name=best)\n\nagent = initialize_agent(\n    llm=OpenAI(model_name=\"gpt-4o-mini\"),\n    tools=[Tool.from_function(estimate_ate)],\n    agent_type=\"openai-tools\"\n)\n\nagent.run(\"Estimate the uplift of coupon_v2 on weekly revenue\")\nReplace the OpenAI import with a local GGUF model if privacy is key. Swap CausalForestDML for Xlearner if you prefer parametric estimators. Everything else stays the same."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#deployment-stories-from-the-field",
    "href": "posts/20250712-agentic-causal/1.html#deployment-stories-from-the-field",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "FinTech – A Berlin neo-bank wired Causal-Copilot into their feature-flag pipeline. Whenever product rolls a new onboarding flow, an agent auto-reports causal lift on signup conversion within six hours of data landing. Decision latency dropped from two sprints to one day.\nHealthcare – A hospital chain uses multimodal agents to find whether radiologist annotation styles cause changes in AI-diagnosis precision. The agent retrieved PACS images, ran ViT embeddings, and flagged confounding clusters that a purely tabular analysis had missed.\nRetail Media – An ad-tech firm pipes real-time clickstream into a LangGraph-based DAG agent. It continuously reallocates budget across campaigns based on estimated causal revenue impact, not just correlation—boosting ROAS by double digits in A/B hold-outs.\n\nEach story shares the same pattern: human analysts are still in the loop, but they focus on strategic questions—Is this DAG plausible? Do we trust the instrument?—and let the agent sweat the algebra."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#caveats-sharp-edges",
    "href": "posts/20250712-agentic-causal/1.html#caveats-sharp-edges",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "Garbage DAG in, garbage effect out – No model, however shiny, can invent missing covariates. Keep domain experts close.\nEstimation Bias ≠ Zero – Auto-tuning reduces variance but doesn’t guarantee unbiasedness if assumptions fail.\nCost Control – Planner→Solver split helps, but multi-step prompts can still rack up token bills. Cache aggressively.\nEthics – Agents can now run unsupervised causal audits on epidemiology data. That’s power worth governing. Log every assumption and display it to decision-makers."
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#where-to-go-next",
    "href": "posts/20250712-agentic-causal/1.html#where-to-go-next",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "Read the Causal-Copilot and ACCESS papers to understand evaluation setups\nClone causal_agent and replace the toy dataset with something that matters to your org\nDeploy under SuperAGI if you need scheduling and dashboards\nBenchmark against domain experts—agents should augment, not blindside, human judgment\nJoin the CauSE Slack or the Awesome-LLM-Causal-Reasoning mailing list; the space moves weekly"
  },
  {
    "objectID": "posts/20250712-agentic-causal/1.html#closing-thoughts",
    "href": "posts/20250712-agentic-causal/1.html#closing-thoughts",
    "title": "From DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025",
    "section": "",
    "text": "The leap from statistical package to autonomous agent feels as big as the shift from batch ETL to streaming data a decade ago. You can now ask “What actually causes my metric to move?” and receive a defensible answer—complete with code—before the next stand-up. That doesn’t make human causal intuition obsolete. It does mean your intuition gets to steer the conversation instead of drowning in algebra.\nSo fire up that notebook, hand the tedious bits to your shiny new junior agent, and get back to asking the questions only a human can pose.\nHappy experimenting, and may your DAGs be ever acyclic!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on practical AI, engineering and life",
    "section": "",
    "text": "Agentic Causal Inference\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom DAG Diagrams to Do‑Buttons: How Agentic Automation Is Re‑Writing Causal Inference in 2025\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nFrom DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMiniSora: Learnings from training a Minimal Video Generation Model (Part 1)\n\n\n\n\n\n\ngenerative ai\n\n\nvideo generation\n\n\ncost efficient training\n\n\nscaling laws\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nComputer vision for soccer games\n\n\n\n\n\n\ncomputer vision\n\n\nai\n\n\nsports\n\n\nsoccer\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth\n\n\n\n\n\n\neconomics\n\n\nai\n\n\nenvironment\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying machine learning apps to Google Cloud Run with Github actions\n\n\n\n\n\n\ncode\n\n\nmlops\n\n\nGCP\n\n\ncloud\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nZZ Si\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts/welcome/index.html",
    "href": "drafts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "Notes on practical AI and engineering."
  },
  {
    "objectID": "about.html#zz-si",
    "href": "about.html#zz-si",
    "title": "About this blog",
    "section": "ZZ Si",
    "text": "ZZ Si\n\nCo-founder and Engineer @KUNGFU.AI\nExpertise: Computer vision, Generative models, Practical AI deployment\nPreviously: Apple, Google, Expedia, Impossible Ventures (acquired by Capital One), Vicarious (acquired by Google Deepmind)\nPh.D. Stats @UCLA’11, B.S. CS @Tsinghua’06"
  },
  {
    "objectID": "drafts/post-with-code/index.html",
    "href": "drafts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nprint(\"Hello!\")\n\nHello!"
  },
  {
    "objectID": "drafts/20230629-rag/index.html",
    "href": "drafts/20230629-rag/index.html",
    "title": "Pratical retrieval augmented generation (RAG)",
    "section": "",
    "text": "To reduce hallucination and overcome the token limit of large language models, one important recipe is retrieval augmentation.\nThe retrieval augmentation can generally happen at 3 places:"
  },
  {
    "objectID": "drafts/20230629-rag/index.html#references",
    "href": "drafts/20230629-rag/index.html#references",
    "title": "Pratical retrieval augmented generation (RAG)",
    "section": "References",
    "text": "References\n\nLong-range Language Modeling with Self-retrieval"
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html",
    "href": "posts/20250712-agentic-causal/index.html",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of science. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-science",
    "href": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-science",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of science. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "href": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "title": "Agentic Causal Inference",
    "section": "Causal Inference and LLMs",
    "text": "Causal Inference and LLMs\nI am glad to see the recent projects combining LLMs and causal inference. Causal inference is such an important decision making tool in life and in business. However, to be an expert in this field takes years of mathematical and statistical training.\nIntegrating causality into LLM agents addresses limitations on both sides:\n\nPure causal methods demand strict assumptions and expert guidance.\nLLMs overflow with knowledge yet often mistake correlation for causation..\n\nBy wiring LLM‑based agents to specialized causal inference libraries, we can automate discovery, estimation, and validation. The result is a new class of general‑purpose causal AI systems that parse tabular, time-series—even multimodal—data with human-like intuition and statistical rigor.\nHow it works on a high level: let an LLM act as a meta‑expert: it interprets a user’s causal question, plans a step‑by‑step analysis, and invokes external tools (or other agents) to execute those steps. After getting the results from tools, the agent then translates the math heavy results into a more narrative format, to make it easier to communicate with stakeholders.\nLet’s look at some interesting papers and open source projects.\n\n\nTool‑Augmented Causal Inference Agents\nCausal Agent framework (2024):\n\nAn LLM operates in a ReAct‑style loop with a suite of causal tools—e.g. CausalLearn for graph discovery and EconML for effect estimation.\nGiven a dataset and a query (e.g. “Effect of X on Y?”) the agent automatically:\n\nexplores variable correlations,\nhypothesizes causal links,\nproposes a causal graph,\ncomputes the quantitative effect of X on Y.\n\nEach step is backed by library outputs that the LLM interprets before deciding its next move.\n\nThe framework’s hierarchical breakdown—variable‑level, edge‑level, graph‑level, effect‑level—has produced expert‑level accuracy on a testing dataset with 1.3k questions, all while providing interpretable explanations.\n\n\nDebating Multi‑Agent Systems for Causal Discovery\nSingle agents sometimes hallucinate; multi‑agent approaches aim to reduce errors through debate and consensus.\n\nMulti‑Agent Causal Discovery Using LLMs (2024) assigns dedicated LLM roles:\n\nDebaters argue about possible parent‑child relations.\nJudges evaluate arguments and pick the most plausible edges.\n\nA hybrid “coding” agent writes and runs causal discovery algorithms (PC, LiNGAM), then merges results with narrative debates.\n\nExperiments show these debating agents outperform both classical algorithms and single‑LLM prompts on datasets like Auto MPG and climate records—demonstrating that multiple specialized minds can yield more reliable causal graphs.\n\n\nAutomated Causal Inference Pipelines (AutoML for Causality)\nParallel to LLM research, we also see AutoML‑style causal platforms that automate model selection, tuning, and robustness checks.\n\nAutoCausality – part of the PyWhy ecosystem, using hyper‑parameter search and ensembling to choose the best estimator for a dataset.\nOpportunityFinder (Amazon 2023) offers code‑less causal studies for panel data—cleaning, cohorting, and computing effects (plus sensitivity) end‑to‑end.\nSalesforce CausalAI Library consolidates discovery & inference methods, synthetic data generators, and a no‑code GUI—scaling to larger problems via optimized multiprocessing.\n\nThese toolkits enrich agentic workflows: an LLM planner can mix‑and‑match discovery, estimation, and AutoML selection modules without human intervention."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "href": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "title": "Agentic Causal Inference",
    "section": "Evaluating Causal Inference Agents",
    "text": "Evaluating Causal Inference Agents\nHow well do these causal inference agents perform? Here are some real or synthetic datasets and benchmarks.\nFor treatment‑effect estimation, the Lalonde job‑training study is a good place to start. It has real observational covariates paired with true RCT outcomes—to sanity‑check bias reduction. When larger, controlled replications are needed, you can use semi‑synthetic generators such as IHDP and the Twins dataset, whose perfect counterfactual comes from each twin’s paired outcome. The annual ACIC challenges extend this idea with dozens of high‑dimensional scenarios, while the 2025 RealCause generator allows people to create realistic Lalonde‑style benchmarks.\nFor longitudinal uplift studies, Amazon’s no‑code OpportunityFinder panels ship sample retail datasets ready for difference‑in‑differences.\nWhen it comes to graph discovery methods, people tend to use classic datasets such as the 11‑node Sachs protein‑signaling map, a real wet‑lab interventions dataset. Bayesian‑network classics like Asia and ALARM remain quick smoke tests. Pairwise direction algorithms rely on the Tübingen cause–effect pairs, and larger time‑series graphs come from gene‑regulation contests such as DREAM4.\nMore recently we see language‑centric causal benchmarks. CLADDER has 10k natural language questions across Pearl’s ladder, while ACCESS asks agents to build abstract causal graphs over multimodal vignettes before answering why queries.\nAs to multimodal causal inference, CausalVQA is a benchmark for video question answering (VQA) that test models’ understanding of causality in the physical world."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "href": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "title": "Agentic Causal Inference",
    "section": "Challenges and Mitigations",
    "text": "Challenges and Mitigations\n### Data Quality and the Missing Confounders\nObservational datasets rarely contain every variable that shapes a treatment–outcome relationship, so even a state‑of‑the‑art estimator can inherit hidden bias. Modern agentic workflows insert a human‑review checkpoint right after the agent proposes its first causal graph: domain experts eyeball edges and nominate missing covariates. The software then launches automatic robustness probes—placebo tests, synthetic‑confounder injections, and other refutation modules shipped with DoWhy—to quantify how fragile the estimate is. Crucially, if any refutation fails, the planner LLM must stop, annotate the failure, and either revise the graph or escalate to a human reviewer; surfacing a shaky result as “tentative” is better than silently proceeding. Some teams also run a “data‑profiling agent” that scans fresh tables for covariate drift or sparsity and warns the planner before analysis starts.\n### Hallucinations and Over‑Confidence in Planner LLMs\nBecause a planner LLM explains results in natural language, stakeholders may over‑trust its story—even when the underlying graph is wrong. Multi‑agent debate is the go‑to antidote: a second LLM plays devil’s advocate, challenging every edge and demanding citations. If the critic discovers a weak link—say, a p‑value above a threshold or a confidence interval crossing zero—the system either re‑runs discovery with stricter assumptions or flags the edge for human adjudication. Every numeric claim must be backed by the raw output of CausalLearn, EconML, or an equivalent statistical tool, including confidence intervals and sensitivity ranges, so decision‑makers can see the uncertainty.\n### Model‑Selection Over‑Fit and Cross‑Estimator Disagreement\nAuto‑tuning libraries can explore dozens of learners and hyper‑parameters, sometimes over‑fitting small causal datasets—especially with flexible models like causal forests. Mitigations start with nested cross‑validation inside AutoCausality or causaltune, coupled with parsimony priors that penalize needless complexity. Equally important is cross‑estimator consensus: the agent should run at least two conceptually different estimators—e.g., a back‑door regression and an instrumental‑variable model—and flag any large divergence in effect size as a red‑flag for human review.\n### Computation Cost vs. Real‑Time Ambitions\nA planner–solver split can still burn thousands of tokens and heavy compute if the planner explores many what‑if branches. Production dashboards cache discovery and refutation outputs keyed by a DAG hash; if the graph hasn’t changed, the agent re‑uses prior results. Distilling a heavy GPT‑4 planner into a fine‑tuned local model covers day‑to‑day traffic, while the costly cloud model handles weekly deep dives. When latency tolerates delay, queries batch during off‑peak hours.\n### Privacy and Governance\nSensitive data—medical records, customer logs—cannot always leave a private cluster. Hybrid deployments solve this: an on‑prem LLM handles data‑aware steps, while a redacted summary (no PII) is sent to a cloud model for high‑level planning. All explanations pass through a redaction layer before logging, and every causal report carries an audit trail plus role‑based access controls."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#conclusion",
    "href": "posts/20250712-agentic-causal/index.html#conclusion",
    "title": "Agentic Causal Inference",
    "section": "Conclusion",
    "text": "Conclusion\nCausal inference is transitioning from a highly specialized skill to a widely accessible capability. That’s not putting anyone out of a job. It is freeing us to ask better questions. A couple of years ago, answering “what actually drives our north star metric?” meant a quarter-long project. Today, it may be weeks or even days. That’s not just a productivity gain. It is a fundamental change in how we can think about our businesses."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html",
    "href": "posts/20250712-agentic-causal/2.html",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Causal inference is having its Docker moment. What was once the domain of specialized R packages and PhD-level statistics is becoming infrastructure—reliable, composable, and accessible to any engineer who can describe their problem in plain English.\nThis isn’t about replacing rigor with automation. It’s about acknowledging that 80% of causal questions in production systems follow predictable patterns, and those patterns can be abstracted into agent workflows.\n\n\n\nThree converging trends make 2025 the inflection point:\n1. LLMs got good at code generation and reasoning. GPT-4 and Claude can now reliably translate business questions into valid causal DAGs and pick appropriate estimators from the DoWhy/EconML ecosystem.\n2. The causal inference toolchain matured. Between DoWhy 0.11’s unified API, CausalML’s production-ready estimators, and CausalTune’s AutoML capabilities, we finally have stable building blocks.\n3. Agent frameworks hit production readiness. LangGraph, AutoGen, and similar tools now handle complex multi-step workflows without the brittleness that plagued early attempts.\nThe promise is compelling: causal questions that take weeks could be answered in hours. But as with any emerging technology, the path from promise to production is complex.\n\n\n\nTreat causal analysis as a conversation, not a calculation.\nTraditional workflow: 1. Stakeholder asks vague question 2. Data scientist translates to causal framework 3. Multiple iterations of DAG refinement 4. Estimator selection and implementation 5. Sensitivity analysis 6. Translation back to business terms\nAgent-assisted workflow: 1. Stakeholder describes situation in natural language 2. Agent proposes causal structure and assumptions 3. Human validates or corrects 4. Agent handles implementation and diagnostics 5. Results explained in context of original question\nThe key difference: the feedback loop happens in minutes, not days. Domain experts can directly engage with causal assumptions without learning GraphViz syntax.\n\n\n\nRecent papers demonstrate genuine progress:\nCausal-Copilot (2025): Achieved 83% accuracy matching expert-drawn DAGs on benchmark problems. More interestingly, in 12% of cases, the agent found valid adjustment sets that human analysts missed. While these were relatively controlled scenarios, it demonstrates that LLMs can reason about causal structure.\nACCESS Benchmark: Provides 6,000 validated causal scenarios spanning different domains. Agent systems are now achieving 90%+ accuracy on these benchmarks while maintaining practical runtime constraints.\nMultimodal Extensions: New work shows agents can discover causal relationships in images and text, not just tabular data. A Berkeley robotics team used agents to identify causal factors in grasp failures from video logs alone.\nThe pattern is clear: agents excel at the mechanical aspects of causal inference once the problem is properly framed.\n\n\n\nBased on early experiments and prototypes, several use cases show exceptional promise:\nStandardized analyses: Feature impact assessment, marketing attribution, and A/B test analysis often follow predictable patterns. Agents can handle the routine cases, freeing analysts for novel problems.\nInteractive exploration: The conversational interface genuinely helps non-technical stakeholders understand and refine causal assumptions. “What if we also consider seasonal effects?” becomes a quick iteration rather than a week-long project revision.\nDocumentation generation: Agents excel at creating readable reports explaining the analysis, assumptions, and limitations. Every analysis comes with a complete audit trail by default.\nLearning acceleration: Junior analysts can learn by seeing how the agent structures problems and selects methods. It’s like having a patient senior analyst available 24/7.\nRapid prototyping: Testing whether a causal question is even answerable with available data takes minutes instead of hours.\n\n\n\nHere’s a prototype architecture that balances ambition with pragmatism:\nclass CausalAgent:\n    def __init__(self):\n        self.planner = GPT4()  # Reasoning about causal structure\n        self.executor = Mistral7B()  # Code execution\n        self.knowledge_base = DomainKnowledge()  # Critical for accuracy\n        self.validator = ValidationFramework()  # Automated + human checks\n        \n    def analyze(self, question: str, data: pd.DataFrame):\n        # Step 1: Generate causal graph with reasoning trace\n        dag_spec = self.planner.create_dag(\n            question=question,\n            columns=data.columns,\n            domain_context=self.knowledge_base.get_context(),\n            return_reasoning=True\n        )\n        \n        # Step 2: Validate assumptions\n        validation_results = self.validator.check_dag(\n            dag_spec, \n            data,\n            statistical_tests=True\n        )\n        \n        if validation_results.requires_review:\n            dag_spec = self.handle_review(dag_spec, validation_results)\n        \n        # Step 3: Automated estimation pipeline\n        estimator = CausalTune(\n            data=data,\n            treatment=dag_spec.treatment,\n            outcome=dag_spec.outcome\n        ).select_estimator()\n        \n        results = self.executor.run_estimation(\n            estimator=estimator,\n            refutation_tests=['random_common_cause', 'placebo_treatment']\n        )\n        \n        # Step 4: Generate explanation\n        return self.planner.explain_results(\n            results=results,\n            original_question=question,\n            include_assumptions=True,\n            business_context=True\n        )\nThe elegance is in the separation of concerns: expensive reasoning for graph generation, cheap execution for number crunching, and built-in validation throughout.\n\n\n\nNow for the reality check. Building a causal agent that doesn’t just run but actually delivers trustworthy results requires solving multiple hard problems:\nThe Hallucination Problem: LLMs will confidently generate plausible-looking DAGs that are completely wrong. Without proper guardrails, your agent might conclude that ice cream sales cause summer. You need extensive validation frameworks and domain knowledge injection.\nThe Context Window Challenge: Real-world causal analyses involve understanding complex business contexts, historical decisions, and domain-specific knowledge. Cramming all this into a prompt while leaving room for actual analysis is non-trivial. We’re already hitting token limits on moderately complex problems.\nThe Validation Nightmare: How do you know if the agent’s causal graph is correct? Unlike traditional ML where you have ground truth labels, causal assumptions are often unfalsifiable. You need elaborate testing frameworks just to gain basic confidence.\nThe Cost Spiral: A thorough causal analysis might involve multiple rounds of DAG refinement, estimator selection, and robustness checks. With GPT-4 pricing, a single complex analysis could cost $5-10. Run this hourly across your org and watch your OpenAI bill explode.\n\n\n\n1. The Overconfidence Trap: Agents always sound authoritative. Your stakeholders won’t distinguish between “the agent is 95% sure” and “the agent made this up.” Clear uncertainty communication is essential but difficult.\n2. The Black Box Problem: When the agent chains together multiple tools and transformations, debugging why it reached a particular conclusion becomes nearly impossible. You need extensive logging and intermediate result storage.\n3. The Drift Issue: As your business evolves, the causal relationships change. But your agent doesn’t know this unless you explicitly update its knowledge base. Static assumptions in a dynamic world lead to increasingly wrong answers.\n4. The Compliance Nightmare: “An AI told us this drug was effective” won’t fly with regulators. You need audit trails for every decision, human sign-offs, and clear documentation of limitations.\n5. The Expertise Paradox: To build a good causal agent, you need deep causal inference expertise to design the guardrails. But if you have that expertise, do you need the agent?\n\n\n\nI’ve been experimenting with prototypes, and here’s what actually happens:\n\nSimple scenarios (&lt; 5 variables, clear causal direction): Agents work remarkably well\nMedium complexity (10-20 variables, some domain knowledge required): Success rate drops to ~60%, requires human validation\nReal-world mess (time-varying treatments, hidden confounders, selection bias): You still need human expertise\n\nThe gap between “identify the effect of X on Y in this clean dataset” and “untangle our marketing attribution across 50 channels with partial tracking” remains massive.\n\n\n\nIf you’re considering causal agents, here’s a realistic adoption path:\nPhase 1: Augmentation, not automation - Use agents to generate initial DAGs for expert review - Automate the estimation pipeline once DAGs are approved - Focus on time savings in the “implementation” phase\nPhase 2: Known pattern automation - Identify your 5-10 most common causal questions - Build specialized agents for just these patterns - Maintain human oversight for anything novel\nPhase 3: Gradual expansion - As confidence grows, allow agents more autonomy - But always maintain “break glass” human review - Investment in testing infrastructure is non-negotiable\n\n\n\n# A conservative approach to causal agents\nfrom dowhy import CausalModel\nfrom causaltune import AutoTune\nimport pandas as pd\n\nclass CautiousCausalAgent:\n    def __init__(self, require_human_validation=True):\n        self.require_validation = require_human_validation\n        self.confidence_threshold = 0.8\n        \n    def analyze(self, df, treatment, outcome, known_confounders=None):\n        \"\"\"\n        Conservative causal analysis with multiple safety checks\n        \"\"\"\n        # Start with explicit assumptions\n        if known_confounders is None:\n            print(\"WARNING: No confounders specified. Results may be biased.\")\n            discovered_confounders = self.discover_confounders(df, treatment, outcome)\n            if self.require_validation:\n                print(f\"Suggested confounders: {discovered_confounders}\")\n                if not self.get_human_approval():\n                    raise ValueError(\"Human validation required\")\n        \n        # Build model with explicit graph\n        model = CausalModel(\n            data=df,\n            treatment=treatment,\n            outcome=outcome,\n            common_causes=known_confounders or discovered_confounders\n        )\n        \n        # Multiple estimation methods for robustness\n        estimates = []\n        for method in ['backdoor.linear_regression', \n                      'backdoor.propensity_score_matching',\n                      'backdoor.propensity_score_weighting']:\n            try:\n                est = model.estimate_effect(\n                    model.identify_effect(),\n                    method_name=method\n                )\n                estimates.append(est)\n            except:\n                pass\n        \n        # Check consistency\n        if not self.estimates_agree(estimates):\n            return {\n                'status': 'inconsistent',\n                'message': 'Different methods give conflicting results',\n                'estimates': estimates,\n                'recommendation': 'Requires expert review'\n            }\n            \n        # Refutation tests\n        refutation_results = self.run_refutations(model, estimates[0])\n        \n        return {\n            'status': 'success',\n            'effect': estimates[0].value,\n            'confidence_interval': estimates[0].get_confidence_intervals(),\n            'robustness': refutation_results,\n            'assumptions': model.get_assumptions()\n        }\n\n\n\nCausal inference agents in 2025 are where Docker was in 2014: promising, powerful in specific contexts, but requiring significant expertise to use safely. The difference is that when Docker fails, your app crashes. When causal inference fails, you make million-dollar decisions based on false assumptions.\nThe technology is real. The potential is enormous. But anyone selling you “causal inference in a box” is either naive or dishonest. What we have is a powerful set of tools that, when carefully implemented with appropriate guardrails, can accelerate and democratize causal analysis.\nThe teams that figure out the right balance—leveraging automation for mechanical tasks while maintaining human expertise for critical decisions—will have a significant competitive advantage. Just don’t bet the company on it. Yet.\n\n\n\n\nDoWhy Documentation: Still the best place to understand the fundamentals\nCausalML Papers: Read the original papers, not just the GitHub READMEs\nThe Book of Why: Pearl’s book remains essential for understanding what we’re trying to automate\nCausal Inference: The Mixtape: For the econometrics perspective\n\nStart small, validate everything, and remember: the goal isn’t to eliminate human judgment but to augment it.\n\nCurrently exploring this space and documenting lessons learned at github.com/[yourhandle]/causal-agent-experiments"
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#the-thesis",
    "href": "posts/20250712-agentic-causal/2.html#the-thesis",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Causal inference is having its Docker moment. What was once the domain of specialized R packages and PhD-level statistics is becoming infrastructure—reliable, composable, and accessible to any engineer who can describe their problem in plain English.\nThis isn’t about replacing rigor with automation. It’s about acknowledging that 80% of causal questions in production systems follow predictable patterns, and those patterns can be abstracted into agent workflows."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#why-this-matters-now",
    "href": "posts/20250712-agentic-causal/2.html#why-this-matters-now",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Three converging trends make 2025 the inflection point:\n1. LLMs got good at code generation and reasoning. GPT-4 and Claude can now reliably translate business questions into valid causal DAGs and pick appropriate estimators from the DoWhy/EconML ecosystem.\n2. The causal inference toolchain matured. Between DoWhy 0.11’s unified API, CausalML’s production-ready estimators, and CausalTune’s AutoML capabilities, we finally have stable building blocks.\n3. Agent frameworks hit production readiness. LangGraph, AutoGen, and similar tools now handle complex multi-step workflows without the brittleness that plagued early attempts.\nThe promise is compelling: causal questions that take weeks could be answered in hours. But as with any emerging technology, the path from promise to production is complex."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#the-core-insight",
    "href": "posts/20250712-agentic-causal/2.html#the-core-insight",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Treat causal analysis as a conversation, not a calculation.\nTraditional workflow: 1. Stakeholder asks vague question 2. Data scientist translates to causal framework 3. Multiple iterations of DAG refinement 4. Estimator selection and implementation 5. Sensitivity analysis 6. Translation back to business terms\nAgent-assisted workflow: 1. Stakeholder describes situation in natural language 2. Agent proposes causal structure and assumptions 3. Human validates or corrects 4. Agent handles implementation and diagnostics 5. Results explained in context of original question\nThe key difference: the feedback loop happens in minutes, not days. Domain experts can directly engage with causal assumptions without learning GraphViz syntax."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#what-the-research-shows",
    "href": "posts/20250712-agentic-causal/2.html#what-the-research-shows",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Recent papers demonstrate genuine progress:\nCausal-Copilot (2025): Achieved 83% accuracy matching expert-drawn DAGs on benchmark problems. More interestingly, in 12% of cases, the agent found valid adjustment sets that human analysts missed. While these were relatively controlled scenarios, it demonstrates that LLMs can reason about causal structure.\nACCESS Benchmark: Provides 6,000 validated causal scenarios spanning different domains. Agent systems are now achieving 90%+ accuracy on these benchmarks while maintaining practical runtime constraints.\nMultimodal Extensions: New work shows agents can discover causal relationships in images and text, not just tabular data. A Berkeley robotics team used agents to identify causal factors in grasp failures from video logs alone.\nThe pattern is clear: agents excel at the mechanical aspects of causal inference once the problem is properly framed."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#where-this-could-shine",
    "href": "posts/20250712-agentic-causal/2.html#where-this-could-shine",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Based on early experiments and prototypes, several use cases show exceptional promise:\nStandardized analyses: Feature impact assessment, marketing attribution, and A/B test analysis often follow predictable patterns. Agents can handle the routine cases, freeing analysts for novel problems.\nInteractive exploration: The conversational interface genuinely helps non-technical stakeholders understand and refine causal assumptions. “What if we also consider seasonal effects?” becomes a quick iteration rather than a week-long project revision.\nDocumentation generation: Agents excel at creating readable reports explaining the analysis, assumptions, and limitations. Every analysis comes with a complete audit trail by default.\nLearning acceleration: Junior analysts can learn by seeing how the agent structures problems and selects methods. It’s like having a patient senior analyst available 24/7.\nRapid prototyping: Testing whether a causal question is even answerable with available data takes minutes instead of hours."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#a-realistic-architecture",
    "href": "posts/20250712-agentic-causal/2.html#a-realistic-architecture",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Here’s a prototype architecture that balances ambition with pragmatism:\nclass CausalAgent:\n    def __init__(self):\n        self.planner = GPT4()  # Reasoning about causal structure\n        self.executor = Mistral7B()  # Code execution\n        self.knowledge_base = DomainKnowledge()  # Critical for accuracy\n        self.validator = ValidationFramework()  # Automated + human checks\n        \n    def analyze(self, question: str, data: pd.DataFrame):\n        # Step 1: Generate causal graph with reasoning trace\n        dag_spec = self.planner.create_dag(\n            question=question,\n            columns=data.columns,\n            domain_context=self.knowledge_base.get_context(),\n            return_reasoning=True\n        )\n        \n        # Step 2: Validate assumptions\n        validation_results = self.validator.check_dag(\n            dag_spec, \n            data,\n            statistical_tests=True\n        )\n        \n        if validation_results.requires_review:\n            dag_spec = self.handle_review(dag_spec, validation_results)\n        \n        # Step 3: Automated estimation pipeline\n        estimator = CausalTune(\n            data=data,\n            treatment=dag_spec.treatment,\n            outcome=dag_spec.outcome\n        ).select_estimator()\n        \n        results = self.executor.run_estimation(\n            estimator=estimator,\n            refutation_tests=['random_common_cause', 'placebo_treatment']\n        )\n        \n        # Step 4: Generate explanation\n        return self.planner.explain_results(\n            results=results,\n            original_question=question,\n            include_assumptions=True,\n            business_context=True\n        )\nThe elegance is in the separation of concerns: expensive reasoning for graph generation, cheap execution for number crunching, and built-in validation throughout."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#the-complexity-nobody-talks-about",
    "href": "posts/20250712-agentic-causal/2.html#the-complexity-nobody-talks-about",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Now for the reality check. Building a causal agent that doesn’t just run but actually delivers trustworthy results requires solving multiple hard problems:\nThe Hallucination Problem: LLMs will confidently generate plausible-looking DAGs that are completely wrong. Without proper guardrails, your agent might conclude that ice cream sales cause summer. You need extensive validation frameworks and domain knowledge injection.\nThe Context Window Challenge: Real-world causal analyses involve understanding complex business contexts, historical decisions, and domain-specific knowledge. Cramming all this into a prompt while leaving room for actual analysis is non-trivial. We’re already hitting token limits on moderately complex problems.\nThe Validation Nightmare: How do you know if the agent’s causal graph is correct? Unlike traditional ML where you have ground truth labels, causal assumptions are often unfalsifiable. You need elaborate testing frameworks just to gain basic confidence.\nThe Cost Spiral: A thorough causal analysis might involve multiple rounds of DAG refinement, estimator selection, and robustness checks. With GPT-4 pricing, a single complex analysis could cost $5-10. Run this hourly across your org and watch your OpenAI bill explode."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#the-pitfalls-that-will-burn-you",
    "href": "posts/20250712-agentic-causal/2.html#the-pitfalls-that-will-burn-you",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "1. The Overconfidence Trap: Agents always sound authoritative. Your stakeholders won’t distinguish between “the agent is 95% sure” and “the agent made this up.” Clear uncertainty communication is essential but difficult.\n2. The Black Box Problem: When the agent chains together multiple tools and transformations, debugging why it reached a particular conclusion becomes nearly impossible. You need extensive logging and intermediate result storage.\n3. The Drift Issue: As your business evolves, the causal relationships change. But your agent doesn’t know this unless you explicitly update its knowledge base. Static assumptions in a dynamic world lead to increasingly wrong answers.\n4. The Compliance Nightmare: “An AI told us this drug was effective” won’t fly with regulators. You need audit trails for every decision, human sign-offs, and clear documentation of limitations.\n5. The Expertise Paradox: To build a good causal agent, you need deep causal inference expertise to design the guardrails. But if you have that expertise, do you need the agent?"
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#production-reality-check",
    "href": "posts/20250712-agentic-causal/2.html#production-reality-check",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "I’ve been experimenting with prototypes, and here’s what actually happens:\n\nSimple scenarios (&lt; 5 variables, clear causal direction): Agents work remarkably well\nMedium complexity (10-20 variables, some domain knowledge required): Success rate drops to ~60%, requires human validation\nReal-world mess (time-varying treatments, hidden confounders, selection bias): You still need human expertise\n\nThe gap between “identify the effect of X on Y in this clean dataset” and “untangle our marketing attribution across 50 channels with partial tracking” remains massive."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#a-pragmatic-path-forward",
    "href": "posts/20250712-agentic-causal/2.html#a-pragmatic-path-forward",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "If you’re considering causal agents, here’s a realistic adoption path:\nPhase 1: Augmentation, not automation - Use agents to generate initial DAGs for expert review - Automate the estimation pipeline once DAGs are approved - Focus on time savings in the “implementation” phase\nPhase 2: Known pattern automation - Identify your 5-10 most common causal questions - Build specialized agents for just these patterns - Maintain human oversight for anything novel\nPhase 3: Gradual expansion - As confidence grows, allow agents more autonomy - But always maintain “break glass” human review - Investment in testing infrastructure is non-negotiable"
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#code-that-actually-works-with-appropriate-skepticism",
    "href": "posts/20250712-agentic-causal/2.html#code-that-actually-works-with-appropriate-skepticism",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "# A conservative approach to causal agents\nfrom dowhy import CausalModel\nfrom causaltune import AutoTune\nimport pandas as pd\n\nclass CautiousCausalAgent:\n    def __init__(self, require_human_validation=True):\n        self.require_validation = require_human_validation\n        self.confidence_threshold = 0.8\n        \n    def analyze(self, df, treatment, outcome, known_confounders=None):\n        \"\"\"\n        Conservative causal analysis with multiple safety checks\n        \"\"\"\n        # Start with explicit assumptions\n        if known_confounders is None:\n            print(\"WARNING: No confounders specified. Results may be biased.\")\n            discovered_confounders = self.discover_confounders(df, treatment, outcome)\n            if self.require_validation:\n                print(f\"Suggested confounders: {discovered_confounders}\")\n                if not self.get_human_approval():\n                    raise ValueError(\"Human validation required\")\n        \n        # Build model with explicit graph\n        model = CausalModel(\n            data=df,\n            treatment=treatment,\n            outcome=outcome,\n            common_causes=known_confounders or discovered_confounders\n        )\n        \n        # Multiple estimation methods for robustness\n        estimates = []\n        for method in ['backdoor.linear_regression', \n                      'backdoor.propensity_score_matching',\n                      'backdoor.propensity_score_weighting']:\n            try:\n                est = model.estimate_effect(\n                    model.identify_effect(),\n                    method_name=method\n                )\n                estimates.append(est)\n            except:\n                pass\n        \n        # Check consistency\n        if not self.estimates_agree(estimates):\n            return {\n                'status': 'inconsistent',\n                'message': 'Different methods give conflicting results',\n                'estimates': estimates,\n                'recommendation': 'Requires expert review'\n            }\n            \n        # Refutation tests\n        refutation_results = self.run_refutations(model, estimates[0])\n        \n        return {\n            'status': 'success',\n            'effect': estimates[0].value,\n            'confidence_interval': estimates[0].get_confidence_intervals(),\n            'robustness': refutation_results,\n            'assumptions': model.get_assumptions()\n        }"
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#the-bottom-line",
    "href": "posts/20250712-agentic-causal/2.html#the-bottom-line",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "Causal inference agents in 2025 are where Docker was in 2014: promising, powerful in specific contexts, but requiring significant expertise to use safely. The difference is that when Docker fails, your app crashes. When causal inference fails, you make million-dollar decisions based on false assumptions.\nThe technology is real. The potential is enormous. But anyone selling you “causal inference in a box” is either naive or dishonest. What we have is a powerful set of tools that, when carefully implemented with appropriate guardrails, can accelerate and democratize causal analysis.\nThe teams that figure out the right balance—leveraging automation for mechanical tasks while maintaining human expertise for critical decisions—will have a significant competitive advantage. Just don’t bet the company on it. Yet."
  },
  {
    "objectID": "posts/20250712-agentic-causal/2.html#where-to-learn-more",
    "href": "posts/20250712-agentic-causal/2.html#where-to-learn-more",
    "title": "From DAG Diagrams to Do-Buttons: How Agentic Automation Is Re-Writing Causal Inference in 2025",
    "section": "",
    "text": "DoWhy Documentation: Still the best place to understand the fundamentals\nCausalML Papers: Read the original papers, not just the GitHub READMEs\nThe Book of Why: Pearl’s book remains essential for understanding what we’re trying to automate\nCausal Inference: The Mixtape: For the econometrics perspective\n\nStart small, validate everything, and remember: the goal isn’t to eliminate human judgment but to augment it.\n\nCurrently exploring this space and documenting lessons learned at github.com/[yourhandle]/causal-agent-experiments"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html",
    "href": "posts/20230802-cloudrun-githubaction/index.html",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "",
    "text": "Deploying ML models and other python apps to cloud can be tedious. Compute instances need to be provisioned; networking needs to be sorted out; autoscaling needs to be configured; secrets and credentials need to be safely managed.\nRather than spending hours on the above Dev-Ops tasks (don’t get me wrong, Dev-Ops and ML-Ops are important), I would like to focus on modeling: recipes that produce the best models and make them available for people to use. After years and many projects, I found Google Cloud Run to be a low maintainence solution, with CI/CD managed by Github Action. Similar solutions can be had with AWS ECS and Azure Container Instances. But this post will focus on Cloud Run."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "href": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along with the tutorial, you need:\n\nDocker\nGoogle Cloud SDK"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "href": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Sample App",
    "text": "Sample App\nLet’s start from a very simple http server and run it locally.\ndocker run --rm -it -p 801:801 python:3.8-slim python -m http.server 801 -d /home/\nRun it locally and we can verify it works by visiting localhost:801 in a browser.\n\nDeploy to Cloud Run manually\nHowever, the above docker image does not quite work for Cloud Run, as Cloud Run requires your app in the docker image to use the PORT environment variable to determine which port the app listens to.\nTo solve this we need to build a simple docker image with the following Dockerfile:\nFROM python:3.8-slim\nENV PORT=8080\nCMD python -m http.server $PORT -d /home\nInstall gcloud and authenticate. Then build and deploy it with the following script (click to expand):\n\n\n\n\n\n\nShell script for deploy to google cloud run\n\n\n\n\n\n# Make sure to fill in the GCP project id:\nproject=your-gcp-project-id\napp=example-app\nplatform=linux/amd64\nregion=us-central1\ndocker build --platform $platform -t example-app-image .\n\nimage=us.gcr.io/$project/$app:latest\ndocker tag example-app-image $image\ndocker push $image\ngcloud run deploy $app --image $image --cpu 1 --memory 1Gi --min-instances 1 --region $region --allow-unauthenticated\n\n\n\nNote that there are a couple of hard-coded defaults like the region (us-central1), and image subdomain (us.gcr.io). Feel free to adjust.\nIf successful, we will see something like this:\n\n\n\n\n\n\nConsole output during deployment\n\n\n\n\n\nDeploying container to Cloud Run service [example-app] in project [your-project-id] region [us-central1]\n✓ Deploying new service... Done.                                                 \n  ✓ Creating Revision...                                                         \n  ✓ Routing traffic...                                                           \n  ✓ Setting IAM Policy...                                                        \nDone.                                                                            \nService [example-app] revision [example-app-...] has been deployed and is serving 100 percent of traffic."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "href": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Manage secrets",
    "text": "Manage secrets\nIf the app needs to access secrets such as API keys and passwords, then it is a necessary to store and manage them securely.\nCreate a secret in GCP’s secret manager, and grant minimal necessary access.\nEach secret is versioned. For example, we may create a secret: MY_API_KEY:latest with latest being the version tag.\nWhen using gcloud run deploy to deploy the app, pass in additional arguments:\n--update-secrets=MY_API_KEY=MY_API_KEY:latest,OTHER_API_KEY=OTHER_API_KEY:latest\nIn the docker container, the secret value will be made available in the environment variable MY_API_KEY."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "href": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Set up a secure Github action for continuous deployment",
    "text": "Set up a secure Github action for continuous deployment\nWhile manually running the gcloud command is sufficient to deploy the app to Cloud Run, sometimes it can make sense to set up continuous deployment triggered by github push or release events.\n\nService account\nFirst, we need to follow these instructions to create a service account and grant some permissions:\nGo to IAM, click “grant access” and set: - principal: the new service account just created - role cloud run admin - role: roles/artifactregistry.createOnPushWriter - role: Secret manager secret accessor\nGrant the default compute-engine account access to Secret Manager Secret Accessor role. Go to IAM and set: - principal: the default compute-engine service account - role: Secret Manager Secret Accessor\nGo to IAM/service accounts, click into the default compute-engine service account, then allow the new service account to use this compute engine service account: - principal: the new service account just created - role: “Service account user”\n\n\n\n\n\n\nTip\n\n\n\nI spent hours debugging permission errors in the github actions and found the above steps helped resolving the errors. More info here and here. However, I suspect some of them are not necessary. Please let me know (zhangzhang.si AT gmail.com) if you have a different experience.\n\n\n\n\nDocker artifacts repository\nA docker artifacts repository must be created in the same project as the Cloud Run service (we assume the location is “us-central1”):\ngcloud artifacts repositories create slack-llm --location=us-central1 --repository-format=docker\nThis artifacts repository will hold the docker image of the app.\n\n\nWorkload identify federation and keyless authentication\nFor better cloud security, Google recommends setting up keyless authentication from github actions. To do that, we need to:\n\n\n\n\n\n\nCreate a Workload Identify Pool\n\n\n\n\n\ngcloud iam workload-identity-pools create \"my-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"Demo pool\" \\\n  --description=\"My Identify Pool\"\n\n\n\n\n\n\n\n\n\nThen create a Workload Identify Provider:\n\n\n\n\n\ngcloud iam workload-identity-pools providers create-oidc \"my-provider\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"my-pool\" \\\n  --display-name=\"Demo provider\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.aud=assertion.aud\" \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\"\n\n\n\n\n\n\n\n\n\nThen allow authentications from the Workload Identity Provider to impersonate the desired Service Account:\n\n\n\n\n\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/attribute.repository/my-org/my-repo\"\nAlternatively, if we do not want to restrict the binding to the specific github repo, then:\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/*\"\n\n\n\n\n\nGithub secrets\nAdd the following github secrets (see instructions on how to add secrets to a github repo):\nWIF_PROVIDER=projects/my-gcp-project-number/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n\nWIF_SERVICE_ACCOUNT=my-service-account@my-project.iam.gserviceaccount.com\n\n\nGithub action yaml file\nNow we should be ready to set up the actual github action. This is a redacted version of my working github action yaml file:\n\n\n\n\n\n\nYAML File\n\n\n\n\n\nYAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}\n\n\n\nPut this in .github/workflows/deploy.yml and the next time you push a change to main, it should automatically deploy to Cloud Run.\nEnjoy!"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "href": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "YAML for Github Action",
    "text": "YAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}"
  }
]
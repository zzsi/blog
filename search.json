[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "Notes on practical AI and engineering."
  },
  {
    "objectID": "about.html#zz-si",
    "href": "about.html#zz-si",
    "title": "About this blog",
    "section": "ZZ Si",
    "text": "ZZ Si\n\nCo-founder and Engineer @KUNGFU.AI\nExpertise: Computer vision, Generative models, Practical AI\nPreviously: Apple, Google, Expedia and 2 other startups\nPh.D. Stats @UCLA’11, B.S. CS @Tsinghua’06"
  },
  {
    "objectID": "drafts/post-with-code/index.html",
    "href": "drafts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nprint(\"Hello!\")\n\nHello!"
  },
  {
    "objectID": "drafts/welcome/index.html",
    "href": "drafts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html",
    "href": "posts/20230802-cloudrun-githubaction/index.html",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "",
    "text": "Deploying ML models and other python apps to cloud can be tedious. Compute instances need to be provisioned; networking needs to be sorted out; autoscaling needs to be configured; secrets and credentials need to be safely managed.\nRather than spending hours on the above Dev-Ops tasks (don’t get me wrong, Dev-Ops and ML-Ops are important), I would like to focus on modeling: recipes that produce the best models and make them available for people to use. After years and many projects, I found Google Cloud Run to be a low maintainence solution, with CI/CD managed by Github Action. Similar solutions can be had with AWS ECS and Azure Container Instances. But this post will focus on Cloud Run."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "href": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along with the tutorial, you need:\n\nDocker\nGoogle Cloud SDK"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "href": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Sample App",
    "text": "Sample App\nLet’s start from a very simple http server and run it locally.\ndocker run --rm -it -p 801:801 python:3.8-slim python -m http.server 801 -d /home/\nRun it locally and we can verify it works by visiting localhost:801 in a browser.\n\nDeploy to Cloud Run manually\nHowever, the above docker image does not quite work for Cloud Run, as Cloud Run requires your app in the docker image to use the PORT environment variable to determine which port the app listens to.\nTo solve this we need to build a simple docker image with the following Dockerfile:\nFROM python:3.8-slim\nENV PORT=8080\nCMD python -m http.server $PORT -d /home\nInstall gcloud and authenticate. Then build and deploy it with the following script (click to expand):\n\n\n\n\n\n\nShell script for deploy to google cloud run\n\n\n\n\n\n# Make sure to fill in the GCP project id:\nproject=your-gcp-project-id\napp=example-app\nplatform=linux/amd64\nregion=us-central1\ndocker build --platform $platform -t example-app-image .\n\nimage=us.gcr.io/$project/$app:latest\ndocker tag example-app-image $image\ndocker push $image\ngcloud run deploy $app --image $image --cpu 1 --memory 1Gi --min-instances 1 --region $region --allow-unauthenticated\n\n\n\nNote that there are a couple of hard-coded defaults like the region (us-central1), and image subdomain (us.gcr.io). Feel free to adjust.\nIf successful, we will see something like this:\n\n\n\n\n\n\nConsole output during deployment\n\n\n\n\n\nDeploying container to Cloud Run service [example-app] in project [your-project-id] region [us-central1]\n✓ Deploying new service... Done.                                                 \n  ✓ Creating Revision...                                                         \n  ✓ Routing traffic...                                                           \n  ✓ Setting IAM Policy...                                                        \nDone.                                                                            \nService [example-app] revision [example-app-...] has been deployed and is serving 100 percent of traffic."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "href": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Manage secrets",
    "text": "Manage secrets\nIf the app needs to access secrets such as API keys and passwords, then it is a necessary to store and manage them securely.\nCreate a secret in GCP’s secret manager, and grant minimal necessary access.\nEach secret is versioned. For example, we may create a secret: MY_API_KEY:latest with latest being the version tag.\nWhen using gcloud run deploy to deploy the app, pass in additional arguments:\n--update-secrets=MY_API_KEY=MY_API_KEY:latest,OTHER_API_KEY=OTHER_API_KEY:latest\nIn the docker container, the secret value will be made available in the environment variable MY_API_KEY."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "href": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Set up a secure Github action for continuous deployment",
    "text": "Set up a secure Github action for continuous deployment\nWhile manually running the gcloud command is sufficient to deploy the app to Cloud Run, sometimes it can make sense to set up continuous deployment triggered by github push or release events.\n\nService account\nFirst, we need to follow these instructions to create a service account and grant some permissions:\nGo to IAM, click “grant access” and set: - principal: the new service account just created - role cloud run admin - role: roles/artifactregistry.createOnPushWriter - role: Secret manager secret accessor\nGrant the default compute-engine account access to Secret Manager Secret Accessor role. Go to IAM and set: - principal: the default compute-engine service account - role: Secret Manager Secret Accessor\nGo to IAM/service accounts, click into the default compute-engine service account, then allow the new service account to use this compute engine service account: - principal: the new service account just created - role: “Service account user”\n\n\n\n\n\n\nTip\n\n\n\nI spent hours debugging permission errors in the github actions and found the above steps helped resolving the errors. More info here and here. However, I suspect some of them are not necessary. Please let me know (zhangzhang.si AT gmail.com) if you have a different experience.\n\n\n\n\nDocker artifacts repository\nA docker artifacts repository must be created in the same project as the Cloud Run service (we assume the location is “us-central1”):\ngcloud artifacts repositories create slack-llm --location=us-central1 --repository-format=docker\nThis artifacts repository will hold the docker image of the app.\n\n\nWorkload identify federation and keyless authentication\nFor better cloud security, Google recommends setting up keyless authentication from github actions. To do that, we need to:\n\n\n\n\n\n\nCreate a Workload Identify Pool\n\n\n\n\n\ngcloud iam workload-identity-pools create \"my-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"Demo pool\" \\\n  --description=\"My Identify Pool\"\n\n\n\n\n\n\n\n\n\nThen create a Workload Identify Provider:\n\n\n\n\n\ngcloud iam workload-identity-pools providers create-oidc \"my-provider\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"my-pool\" \\\n  --display-name=\"Demo provider\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.aud=assertion.aud\" \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\"\n\n\n\n\n\n\n\n\n\nThen allow authentications from the Workload Identity Provider to impersonate the desired Service Account:\n\n\n\n\n\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/attribute.repository/my-org/my-repo\"\nAlternatively, if we do not want to restrict the binding to the specific github repo, then:\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/*\"\n\n\n\n\n\nGithub secrets\nAdd the following github secrets (see instructions on how to add secrets to a github repo):\nWIF_PROVIDER=projects/my-gcp-project-number/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n\nWIF_SERVICE_ACCOUNT=my-service-account@my-project.iam.gserviceaccount.com\n\n\nGithub action yaml file\nNow we should be ready to set up the actual github action. This is a redacted version of my working github action yaml file:\n\n\n\n\n\n\nYAML File\n\n\n\n\n\nYAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}\n\n\n\nPut this in .github/workflows/deploy.yml and the next time you push a change to main, it should automatically deploy to Cloud Run.\nEnjoy!"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "href": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "YAML for Github Action",
    "text": "YAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on practical AI, engineering and life",
    "section": "",
    "text": "Computer vision for soccer games\n\n\n\n\n\n\ncomputer vision\n\n\nai\n\n\nsports\n\n\nsoccer\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth\n\n\n\n\n\n\neconomics\n\n\nai\n\n\nenvironment\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying machine learning apps to Google Cloud Run with Github actions\n\n\n\n\n\n\ncode\n\n\nmlops\n\n\nGCP\n\n\ncloud\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nZZ Si\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html",
    "href": "posts/20240915-soccer-tracking/index.html",
    "title": "Computer vision for soccer games",
    "section": "",
    "text": "I was intrigued to see this example where a variety (at least 5) computer vision techniques to create visual appealing analytics from soccer game footage. Soccer fans and coaches maenjoy this.\nVideo\nThis is an open source demo from Roboflow, and is easy to reproduce. Since it is a proof of concept, more work needs to be done to make it work for other real world videos, where there a large portion of the soccer field is not visible, or when the camera moved fast (which happens quite often). This is a common challenge for practical computer vision: it can be hard to make an impressive model work on your data.\nBelow I share a workflow to reproduce both success and limitations of this soccer tracking example, and some ideas to improve it to make it work on more challenging data. Similar techniques can be applied to other sports, like tennis, (American) football, basketball, pickle ball, etc."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "href": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "title": "Computer vision for soccer games",
    "section": "Reproducing the birds-eye view creation",
    "text": "Reproducing the birds-eye view creation\n\n\n\n\n\n\nPre-requisites\n\n\n\n\n\nPre-requisites\n\nYou need a machine with GPU to run the code. The code is tested on a machine with a GeForce RTX 3090, and it uses about 3GB of GPU memory.\nYou need to have git, docker and python (3.6+) installed.\n\n\n\n\n\n\n\n\n\n\nDownload\n\n\n\n\n\nStep 1: Download the code and data\ncvlization is an open source repo with many working examples of computer vision workflows. Clone the repo:\ngit clone https://github.com/kungfuai/cvlization.git\ncd cvlization\nIn examples/sports/soccer_game_visual_tracking, there is a README file that explains how to download the model weights and example video data (pip install gdown if you haven’t already).\ncd examples/sports/soccer_game_visual_tracking\nbash download_data.sh\n\n\n\n\n\n\n\n\n\nInstall\n\n\n\n\n\nStep 2: Install the dependencies by building a docker image\nChange directory back to the root of the cvlization repo, and run\nbash examples/sports/soccer_game_visual_tracking/build.sh\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\n\n\nStep 3: Run the code\nbash examples/sports/soccer_game_visual_tracking/predict.sh\nIn this script, we are using a 30 second clip from a soccer game. The script will track the pitch and players, identify the team, goal keepers, referee, and ball, and generate a bird’s eye view video. Feel free to modify the script to use a different video or to change the tracking parameters.\nYou will find the output video in examples/sports/soccer_game_visual_tracking/0bfacc_0-radar.mp4. This is the video shown on the top of the page. On a machine with a GeForce RTX 3090, it takes about 20 minutes to run, with 3GB of GPU memory used."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "href": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "title": "Computer vision for soccer games",
    "section": "Under the hood",
    "text": "Under the hood\nThe computer vision models and algorithms under the hood include:\n\nA keypoint detection (pose estimation) model for 32 keypoints on the soccer pitch (Yolo-v8, 70M, training notebook, mAP=0.99, 1 hour on NVidia T4, trained on hundreds of images).\n\n\n\n\n\n\n\nAn object detection model for players, referrees and goal keepers (Yolo-v8, 68M, training notebook, mAP=0.79, 40min on NNivida L4).\n\n\n\n\n\n\n\nAnother object detection model for the ball (Yolo-v8, 68M, training notebook, mAP=0.93, 1.3 hours on NVidia A100). The ball is very small in the image, so it is hard to detect.\nA multi-object tracking model to track the players and the ball (Bytetrack, implementation and python API).\nA vision embedding model and clustering algorithm for team identification. SigLIP is used to extract embedding vectors from cropped players. UMAP is used for dimensionality reduction. K-means is used for clustering. Also Resolve the team IDs for detected goalkeepers based on the proximity to team centroids (based on player locations).\nAn image registration/stitching algorithm to create the bird’s eye view. Homography is estimated between the pitch keypoints and the reference coornidates of the pitch, using OpenCV’s findHomography. The pitch in the footage is then warped to a top-down view using perspectiveTransform.\nPlayer re-identification models (e.g. MOTIP). When the footage is cut or camera is changed to a different angle, the player IDs are lost. We need to re-identify the players in order to connect the player tracks across different clips. I did not find the implemetation in this POC."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "href": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "title": "Computer vision for soccer games",
    "section": "Does it work on other soccer videos?",
    "text": "Does it work on other soccer videos?\nI picked a random soccer game clip, and the result is not as good as the example video. The camera moved faster, zooming in to a partial view of the pitch near the goal post. This posed challenges to the keypoint detection model, and the player tracking model. Some players were not detected due to motion blur and occlusion. Key points of the pitch were not detected in some frames, and the algorithm was not able to create a bird’s eye view for those frames. The result is shown below:\nVideo\nRegardless, it is a great starting point to build a more reliable system for soccer game analytics. For fun, I also tried it on a very challenging video with a couple of professional players against 100 pupils. Interestingly, the algorithm was able to detect most the players, and create a bird’s eye view, as long as a large portion of the pitch is visible:"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "href": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "title": "Computer vision for soccer games",
    "section": "Makeing it better: more accurate player detection and tracking",
    "text": "Makeing it better: more accurate player detection and tracking\n\nTransformers for object tracking\nAccurate tracking requires attending to relationships between detected players on different frames, their roles, jersey colors etc. Transformers architecture is well suited for this task.\n\nGlobal tracking transformer\nGlobal tracking transformers takes a video as input, and predict object tracks in an end-to-end fashion. It was trained on LVIS and COCO, capable of tracking 1000+ categories of objects. Below is the result for tracking persons and the ball. It also identified the billboards though they are not directly useful for our purpose here. This is the tracking result overlayed on the input video:\nVideo\nComparing YOLOv8 and Global Tracking Transformer, the latter seems more accurate.\n\n\n\n\n\n\n\n\n\nYOLOv8\n\n\n\n\n\n\n\nGlobal Tracking Transformer\n\n\n\n\n\n\n\n\nVision-language models, open vocabulary and zero-shot object detection\nWith recent advances in vision-language models, we can leverage the visual knowledge in pretrained large models. How well do they work in detecting players?\n\nGrounding DINO\nThis model has a DINO transformer backbone and produced by grounded pre-training. You can prompt the model with a sentence or a phrase, and it will highlight the corresponding region in the image. Below is the architecture of Grounding DINO:\n\n\n\nArchitecture of Grounding DINO\n\n\n\n\n\nWith one prompt, Grounding DINO was able to detect players but had a hard time distinguishing the goal keeper from other players.\n\n\n\n\nYOLO World\nThis model is an open-vocabulary object detection model. It can detect objects that are not in the training set, and can be used for zero-shot object detection. You can prompt it with a list of words, such as “player, ball, goal keeper”.\nCompared to Grounding DINO, YOLO World seems less accurate and misses some players when they overlap.\n\n\n\nYOLO-World-XL player detection result.\n\n\nThese are just two examples of recent models."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#datasets",
    "href": "posts/20240915-soccer-tracking/index.html#datasets",
    "title": "Computer vision for soccer games",
    "section": "Datasets",
    "text": "Datasets\nYou may need to fine tune the models on more soccer game videos with annotations. Here are some datasets that can be useful:\nSoccerNet is a large-scale dataset for soccer analysis. It contains 550 complete broadcast soccer games and 12 single camera games taken from the major European leagues. It supports various vision tasks such as action spotting, camera calibration, player re-identification and tracking.\nThis Kaggle dataset also contains soccer game videos from Premier League showdowns to FIFA World Cup classics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "href": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "title": "Computer vision for soccer games",
    "section": "Business use cases",
    "text": "Business use cases\nBoardly, here are some areas where computer vision can be used in soccer analytics:\n\nPerformance Analysis: By tracking player movement, positioning, and interactions, teams can better understand individual and team performance, making it easier to identify strengths and areas for improvement.\nTactical Insights: Coaches can analyze formations, pressing patterns, and set-pieces to gain a competitive edge, adjusting their game plans based on data.\n** Player Development**: Young athletes can leverage computer vision technology to receive feedback on their performance and improve their skills over time.\nFan Engagement: Computer vision can create engaging, immersive content for fans, such as 3D replays or interactive match highlights, bringing them closer to the action.\n\nHere is a very incomplete list of companies and use cases:\n\nVeo: AI-powered cameras for automatic sports recording, tracking game action, and AI-tagged highlights for analysis.\nTraceup: Video captures that allow tracking players individually, creating personalized highlight reels that parents, players, and coaches can view from various angles.\nTrack160: Skeleton tracking, identifying and monitoring the movement of players and the ball, tagging and analyzing events in a match, physical and tactical breakdowns of player performances.\nNY Times created 3D stories that allow fans to experience game-defining moments from multiple angles and gain deeper insights into player positioning, ball movement, and tactics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#conclusion",
    "href": "posts/20240915-soccer-tracking/index.html#conclusion",
    "title": "Computer vision for soccer games",
    "section": "Conclusion",
    "text": "Conclusion\nThis is just a start. I am glad to see computer vision applied to everyday life, and hope this post spark some ideas."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html",
    "href": "posts/20240825-sustainable-future/index.html",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "",
    "text": "Since the advent of the free market, human society has experienced an unprecedented wave of growth and prosperity. Global GDP has increased 100-fold, with per-capita GDP rising 15-fold since the early 1800s. However, this tremendous growth has exacted a significant toll on the environment. As we stand on the brink of another nascent revolution, artificial intelligence (AI) promises to usher in a second wave of growth—this time, much more sustainable. Could AI help us achieve the elusive goal of expanding our economies while preserving the planet?"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "href": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "The Miracle and Pitfall of Demand-Driven Production",
    "text": "The Miracle and Pitfall of Demand-Driven Production\n\nThe miracle\nThe past two centuries have indeed been nothing short of a miracle in terms of economic growth, not just for the sheer scale of economic expansion but for its profound impact on human well-being.\nBefore the Industrial Revolution, global poverty was widespread, with the vast majority of the population living on subsistence agriculture, vulnerable to disease, famine, and political instability. But with the advent of mechanized production, steam power, and eventually electricity, societies began to shift from agrarian economies to industrial ones, spurring rapid urbanization and creating millions of new jobs.\n\n (source)\n\n (source)\nAs economies grew, so did living standards. In the 20th century, especially after World War II, growth accelerated dramatically. Advances in medicine, sanitation, and food production allowed populations to boom while simultaneously reducing mortality rates. Global poverty, which once seemed an inescapable fate for most, began to decline sharply. According to the World Bank, extreme poverty (defined as living on less than $1.90 a day) fell from about 80% of the world’s population in 1820 to less than 10% today. This reduction in poverty was most pronounced in Asia, where countries like China and India harnessed industrialization and global trade to lift hundreds of millions out of destitution.\nAs the engines of industry roared to life, they did more than just produce—they created a world where, for the first time, sufficient goods could be made to meet the needs of millions. Farms, once worked by hand, now harnessed the power of machines, yielding crops at unprecedented rates. Factories churned out textiles, tools, and eventually, the comforts of modern living that had once been unimaginable luxuries. This newfound capacity wasn’t just about survival; it was about abundance. Goods that had once been scarce or accessible only to the wealthy became attainable for the masses. Food production soared, homes were built, and technologies that improved everyday life spread across the globe. In this wave of growth, the world became a place where production was not only sufficient but could also fulfill the aspirations of those who sought more than just the bare necessities.\n\n\nThe pitfall\nWhile we feel grateful for the growth and abundance that this era of production has brought us, it’s important to recognize the shadows cast by this prosperity. For every product that meets a need, there are countless others that sit unused, discarded, or wasted. The very systems that allowed us to produce more than ever before also led to overproduction, filling landfills with excess and polluting our air and waters with the byproducts of unchecked growth.\n\n\n\nCar graveyard after Chinese company went bankrupt. Source: @Wolf of X\n\n\n\n\n\nAerial picture of the tire graveyard in Kuwait. Final resting place of over 7,000,000 rubber tires. Source: @Wolf of X\n\n\n\n\n\nClothing graveyard. The so-called “clothing graveyard,” about 30,000 tons of discarded clothing piled in a landfill in the Atacama Desert, Chile, in 2021. Source: Antonio Cossio—picture alliance/Getty Images\n\n\nIs such a level of waste inevitable? I would argue that it is, given the nature of how our economies have evolved. The growth we’ve witnessed, particularly over the last century, has been driven largely by demand—an insatiable appetite for more. With the rise of consumerism, the focus shifted from simply meeting needs to creating new desires. As historian Frederick Allen observed, “Business had learned as never before the importance of the ultimate consumer. Unless he could be persuaded to buy and buy lavishly, the whole stream of six-cylinder cars, super heterodynes, cigarettes, rouge compacts, and electric ice boxes would be dammed up at its outlets.” (source)\nThis relentless push to fuel demand led companies to innovate not just in production but also in persuasion. Advertising, marketing, and product design all became tools to keep the consumer engaged and always wanting more. The result? A system where the pressure to buy, to replace, and to upgrade created a cycle of overproduction and, inevitably, waste."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "href": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Is consumerism at fault?",
    "text": "Is consumerism at fault?\nThe solution is not to stay away from consumerism and demand-driven market economy. Without demand, there would be no profit, and without profit, companies would have no reason to put products on the market. This, in turn, would halt productivity, leaving not enough food on families’ tables or goods in their homes.\nOver-production is also inevitable. The reality is that producing just enough to meet actual needs isn’t sufficient, because market systems and distribution networks are inherently imperfect. Food, clothes that are produced do not always reach who need them. True efficiency is hard to achieve, and inequality makes this even worse. If the distribution efficiency is only 10%, then we must produce ten times the necessary amount to meet the demand. This excess production, while ensuring availability, often leads to surplus and waste.\nSurplus eats into profits if it isn’t consumed. To keep factories running, corporations thriving, and jobs secure, our dear consumers must continually want more. This is the crux of the demand-driven economy: without constant consumption, the entire system risks stagnation. As a result, businesses invest heavily in marketing, innovation, and new product lines to stimulate desire, encouraging consumers to keep buying—whether or not their needs have truly changed.\n\n\n\nGrowth of supply and manufactured demand beyond need"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "href": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "A way out: targeted production with AI",
    "text": "A way out: targeted production with AI\nAmazon’s inventory planning system points to a promising direction. Algorithms can forecast what consumers are likely to purchase with remarkable accuracy, which allow buying and placing inventory accordingly to optimize order fulfillment. As a result, efficiency went up, and waste went down.\nAnd we can push this even further. If demand is way higher than actual need, why not shift production to better match what people really need? Imagine if we weren’t constantly hit with endless ads and social media bragging. Our homes would be less cluttered, people wouldn’t need to take on debt just to keep up with the luxury status game, and we could all spend more time with loved ones or out in nature. Life would feel simpler and more focused on what really matters, rather than being driven by overconsumption.\nThis can happen through targeted production, with AI helping in two ways: automation (boosting production efficiency) and forecasting (improving market efficiency).\nAutomation isn’t new—it’s been part of past tech revolutions—but AI is different because it’s more versatile. It can handle many things from language tasks to tool use, extending the ‘Crown Jewels’ of human intelligence. AI can streamline workflows across corporate functions like accounting, finance, engineering, sales, and marketing, making processes faster and more efficient. Forecasting will further increase market efficiency by accurately predicting demand, allowing businesses to align production more closely with real-time consumer needs. These two factors—automation and forecasting—make anticipatory production and just-in-time production possible. Instead of waiting for demand to fully materialize, we can anticipate and initiate production just ahead of time—producing what is likely needed, when it’s needed.\nIndirectly, AI can help curb the constant stimulation of consumer desires. The problem isn’t advertising itself but rather the excessive advertising that arises in overcrowded, saturated markets. When businesses struggle to meaningfully differentiate their products, they rely heavily on aggressive marketing to capture attention, contributing to the cycle of overconsumption. This reflects poor planning and a lack of clear insight into what consumers truly need—a symptom of incomplete information and insufficient foresight into future demands.\nWhen businesses begin to realize they can be profitable with automation and better planning instead of excessive advertising, they can step back from the exhausting zero-sum game of trying to out-market each other. Their shareholders and employees can finally find peace."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "href": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe AI revolution is still in its early days, and there are challenges like job displacement and energy use that worry people. But despite these hurdles, I am hopeful AI can help future generations enjoy a more sustainable and prosperous future."
  },
  {
    "objectID": "drafts/20230629-rag/index.html",
    "href": "drafts/20230629-rag/index.html",
    "title": "Pratical retrieval augmented generation (RAG)",
    "section": "",
    "text": "To overcome the token limit of large language models, one important recipe is retrieval augmentation.\nThe augmentation can generally happen at 3 places:"
  },
  {
    "objectID": "drafts/20230629-rag/index.html#references",
    "href": "drafts/20230629-rag/index.html#references",
    "title": "Pratical retrieval augmented generation (RAG)",
    "section": "References",
    "text": "References\n\nLong-range Language Modeling with Self-retrieval"
  }
]
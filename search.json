[
  {
    "objectID": "posts/20240825-sustainable-future/index.html",
    "href": "posts/20240825-sustainable-future/index.html",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "",
    "text": "Since the advent of the free market, human society has experienced an unprecedented wave of growth and prosperity. Global GDP has increased 100-fold, with per-capita GDP rising 15-fold since the early 1800s. However, this tremendous growth has exacted a significant toll on the environment. As we stand on the brink of another nascent revolution, artificial intelligence (AI) can usher in a second wave of growth—this time, much more sustainable. Could AI help us achieve the elusive goal of expanding our economies while preserving the planet?"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "href": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "The Miracle and Pitfall of Demand-Driven Production",
    "text": "The Miracle and Pitfall of Demand-Driven Production\n\nThe miracle\nThe past two centuries have indeed been nothing short of a miracle in terms of economic growth, not just for the sheer scale of economic expansion but for its profound impact on human well-being.\nBefore the Industrial Revolution, global poverty was widespread, with the vast majority of the population living on subsistence agriculture, vulnerable to disease, famine, and political instability. But with the advent of mechanized production, steam power, and eventually electricity, societies began to shift from agrarian economies to industrial ones, spurring rapid urbanization and creating millions of new jobs.\n\n (source)\n\n (source)\nAs economies grew, so did living standards. In the 20th century, especially after World War II, growth accelerated dramatically. Advances in medicine, sanitation, and food production allowed populations to boom while simultaneously reducing mortality rates. Global poverty, which once seemed an inescapable fate for most, began to decline sharply. According to the World Bank, extreme poverty (defined as living on less than $1.90 a day) fell from about 80% of the world’s population in 1820 to less than 10% today. This reduction in poverty was most pronounced in Asia, where countries like China and India harnessed industrialization and global trade to lift hundreds of millions out of destitution.\nAs the engines of industry roared to life, they did more than just produce—they created a world where, for the first time, sufficient goods could be made to meet the needs of millions. Farms, once worked by hand, now harnessed the power of machines, yielding crops at unprecedented rates. Factories churned out textiles, tools, and eventually, the comforts of modern living that had once been unimaginable luxuries. This newfound capacity wasn’t just about survival; it was about abundance. Goods that had once been scarce or accessible only to the wealthy became attainable for the masses. Food production soared, homes were built, and technologies that improved everyday life spread across the globe. In this wave of growth, the world became a place where production was not only sufficient but could also fulfill the aspirations of those who sought more than just the bare necessities.\n\n\nThe pitfall\nWhile we feel grateful for the growth and abundance that this era of production has brought us, it’s important to recognize the shadows cast by this prosperity. For every product that meets a need, there are countless others that sit unused, discarded, or wasted. The very systems that allowed us to produce more than ever before also led to overproduction, filling landfills with excess and polluting our air and waters with the byproducts of unchecked growth.\n\n\n\nCar graveyard after Chinese company went bankrupt. Source: @Wolf of X\n\n\n\n\n\nAerial picture of the tire graveyard in Kuwait. Final resting place of over 7,000,000 rubber tires. Source: @Wolf of X\n\n\n\n\n\nClothing graveyard. The so-called “clothing graveyard,” about 30,000 tons of discarded clothing piled in a landfill in the Atacama Desert, Chile, in 2021. Source: Antonio Cossio—picture alliance/Getty Images\n\n\nIs such a level of waste inevitable? I would argue that it is, given the nature of how our economies have evolved. The growth we’ve witnessed, particularly over the last century, has been driven largely by demand—an insatiable appetite for more. With the rise of consumerism, the focus shifted from simply meeting needs to creating new desires. As historian Frederick Allen observed, “Business had learned as never before the importance of the ultimate consumer. Unless he could be persuaded to buy and buy lavishly, the whole stream of six-cylinder cars, super heterodynes, cigarettes, rouge compacts, and electric ice boxes would be dammed up at its outlets.” (source)\nThis relentless push to fuel demand led companies to innovate not just in production but also in persuasion. Advertising, marketing, and product design all became tools to keep the consumer engaged and always wanting more. The result? A system where the pressure to buy, to replace, and to upgrade created a cycle of overproduction and, inevitably, waste."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "href": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Is consumerism at fault?",
    "text": "Is consumerism at fault?\nThe solution is not to stay away from consumerism and demand-driven market economy. Without demand, there would be no profit, and without profit, companies would have no reason to put products on the market. This, in turn, would halt productivity, leaving not enough food on families’ tables or goods in their homes.\nOver-production is also inevitable. The reality is that producing just enough to meet actual needs isn’t sufficient, because market systems and distribution networks are inherently imperfect. Food, clothes that are produced do not always reach who need them. True efficiency is hard to achieve, and inequality makes this even worse. If the distribution efficiency is only 10%, then we must produce ten times the necessary amount to meet the demand. This excess production, while ensuring availability, often leads to surplus and waste.\nSurplus eats into profits if it isn’t consumed. To keep factories running, corporations thriving, and jobs secure, our dear consumers must continually want more. This is the crux of the demand-driven economy: without constant consumption, the entire system risks stagnation. As a result, businesses invest heavily in marketing, innovation, and new product lines to stimulate desire, encouraging consumers to keep buying—whether or not their needs have truly changed.\n\n\n\nGrowth of supply and manufactured demand beyond need"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "href": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "A way out: targeted production with AI",
    "text": "A way out: targeted production with AI\nAmazon’s inventory planning system points to a promising direction. Algorithms can forecast what consumers are likely to purchase with remarkable accuracy, which allow buying and placing inventory accordingly to optimize order fulfillment. As a result, efficiency went up, and waste went down.\nAnd we can push this even further. If demand is way higher than actual need, why not shift production to better match what people really need? Imagine if we weren’t constantly hit with endless ads and social media bragging. Our homes would be less cluttered, people wouldn’t need to take on debt just to keep up with the luxury status game, and we could all spend more time with loved ones or out in nature. Life would feel simpler and more focused on what really matters, rather than being driven by overconsumption.\nThis can happen through targeted production, with AI helping in two ways: automation (boosting production efficiency) and forecasting (improving market efficiency).\nAutomation isn’t new—it’s been part of past tech revolutions—but AI is different because it’s more versatile. It can handle many things from language tasks to tool use, extending the ‘Crown Jewels’ of human intelligence. AI can streamline workflows across corporate functions like accounting, finance, engineering, sales, and marketing, making processes faster and more efficient. Forecasting will further increase market efficiency by accurately predicting demand, allowing businesses to align production more closely with real-time consumer needs. These two factors—automation and forecasting—make anticipatory production and just-in-time production possible. Instead of waiting for demand to fully materialize, we can anticipate and initiate production just ahead of time—producing what is likely needed, when it’s needed.\nIndirectly, AI can help curb the constant stimulation of consumer desires. The problem isn’t advertising itself but rather the excessive advertising that arises in overcrowded, saturated markets. When businesses struggle to meaningfully differentiate their products, they rely heavily on aggressive marketing to capture attention, contributing to the cycle of overconsumption. This reflects poor planning and a lack of clear insight into what consumers truly need—a symptom of incomplete information and insufficient foresight into future demands.\nWhen businesses begin to realize they can be profitable with automation and better planning instead of excessive advertising, they can step back from the exhausting zero-sum game of trying to out-market each other. Their shareholders and employees can finally find peace."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "href": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe AI revolution is still in its early days, and there are challenges like job displacement and energy use that worry people. But despite these hurdles, I am hopeful AI can help future generations enjoy a more sustainable and prosperous future."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html",
    "href": "posts/20241002-minisora-part1/index.html",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#introduction",
    "href": "posts/20241002-minisora-part1/index.html#introduction",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "href": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Pareto frontier: Aiming for good and small",
    "text": "Pareto frontier: Aiming for good and small\n\n\n\n\n\nThis graph from “Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI” illustrates a key challenge in AI development. SORA would be a frontier model at the resource-intensive end of the spectrum. We want to move in the direction of the green arrow, striving for lower training cost while maintaining high quality.\nAccess to vast computational resources, such as 10,000 A100 GPUs, is limited to a handful of organizations. Even if such resources were widely available, focusing solely on resource-intensive methods would be an inefficient use of our capabilities. The design space for training recipes is vast, and a strategic approach involves exploring this space through low-cost experiments before scaling up when confidence is high.\nThis raises an intriguing question: With a modest budget, is it possible to train a general-purpose video generation model comparable to SORA?"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "href": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "The need for controlling the domain complexity",
    "text": "The need for controlling the domain complexity\n\nThe challenge of training general-purpose models with limited resources\nSORA’s training costs likely run into tens of millions of dollars, driven by both data and model size. Larger datasets necessitate longer training times, while bigger models require both extended training periods and more high-end GPUs.\nIs it feasible to train a high-quality model with a significantly smaller dataset? This seems impossible due to the inherent complexity of our world. Are one million video clips sufficient to capture our world’s complexity? 10 Million? 100 Million? Probably more than that. While sample-efficient algorithms can help reduce the required data size, the order of magnitude for necessary data likely remains substantial.\nSimilarly, training a high-quality model with a much smaller architecture presents its own challenges. Unless a dramatically more efficient architecture than Transformers emerges, a small model would struggle to capture the complexity present in such vast datasets.\nTherefore, to make progress with limited resources, we must find ways to reduce the data size.\n\n\nExploring niche domains: A path to low-budget training\nNiche domains can be significantly simpler than our physical world, potentially allowing a few tens of thousands of observations to sufficiently represent the domain. With a drastic reduction in data size, smaller models and lower training costs become feasible.\nWe can conceptualize a series of domains, progressing from simple to complex:\n\n2D Flying MNIST (a 2D world with colorful handwritten digits moving at constant speed, bouncing off boundaries)\n2D arcade games (Pong, Breakout, etc.)\nAnime and cartoons\nLimited locations: video walkthroughs of 3D house models, fly-through views of objects (e.g., NERF models)\nLimited objects: close-up videos of specific subjects (e.g., dogs, selfie videos)\nLimited scenery: footage of hiking trails, beaches, etc.\nPublic video datasets: UCF-101, Panda-70M, InterVid, etc.\nThe real world, and our collective video reservoir.\n\nA strategic approach involves starting from the simplest domain and gradually progressing towards more complex ones. Effective training recipes discovered in simpler domains are expected to scale to more complex scenarios with straightforward increases in data and model size.\nInterestingly, this mirrors how humans learn: start from simple lessons and gradually build up to more complex concepts.\n\n\nPre-train or fine-tune?\nFine-tuning is an effective strategy to reduce training costs, but it comes with certain limitations:\n\nFixed architecture: The model’s architecture is predetermined, which can be a significant constraint as we may still be far from an optimal design for video generation tasks.\nVAE dependency: Pre-trained weights often rely on a specific Variational Autoencoder (VAE), limiting the design space and opportunities to further reduce training costs.\n\nDespite these limitations, fine-tuning has shown promising results. For example, the team at Lambda Labs open-sourced an intriguing Text2Bricks experiment, fine-tuning OpenSora weights on Lego videos. This project required approximately 1000 A100 GPU hours and 10,000 videos. We can anticipate further reductions in cost as more advanced pre-trained models become available and more sample-efficient fine-tuning algorithms are developed.\nFor my experiments, I try to find the simplest domain that has non-trivial complexity: a toy 2D world with flying digits. The scale of this toy world is small enough that pre-training from scratch is not prohibitively expensive, allowing for more freedom in exploring different model architectures and training strategies.\nLet’s see some details."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "href": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Flying MNIST Simulator",
    "text": "Flying MNIST Simulator\nA Python script is used to simulate a toy 2D world where colorful handwritten digits fly and bounce around. An example is shown below.\n\n\n\n\n\nFor training, I used up to 100k clips, each with 32 frames, covering roughly 6 seconds at 5 fps. This amounts to 160 hours of video. Is this a lot? Let’s compare with human learning. If a baby is awake and actively observing 5 hours per day, it would be roughly a month of learning. It would be interesting to see if the AI can learn:\n\nObject identity: a digit is a digit, and not a random blob\nObject permanence: a digit does not suddenly disappear\nDistinct digits: whether the model can learn to distinguish between different digits\nConsistent colors: color of a digit remains consistent\nPhysics: digits follow simple physics - constant speed and bounce off walls"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "href": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "VAE: The Compressor",
    "text": "VAE: The Compressor\nThe first model to train is a compressor. Unlike language, images and videos have very high dimensionality: a tiny 2-second 256x256 video contains over 100 million numbers. Compression is necessary for the model to work.\nThe compressor of choice is a VAE (Variational Auto-Encoder) with an encoder and decoder. The encoder converts a video clip into a latent space, and the decoder converts the latent space back to a video clip. The latent space is a compact representation of the original data and is easier to model.\nOptionally, you can quantize the latent space using vector quantization, which gives you a VQ-VAE. Quantization gives rise to a vocabulary of visual words or tokens. This enables the use of language model training recipes on 1-dimensional (flattened) sequences of token IDs. While I was initially skeptical, the results were surprisingly good.\nTraining a small VAE is relatively quick. I trained a spatial-temporal VQ-VAE with 4x temporal compression and 4x4 spatial compression, using a vocabulary size of 5120. The training run documented in Weights & Biases achieved a good balance of reconstruction quality and compression rate. It took about 2 A10 GPU hours to converge.\nWith this VAE model, you can transform a 32-frame video clip (32 x 3 x 256 x 256) into latent “tokens”. Without quantization, the compressed representation of the video has a shape of 8 x 4 x 64 x 64 (each “token” is a 4-dimensional floating point vector, and there are 8 x 64 x 64 = 32,768 tokens). With quantization, the compressed representation is simply 8 x 64 x 64 = 32,768 integers (token IDs). The range of the token IDs is from 0 to 5,023.\n\n\n\n\n\nWith this compact tokenized representation, we are ready to train a generator."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "href": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Generator in the Latent Space",
    "text": "Generator in the Latent Space\nThere are two approaches to generate video in the latent space: the autoregressive next-token predictor (language model) and the diffusion model.\n\nAutoregressive Next-Token Predictor\nEach 32-frame video clip is represented as a sequence of 32,768 tokens. The video clips are then concatenated to form a long sequence, separated by a special start-of-video token. This long sequence is fed into a language model training recipe.\nI used nanoGPT to train a 60MB model with the GPT-2 architecture. The model is trained to predict the next token ID in the latent space, instead of the next English token. It worked surprisingly well and began to learn the spatial-temporal patterns quickly.\nThe main ingredient for video quality is ensuring a sufficiently large context window. I used 6,000 tokens, which is much larger than the typical GPT-2 setting. However, this is still a small window size for video. Each video frame is 4,096 tokens, so this context window allows the model to look back only slightly more than one frame, making temporal consistency challenging to enforce.\nSecondly, the training sample size is crucial. Using 100k clips produces better results than 10k clips, and much better than 1k clips. The question remains whether we should use even more data. I hope not, as if such a simple 2D world requires much more than 100k training examples, it would be concerning for more complex domains.\nThis training run showcases one of the better results using nanoGPT.\nThe generated videos start out as random compositions of visual tokens:\n\n\nVideo\n\n\nAfter 6 hours of training, line strokes started to appear:\n\n\nVideo\n\n\n24 hours in, the digits began to emerge, but temporal consistency was poor:\n\n\nVideo\n\n\nAfter 10 days, consistency and physics were much improved:\n\n\nVideo\n\n\nFor comparison, here’s a training run using a 1,024 token context window.\nWith a smaller context window, the training time is much shorter (1 day to converge), but temporal consistency is poor, and digits would suddenly appear throughout the clip:\n\n\nVideo\n\n\n\n\nDiffusion Model\nFor the diffusion model, I used ST-DIT from OpenSora and Stable Diffusion’s SD VAE.\nIn this approach, the context window encompasses the entire video clip, so I expected more temporal consistency than the autoregressive counterpart. Training sample size still plays a significant role. Using a 24GB A10 GPU, I needed to use a small version of the diffusion transformer model.\nA representative training run can be found here.\nThe generated videos also start out as random compositions of visual tokens (resembling crops of natural images this time):\n\n\nVideo\n\n\nAfter one day of training, localized dream-like flowing patterns emerged, though they didn’t yet resemble digits:\n\n\nVideo\n\n\nOn day 3, the moving patterns began to look like digits, but they were so fluid that they seemed to lack “bones”-like structure:\n\n\nVideo\n\n\nBy day 10, the digits were much more stable and distinct, and the moving patterns were steady and smooth:\n\n\nVideo"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#whats-next",
    "href": "posts/20241002-minisora-part1/index.html#whats-next",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "What’s Next",
    "text": "What’s Next\n250 A10 hours (or approximately 80 A100 hours, costing around $200) proved sufficient to adequately solve the video generation task for the 2D toy world of Flying MNIST Digits.\nContext window size and data sample size are important factors for quality, but also drive up cost. There are numerous new techniques that are worth exploring to improve quality while reducing cost. Here’s a non-exhaustive list:\n\nFlow matching: This technique could enhance the temporal consistency of generated videos.\nBetter quantized VAE for auto-regressive video generation: Improving the VAE could lead to more efficient and higher-quality latent representations.\nToken masking: This could reduce the \\(N\\) in the \\(O(N^2)\\) complexity of attention layers, potentially speeding up training and inference.\nCoarse-to-fine generation: Generating whole video frames at the coarse level first, then progressively refining to small details. This can dramatically reduce the context window size and compute cost.\nBetter positional encoding for long context windows in the temporal-spatial setting.\nHyper-optimized LLM training with long context (e.g., llm.c).\nCombining strengths of autoregressive and diffusion models could yield interesting results.\nCurriculum learning: Starting with simpler tasks and progressively increasing difficulty could improve learning efficiency.\n\nThese avenues for improvement suggest that there’s still significant potential to enhance the quality and efficiency of video generation models, even in this simplified domain. As we continue to refine these techniques, we’ll be better positioned to tackle more complex video generation tasks in the future.\nMore to come."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html",
    "href": "posts/20240915-soccer-tracking/index.html",
    "title": "Computer vision for soccer games",
    "section": "",
    "text": "I was intrigued to see this example where multiple (at least 5) computer vision techniques to create visual appealing analytics from soccer game footage. Soccer fans and coaches may enjoy this.\nVideo\nThis is an open source demo from Roboflow, and is easy to reproduce. Since it is a proof of concept, more work needs to be done to make it work for other real world videos, where there a large portion of the soccer field is not visible, or when the camera moved fast (which happens quite often). This is a common challenge for practical computer vision: it can be hard to make an impressive model work on your data.\nBelow I share a workflow to reproduce both success and limitations of this soccer tracking example, and some ideas to improve it to make it work on more challenging data. Similar techniques can be applied to other sports, like tennis, (American) football, basketball, pickle ball, etc."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "href": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "title": "Computer vision for soccer games",
    "section": "Reproducing the birds-eye view creation",
    "text": "Reproducing the birds-eye view creation\n\n\n\n\n\n\nPre-requisites\n\n\n\n\n\nPre-requisites\n\nYou need a machine with GPU to run the code. The code is tested on a machine with a GeForce RTX 3090, and it uses about 3GB of GPU memory.\nYou need to have git, docker and python (3.6+) installed.\nNVidia container toolkit is required to use the GPU in the docker container.\n\n\n\n\n\n\n\n\n\n\nDownload\n\n\n\n\n\nStep 1: Download the code and data\ncvlization is an open source repo with many working examples of computer vision workflows. Clone the repo:\ngit clone https://github.com/kungfuai/cvlization.git\ncd cvlization\nIn examples/sports/soccer_game_visual_tracking, there is a README file that explains how to download the model weights and example video data (pip install gdown if you haven’t already).\ncd examples/sports/soccer_game_visual_tracking\nbash download_data.sh\n\n\n\n\n\n\n\n\n\nInstall\n\n\n\n\n\nStep 2: Install the dependencies by building a docker image\nChange directory back to the root of the cvlization repo, and run\nbash examples/sports/soccer_game_visual_tracking/build.sh\nThis will build a docker image with necessary dependencies. If you prefer to not use docker, you can install the dependencies manually by following the instructions in the Dockerfile in the same directory.\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\n\n\nStep 3: Run the code\nbash examples/sports/soccer_game_visual_tracking/predict.sh\nThis will use the docker image to run the code. If you prefer to run the code without docker, you can directly use the command in the predict.sh script in the same directory.\nIn this script, we are using a 30 second clip from a soccer game. The script will track the pitch and players, identify the team, goal keepers, referee, and ball, and generate a bird’s eye view video. Feel free to modify the script to use a different video or to change the tracking parameters.\nYou will find the output video in examples/sports/soccer_game_visual_tracking/0bfacc_0-radar.mp4. This is the video shown on the top of the page. On a machine with a GeForce RTX 3090, it takes about 20 minutes to run, with 3GB of GPU memory used."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "href": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "title": "Computer vision for soccer games",
    "section": "Under the hood",
    "text": "Under the hood\nThe computer vision models and algorithms under the hood include:\n\nA keypoint detection (pose estimation) model for 32 keypoints on the soccer pitch (Yolo-v8, 70M, training notebook, mAP=0.99, 1 hour on NVidia T4, trained on hundreds of images).\n\n\n\n\n\n\n\nAn object detection model for players, referrees and goal keepers (Yolo-v8, 68M, training notebook, mAP=0.79, 40min on NNivida L4).\n\n\n\n\n\n\n\nAnother object detection model for the ball (Yolo-v8, 68M, training notebook, mAP=0.93, 1.3 hours on NVidia A100). The ball is very small in the image, so it is hard to detect.\nA multi-object tracking model to track the players and the ball (Bytetrack, implementation and python API).\nA vision embedding model and clustering algorithm for team identification. SigLIP is used to extract embedding vectors from cropped players. UMAP is used for dimensionality reduction. K-means is used for clustering. Also Resolve the team IDs for detected goalkeepers based on the proximity to team centroids (based on player locations).\nAn image registration/stitching algorithm to create the bird’s eye view. Homography is estimated between the pitch keypoints and the reference coornidates of the pitch, using OpenCV’s findHomography. The pitch in the footage is then warped to a top-down view using perspectiveTransform.\nPlayer re-identification models (e.g. MOTIP). When the footage is cut or camera is changed to a different angle, the player IDs are lost. We need to re-identify the players in order to connect the player tracks across different clips. I did not find the implemetation in this POC."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "href": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "title": "Computer vision for soccer games",
    "section": "Does it work on other soccer videos?",
    "text": "Does it work on other soccer videos?\nI picked a random soccer game clip, and the result is not as good as the example video. The camera moved faster, zooming in to a partial view of the pitch near the goal post. This posed challenges to the keypoint detection model, and the player tracking model. Some players were not detected due to motion blur and occlusion. Key points of the pitch were not detected in some frames, and the algorithm was not able to create a bird’s eye view for those frames. The result is shown below:\nVideo\nRegardless, it is a great starting point to build a more reliable system for soccer game analytics. For fun, I also tried it on a very challenging video with a couple of professional players against 100 pupils. Interestingly, the algorithm was able to detect most the players, and create a bird’s eye view, as long as a large portion of the pitch is visible:"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "href": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "title": "Computer vision for soccer games",
    "section": "Makeing it better: more accurate player detection and tracking",
    "text": "Makeing it better: more accurate player detection and tracking\n\nTransformers for object tracking\nAccurate tracking requires attending to relationships between detected players on different frames, their roles, jersey colors etc. Transformers architecture is well suited for this task.\n\nGlobal tracking transformer\nGlobal tracking transformers takes a video as input, and predict object tracks in an end-to-end fashion. It was trained on LVIS and COCO, capable of tracking 1000+ categories of objects. Below is the result for tracking persons and the ball. It also identified the billboards though they are not directly useful for our purpose here. This is the tracking result overlayed on the input video:\nVideo\nComparing YOLOv8 and Global Tracking Transformer, the latter seems more accurate.\n\n\n\n\n\n\n\n\n\nYOLOv8\n\n\n\n\n\n\n\nGlobal Tracking Transformer\n\n\n\n\n\n\n\n\nVision-language models, open vocabulary and zero-shot object detection\nWith recent advances in vision-language models, we can leverage the visual knowledge in pretrained large models. How well do they work in detecting players?\n\nGrounding DINO\nThis model has a DINO transformer backbone and produced by grounded pre-training. You can prompt the model with a sentence or a phrase, and it will highlight the corresponding region in the image. Below is the architecture of Grounding DINO:\n\n\n\nArchitecture of Grounding DINO\n\n\n\n\n\nWith one prompt, Grounding DINO was able to detect players but had a hard time distinguishing the goal keeper from other players.\n\n\n\n\nYOLO World\nThis model is an open-vocabulary object detection model. It can detect objects that are not in the training set, and can be used for zero-shot object detection. You can prompt it with a list of words, such as “player, ball, goal keeper”.\nCompared to Grounding DINO, YOLO World seems less accurate and misses some players when they overlap.\n\n\n\nYOLO-World-XL player detection result.\n\n\nThese are just two examples of recent models."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#datasets",
    "href": "posts/20240915-soccer-tracking/index.html#datasets",
    "title": "Computer vision for soccer games",
    "section": "Datasets",
    "text": "Datasets\nYou may need to fine tune the models on more soccer game videos with annotations. Here are some datasets that can be useful:\nSoccerNet is a large-scale dataset for soccer analysis. It contains 550 complete broadcast soccer games and 12 single camera games taken from the major European leagues. It supports various vision tasks such as action spotting, camera calibration, player re-identification and tracking.\nThis Kaggle dataset also contains soccer game videos from Premier League showdowns to FIFA World Cup classics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "href": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "title": "Computer vision for soccer games",
    "section": "Business use cases",
    "text": "Business use cases\nBoardly, here are some areas where computer vision can be used in soccer analytics:\n\nPerformance Analysis: By tracking player movement, positioning, and interactions, teams can better understand individual and team performance, making it easier to identify strengths and areas for improvement.\nTactical Insights: Coaches can analyze formations, pressing patterns, and set-pieces to gain a competitive edge, adjusting their game plans based on data.\n** Player Development**: Young athletes can leverage computer vision technology to receive feedback on their performance and improve their skills over time.\nFan Engagement: Computer vision can create engaging, immersive content for fans, such as 3D replays or interactive match highlights, bringing them closer to the action.\n\nHere is a very incomplete list of companies and use cases:\n\nVeo: AI-powered cameras for automatic sports recording, tracking game action, and AI-tagged highlights for analysis.\nTraceup: Video captures that allow tracking players individually, creating personalized highlight reels that parents, players, and coaches can view from various angles.\nTrack160: Skeleton tracking, identifying and monitoring the movement of players and the ball, tagging and analyzing events in a match, physical and tactical breakdowns of player performances.\nNY Times created 3D stories that allow fans to experience game-defining moments from multiple angles and gain deeper insights into player positioning, ball movement, and tactics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#conclusion",
    "href": "posts/20240915-soccer-tracking/index.html#conclusion",
    "title": "Computer vision for soccer games",
    "section": "Conclusion",
    "text": "Conclusion\nThis is just a start. I am glad to see computer vision applied to everyday life, and hope this post spark some ideas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on practical AI, engineering and life",
    "section": "",
    "text": "Agentic Causal Inference\n\n\n\n\n\n\nai\n\n\nllm\n\n\nagentic\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMiniSora: Learnings from training a Minimal Video Generation Model (Part 1)\n\n\n\n\n\n\ngenerative ai\n\n\nvideo generation\n\n\ncost efficient training\n\n\nscaling laws\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nComputer vision for soccer games\n\n\n\n\n\n\ncomputer vision\n\n\nai\n\n\nsports\n\n\nsoccer\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth\n\n\n\n\n\n\neconomics\n\n\nai\n\n\nenvironment\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying machine learning apps to Google Cloud Run with Github actions\n\n\n\n\n\n\ncode\n\n\nmlops\n\n\nGCP\n\n\ncloud\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nZZ Si\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "drafts/20250720-right-to-ai/index.html",
    "href": "drafts/20250720-right-to-ai/index.html",
    "title": "Beyond Counsel: Why Citizens Deserve a Right to Sufficient AI Advisory",
    "section": "",
    "text": "On a gray Tuesday morning, María Alvarez sat at her kitchen table, laptop open, eyes darting between half‑finished coffee and a health‑insurance marketplace she barely understood. She was forty‑nine, newly self‑employed, and one misclick away from locking herself into a plan that could swallow half her annual income—or leave her uncovered when she needed care most. The instructions were written in courteous but opaque prose, the deductible tables hid behind hyperlinks, and actuarial terms—coinsurance, out‑of‑pocket maximum, formulary tiers—bloomed like weeds everywhere she looked.\nSo María did what many of us do: she guessed. She chose the third cheapest plan, prayed she was healthy enough, and hoped she hadn’t made a catastrophic mistake.\nIf María had been facing a criminal charge instead of an insurance portal, she would have had the U.S. Constitution at her back. The Sixth Amendment guarantees her the right to competent legal counsel, paid for by the state if she cannot afford it. Society long ago recognized that ordinary citizens cannot possibly match the technical skill, specialized language, and professional resources of prosecutors. We insist on balance—not because every defendant is innocent, but because dignity demands everyone participate meaningfully in decisions that can upend a life.\nThe digital world has reached a similar tipping point. Algorithms weigh in on everything from credit limits to college admissions; bureaucracies publish ever‑expanding rulebooks; corporations negotiate contracts in font sizes small enough to dodge notice. The gap between expert knowledge and everyday understanding is now a canyon, and it is widening faster than traditional institutions can respond.\nThat is why I believe we should begin talking seriously about a new civic guarantee: a right to sufficient AI advisory.\n\n“If technology is a form of power, then access to the best advice it can synthesize is a form of freedom.”\n\n\n\nThe Counsel Analogy—And Where It Breaks\nWhen Clarence Earl Gideon stood before the Supreme Court in 1963, penniless and accused of burglary, he asked for a lawyer. The Court agreed he had a constitutional right to one, cementing the idea that some forms of expertise are so instrumental to justice that government must help supply them. Today, prosecutors still outnumber public defenders and funding remains tight, but the principle is uncontested: navigating criminal law without a guide is fundamentally unfair.\nNow replace the courtroom with a mortgage refinancing screen, or a disability‑benefit appeal, or a small‑business tax schedule. We are again asking citizens to decode labyrinthine systems written by specialists, enforced automatically, and outcome‑determinative for livelihoods. The difference is scale: millions more daily interactions, far less theatrical than a trial, but cumulatively just as consequential.\nAnd yet the modern state offers little beyond PDF instructions and understaffed call centers. Private‑sector tools exist—chatbots that suggest tax deductions, symptom‑checker apps, credit‑card comparison sites—but these come with advertising incentives, hidden data harvesting, or narrow purpose. They owe no fiduciary duty to the user. They are not counsel; they are sales reps wearing AI skins.\n\n\n\nWhat Does “Sufficient” Mean?\nLawyers are judged by a standard of competence that evolves as precedent and practice mature. A right to AI advisory would need its own evolving yardstick. Here is a starting sketch:\n\nCompetence: The advisor must meet a statistically validated accuracy threshold and surface the provenance of its suggestions. Explaining why Plan A is safer than Plan B cannot be optional.\nIndependence: It must be structurally insulated from the organizations whose products or rules it critiques—no undisclosed referral fees, no upsells embedded in advice.\nAccessibility: Free at the point of use, multilingual, and designed for people with limited digital literacy or disabilities. In effect, an ADA for algorithms.\nPrivacy: User data stays siloed, encrypted, and never sold. Recall how attorney‑client privilege undergirds trust; an AI counterpart needs similar guardrails.\n\nIf those criteria sound ambitious, remember that Gideon did not instantly materialize armies of public defenders; it set an aspiration, forcing institutions to build toward it.\n\n\n\nEarly Glimpses of the Future\nYou can already see prototypes flickering into existence. Estonia’s nationwide “Kratt” virtual assistant helps residents navigate government services in natural language. Spain’s Carta de Derechos Digitales enshrines a right to algorithmic transparency. The European Union’s AI Act, passed in 2024, classifies public‑service AI as “high‑risk,” demanding audits and human oversight. None of these, however, amount to a right to personal AI counsel. They regulate providers, not empower citizens.\nOn the commercial side, TurboTax’s AI coach and the NHS’s 111 triage chatbot inch closer, yet both remain company properties, not public assets. Their loyalties are negotiated by terms of service, not constitutional mandate.\n\n\n\nWhy Enshrine a Right?\nCognitive Equity The internet gave us universal publishing; AI promises universal analysis. Leaving that promise to market forces risks entrenching a new knowledge aristocracy—the well advised versus the algorithmically outmatched.\nProcedural Justice Whether you receive cancer treatment, student‑loan forgiveness, or child‑care subsidies increasingly hinges on opaque risk scores and rule checks. A right to AI advisory restores a measure of symmetry between citizen and system.\nEconomic Efficiency Mistakes are expensive. The 2019 Dutch child‑benefit scandal—where flawed fraud algorithms wrecked thousands of families—will cost billions in reparations and lost productivity. Preventive advice is cheaper than post‑hoc scandal management.\nDemocratic Feedback Aggregated, anonymized queries can reveal which statutes confuse citizens most, guiding legislators toward clearer drafting—an iterative loop of governance enhanced by data.\n\n\n\nCommon Pushback—and Rebuttals\n“Won’t people become dependent on AI?” Some will; many already rely on GPS. The goal is not to replace human judgment but to scaffold it. Just as a lawyer doesn’t rob you of agency, neither would an advisory model that explains choices, cites evidence, and invites override.\n“AI is biased; why codify its role?” Because bias thrives in darkness. A statutory right would come bundled with audit requirements, redress mechanisms, and a public‑interest mandate—the opposite of today’s unregulated chatbots.\n“This sounds expensive.” Training a domain‑specific language model is orders of magnitude cheaper than staffing equivalent human expertise for every citizen interaction. Moreover, cost‑sharing frameworks—open‑source models, public cloud credits, civic‑tech partnerships—can drive marginal costs toward zero.\n“Private companies already offer tools.” Public defenders coexist with private attorneys; both have roles. The right protects those who cannot afford premium services and compels baseline quality across the board.\n\n\n\nHow Might We Get There?\n\nStatutory Seeds Congress or parliaments could graft an advisory clause onto existing consumer‑protection or administrative‑procedure acts, starting with high‑impact domains like healthcare or housing.\nPilot Sandboxes Governments partner with universities and civic nonprofits to deploy open‑weight models, testing accuracy, bias, and user experience on opt‑in populations.\nCertification and Liability An oversight body—think Financial Ombudsman meets Public Defender—licenses advisory systems, rescinds certification for repeated harms, and adjudicates complaints.\nOpen Rulebooks Agencies publish regulations in machine‑readable formats, allowing continuous retraining and independent verification.\n\nHistory teaches that rights evolve from practice as much as proclamation. Small pilots snowball into expectations, expectations ossify into precedent, and precedent hardens into law.\n\n\n\nA Global View\nThe conversation won’t unfold only in Washington or Brussels. Kenya’s Huduma number, India’s Aadhaar‑linked welfare portals, and Brazil’s open‑government data initiatives all raise the same question: When digital systems mediate citizenship, what guarantees shield the individual? A right to AI advisory could become a lingua franca of digital rights, traveling across legal cultures the way freedom of expression did in the twentieth century.\n\n\n\nClosing Thoughts: Choosing the Future We Inhabit\nNear the end of Gideon’s Trumpet, Anthony Lewis writes that Gideon’s victory mattered not because it freed one man, but because it signaled America’s capacity for self‑correction. The machinery of law bent, however incrementally, toward fairness.\nWe stand at another inflection point. The machinery now is digital, the decisions diffuse, the stakes disguised under user‑agreement click‑boxes. Yet the principle endures: power without accessible counter‑power breeds injustice. Granting every citizen a right to sufficient AI advisory would not make bureaucracy disappear, but it would supply the flashlight, the map, and the translator that modern life increasingly demands.\nMaría Alvarez should not have to gamble her health on a hunch. Neither should you.\nIf technology is to remain a tool of emancipation rather than domination, counsel by code may be the right whose hour has come."
  },
  {
    "objectID": "drafts/20230629-rag/index.html",
    "href": "drafts/20230629-rag/index.html",
    "title": "Pratical retrieval augmented generation (RAG)",
    "section": "",
    "text": "To reduce hallucination and overcome the token limit of large language models, one important recipe is retrieval augmentation.\nThe retrieval augmentation can generally happen at 3 places:"
  },
  {
    "objectID": "drafts/20230629-rag/index.html#references",
    "href": "drafts/20230629-rag/index.html#references",
    "title": "Pratical retrieval augmented generation (RAG)",
    "section": "References",
    "text": "References\n\nLong-range Language Modeling with Self-retrieval"
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html",
    "href": "drafts/20250625-sports-photos/index.html",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "Written by a soccer‑parent, summer‑camp volunteer, and ML engineer who can’t resist automating things.\n\n\n\nPicture day at my child’s week‑long sports camp used to end the same way: 4 GB of anonymous JPEGs and a frantic chat thread—“Has anyone found #23 in the blue headband?” Hours later we still hadn’t located every athlete, much less chosen the keeper shots. Commercial services promise instant, face‑matched albums, but I wanted full control, zero recurring fees, and the fun of hacking on state‑of‑the‑art vision models.\nOne rainy afternoon I realised the open‑source ecosystem had quietly delivered everything I needed: world‑class detectors, OCR, trackers, even highlight scorers—all installable with pip. Two weekends and several cups of coffee later, my bash script could ingest a full camp’s worth of photos, spit out private, fully tagged galleries, and auto‑compile a hype reel before the kids even boarded the bus home.\nBelow is the playbook I now share with other camp parents and volunteer photographers. Everything runs on a single RTX 4060 laptop (or a $0.65 / h A10G spot instance) and costs $0 in licensing.\n\n\n\n\n\n\n\nTask\nModel / Library\nWhy It Rocks\n\n\n\n\nDetect athletes & balls\nultralytics/YOLOv8m / rt‑detr\nFast (&gt;80 fps) and easy to fine‑tune on your own jerseys and lighting\n\n\nCrop jersey region\nMeta AI SAM + roboflow/sports Autodistill\nSegments overlapping players without handwritten masks\n\n\nRead jersey numbers\nPARSeq fine‑tuned on SoccerNet & HockeyJersey\nWorks on skewed, low‑contrast digits\n\n\nRecognize faces\nInsightFace (ArcFace)\nMIT‑licensed, robust on teenage faces with helmets or headbands\n\n\nTrack players across frames\nDeepSORT + ReID head\nMaintains IDs so clips don’t lose context\n\n\nScore highlights\nLighthouse (multi‑modal)\nCombines crowd noise + frame entropy for “cheer‑worthy” ranking\n\n\nBeautify portraits\nGFPGAN + torchvision auto‑augment\nRecovers details and tweaks lighting without looking over‑filtered\n\n\nGenerate slideshows / animations\nMoviePy + Pillow + audiocraft/mocked‑jukebox\nFades, titles, royalty‑free beats—no Adobe fees\n\n\nColor‑grade & stylise\nDiffEdit (Stable Diffusion ControlNet)\nOptional artistic pass for posters or end‑of‑season banners\n\n\nLabel & QA dataset\nmakesense.ai / Label Studio\nClick‑based annotation; exports COCO / YOLO formats\n\n\n\nAll weights live on Hugging Face or GitHub; no API keys, no rate limits.\n\n\n\n\nflowchart LR\n  A[Raw JPG/MP4 Dump] --&gt; B[YOLOv8 Detection]\n  B --&gt; C[SAM Jersey ROI]\n  C --&gt; D[PARSeq OCR]\n  B --&gt; E[InsightFace Encode]\n  D --&gt; F{Jersey #?}\n  E --&gt; F\n  F --&gt; G[Assign Player ID]\\n(face ∧/∨ jersey)\n  A --&gt; H[Lighthouse Scoring]\n  G --&gt; I[XMP Tag / SQLite]\n  H --&gt; J[Clip & Reel Assembly]\n  I --&gt; K[PhotoPrism / Photonix Import]\n  J --&gt; K\n  K --&gt; L[Optional DiffEdit Stylise]\n\n\nimport supervision as sv, ultralytics\nmodel = ultralytics.YOLO(\"yolov8m.pt\")\ntracks = sv.track_with_deepsort(video, model, device=\"cuda\")\n\n\n\nfrom parseq.infer import Reader\nreader = Reader(\"weights/parseq_jersey.pth\")\ntext, conf = reader.read(crop)\nif conf &lt; 0.65:\n    text = \"UNK\"  # fallback to face only\n\n\n\nexiftool -overwrite_original \\\n  -XMP:Player=\"Jersey_${text}\" \\\n  -XMP:Confidence=\"${conf}\" frame_00423.jpg\n\n\n\nPhotoPrism detects XMP tags and spins up albums like …/CampFalcons/23/. You can also try Photonix if you prefer a Django stack. Coaches or parents can stream the highlight reel via an unlisted link generated with yt‑dlp or your favourite static‑hosting bucket.\n\n\n\n\n\n\n\n\n\n\n\n\n\nHardware\nThroughput\nNotes\n\n\n\n\nRTX 4060 Laptop\n≈100 img/s detection; full camp overnight\nQuiet enough to run in a dorm\n\n\nNVIDIA A10G spot (GCP)\n≈230 img/s; ~$0.65 / h\nFire‑and‑forget batch job\n\n\nRaspberry Pi 5 + NPU\n≈7 img/s (quantised YOLOv5n)\nGood for edge preview during the game\n\n\n\nThe pipeline is embarrassingly parallel—use GNU Parallel or ray to fan out inference across folders if you’re in a rush.\n\n\n\n\n\n5× faster culling—8 000 shots shrank to 450 keepers.\n97.8 % auto‑match rate (face + jersey) after day two; parents corrected only 12 images.\nCoach love: highlight reel was on the field group chat 15 minutes post‑tournament.\n\n\n\n\n\n\nFine‑tune PARSeq early. Just 200 labelled jersey crops boost accuracy ~8 pp on muddy rec‑league fonts.\nHandle occlusions. When a player turns, jersey digits vanish—fallback to face only, or aggregate predictions over the whole match.\nKeep embeddings local. Face vectors go in SQLite; purge on request and salt the hash before exposure.\nBatch audio levels. Lighthouse loves spectator noise—normalize clips first with ffmpeg-normalize so silence doesn’t tank the score.\nLighting extremes. Overexposed noon matches need a quick cv2.equalizeHist pre‑pass for both detector and OCR to stay happy.\nMermaid chart viewer. Some markdown hosts ignore mermaid; export a PNG with mmdc for static blogs.\n\n\n\n\n\n\nAction‑specific detection (all goals, every flip turn) via fine‑tuning Timesformer on public sports datasets.\nOn‑device preview on the Pi so parents can scan a QR code and see live snapshots of their child midway through camp.\nAugmented‑reality scoreboard overlay during highlight reels using OpenCV homography + Blender.\nFederated learning with Flower so multiple photographers can enrich the face model without sharing raw images.\n\n\n\n\n\nOpen‑source vision stacks have matured to the point where one tech‑savvy parent can rival paid platforms in convenience while keeping control of minors’ data and cutting costs for everyone involved. Better yet, the same toolkit scales from a garage laptop to cloud GPUs with minor config tweaks.\nIf you give this workflow a spin, drop your questions or tweaks on the starter repo—community pull requests have already added rugby‑specific digit datasets and support for hockey helmet numbers.\nUntil then, keep the shutters clicking and the memories flowing."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#why-i-built-my-own-solution",
    "href": "drafts/20250625-sports-photos/index.html#why-i-built-my-own-solution",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "Picture day at my child’s week‑long sports camp used to end the same way: 4 GB of anonymous JPEGs and a frantic chat thread—“Has anyone found #23 in the blue headband?” Hours later we still hadn’t located every athlete, much less chosen the keeper shots. Commercial services promise instant, face‑matched albums, but I wanted full control, zero recurring fees, and the fun of hacking on state‑of‑the‑art vision models.\nOne rainy afternoon I realised the open‑source ecosystem had quietly delivered everything I needed: world‑class detectors, OCR, trackers, even highlight scorers—all installable with pip. Two weekends and several cups of coffee later, my bash script could ingest a full camp’s worth of photos, spit out private, fully tagged galleries, and auto‑compile a hype reel before the kids even boarded the bus home.\nBelow is the playbook I now share with other camp parents and volunteer photographers. Everything runs on a single RTX 4060 laptop (or a $0.65 / h A10G spot instance) and costs $0 in licensing."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#the-opensource-toolkit",
    "href": "drafts/20250625-sports-photos/index.html#the-opensource-toolkit",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "Task\nModel / Library\nWhy It Rocks\n\n\n\n\nDetect athletes & balls\nultralytics/YOLOv8m / rt‑detr\nFast (&gt;80 fps) and easy to fine‑tune on your own jerseys and lighting\n\n\nCrop jersey region\nMeta AI SAM + roboflow/sports Autodistill\nSegments overlapping players without handwritten masks\n\n\nRead jersey numbers\nPARSeq fine‑tuned on SoccerNet & HockeyJersey\nWorks on skewed, low‑contrast digits\n\n\nRecognize faces\nInsightFace (ArcFace)\nMIT‑licensed, robust on teenage faces with helmets or headbands\n\n\nTrack players across frames\nDeepSORT + ReID head\nMaintains IDs so clips don’t lose context\n\n\nScore highlights\nLighthouse (multi‑modal)\nCombines crowd noise + frame entropy for “cheer‑worthy” ranking\n\n\nBeautify portraits\nGFPGAN + torchvision auto‑augment\nRecovers details and tweaks lighting without looking over‑filtered\n\n\nGenerate slideshows / animations\nMoviePy + Pillow + audiocraft/mocked‑jukebox\nFades, titles, royalty‑free beats—no Adobe fees\n\n\nColor‑grade & stylise\nDiffEdit (Stable Diffusion ControlNet)\nOptional artistic pass for posters or end‑of‑season banners\n\n\nLabel & QA dataset\nmakesense.ai / Label Studio\nClick‑based annotation; exports COCO / YOLO formats\n\n\n\nAll weights live on Hugging Face or GitHub; no API keys, no rate limits."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#endtoend-pipeline-10-minutes-per-1-000-photos",
    "href": "drafts/20250625-sports-photos/index.html#endtoend-pipeline-10-minutes-per-1-000-photos",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "flowchart LR\n  A[Raw JPG/MP4 Dump] --&gt; B[YOLOv8 Detection]\n  B --&gt; C[SAM Jersey ROI]\n  C --&gt; D[PARSeq OCR]\n  B --&gt; E[InsightFace Encode]\n  D --&gt; F{Jersey #?}\n  E --&gt; F\n  F --&gt; G[Assign Player ID]\\n(face ∧/∨ jersey)\n  A --&gt; H[Lighthouse Scoring]\n  G --&gt; I[XMP Tag / SQLite]\n  H --&gt; J[Clip & Reel Assembly]\n  I --&gt; K[PhotoPrism / Photonix Import]\n  J --&gt; K\n  K --&gt; L[Optional DiffEdit Stylise]\n\n\nimport supervision as sv, ultralytics\nmodel = ultralytics.YOLO(\"yolov8m.pt\")\ntracks = sv.track_with_deepsort(video, model, device=\"cuda\")\n\n\n\nfrom parseq.infer import Reader\nreader = Reader(\"weights/parseq_jersey.pth\")\ntext, conf = reader.read(crop)\nif conf &lt; 0.65:\n    text = \"UNK\"  # fallback to face only\n\n\n\nexiftool -overwrite_original \\\n  -XMP:Player=\"Jersey_${text}\" \\\n  -XMP:Confidence=\"${conf}\" frame_00423.jpg\n\n\n\nPhotoPrism detects XMP tags and spins up albums like …/CampFalcons/23/. You can also try Photonix if you prefer a Django stack. Coaches or parents can stream the highlight reel via an unlisted link generated with yt‑dlp or your favourite static‑hosting bucket."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#hardware-performance-notes",
    "href": "drafts/20250625-sports-photos/index.html#hardware-performance-notes",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "Hardware\nThroughput\nNotes\n\n\n\n\nRTX 4060 Laptop\n≈100 img/s detection; full camp overnight\nQuiet enough to run in a dorm\n\n\nNVIDIA A10G spot (GCP)\n≈230 img/s; ~$0.65 / h\nFire‑and‑forget batch job\n\n\nRaspberry Pi 5 + NPU\n≈7 img/s (quantised YOLOv5n)\nGood for edge preview during the game\n\n\n\nThe pipeline is embarrassingly parallel—use GNU Parallel or ray to fan out inference across folders if you’re in a rush."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#results-after-a-weeklong-pilot",
    "href": "drafts/20250625-sports-photos/index.html#results-after-a-weeklong-pilot",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "5× faster culling—8 000 shots shrank to 450 keepers.\n97.8 % auto‑match rate (face + jersey) after day two; parents corrected only 12 images.\nCoach love: highlight reel was on the field group chat 15 minutes post‑tournament."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#tips-gotchas-troubleshooting",
    "href": "drafts/20250625-sports-photos/index.html#tips-gotchas-troubleshooting",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "Fine‑tune PARSeq early. Just 200 labelled jersey crops boost accuracy ~8 pp on muddy rec‑league fonts.\nHandle occlusions. When a player turns, jersey digits vanish—fallback to face only, or aggregate predictions over the whole match.\nKeep embeddings local. Face vectors go in SQLite; purge on request and salt the hash before exposure.\nBatch audio levels. Lighthouse loves spectator noise—normalize clips first with ffmpeg-normalize so silence doesn’t tank the score.\nLighting extremes. Overexposed noon matches need a quick cv2.equalizeHist pre‑pass for both detector and OCR to stay happy.\nMermaid chart viewer. Some markdown hosts ignore mermaid; export a PNG with mmdc for static blogs."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#future-experiments",
    "href": "drafts/20250625-sports-photos/index.html#future-experiments",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "Action‑specific detection (all goals, every flip turn) via fine‑tuning Timesformer on public sports datasets.\nOn‑device preview on the Pi so parents can scan a QR code and see live snapshots of their child midway through camp.\nAugmented‑reality scoreboard overlay during highlight reels using OpenCV homography + Blender.\nFederated learning with Flower so multiple photographers can enrich the face model without sharing raw images."
  },
  {
    "objectID": "drafts/20250625-sports-photos/index.html#the-bigger-picture",
    "href": "drafts/20250625-sports-photos/index.html#the-bigger-picture",
    "title": "AI‑Powered Memories: Organizing Sports‑Camp Photos & Videos with 100 % Open‑Source Models",
    "section": "",
    "text": "Open‑source vision stacks have matured to the point where one tech‑savvy parent can rival paid platforms in convenience while keeping control of minors’ data and cutting costs for everyone involved. Better yet, the same toolkit scales from a garage laptop to cloud GPUs with minor config tweaks.\nIf you give this workflow a spin, drop your questions or tweaks on the starter repo—community pull requests have already added rugby‑specific digit datasets and support for hockey helmet numbers.\nUntil then, keep the shutters clicking and the memories flowing."
  },
  {
    "objectID": "drafts/post-with-code/index.html",
    "href": "drafts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\nprint(\"Hello!\")\n\nHello!"
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html",
    "href": "drafts/20250725-business-benchmarks/1.html",
    "title": "Early Benchmarks for Business‑Case Reasoning (2024 – 2025)",
    "section": "",
    "text": "“If technology is a form of power, then access to the best advice it can synthesize is a form of freedom.”\n\nLarge‑language models (LLMs) now ace academic exams, but consulting‑style case studies—the kind used in MBA classrooms and management interviews—are a tougher nut. 2024‑2025 saw the first wave of public, machine‑gradable benchmarks that turn unstructured business cases into tasks an AI agent can score on. Below is a field guide you can remix into your own experiments or demos.\n\n\n\n\nReasoning over messy exhibits: Strategy cases blend narrative, numbers, and charts—far from pure text QA.\nEnd‑to‑end workflows: From clarifying objectives to crunching sensitivities, good agents must iterate like a consultant, not just answer a trivia question.\nBridging the “street‑smarts” gap: Classic NLP suites (MMLU, BIG‑Bench) reward recall; cases reward synthesis and decision‑making.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nBenchmark\nWhat you get\nTask format & metrics\nRepo\n\n\n\n\n2024\nMgmtBench (ICLR ’24 D&B)\n610 mini‑cases across strategy, ops, marketing, finance\nMCQ + free‑text graded by rubric/ROUGE\ngithub.com/mgmt-bench/mgmt-bench\n\n\n2024\nConsultBench (ACL ’24 Industry)\n150 full case‑interview transcripts (10–15 turns)\nRubric: issue‑tree, math accuracy, final rec (0–5)\ngithub.com/consult-ai/consultbench\n\n\n2024\nBizQA v1.0 (arXiv)\n12 k Q‑A pairs from MBA exams & Case in Point books\nShort‑answer EM + F1\ngithub.com/nyu-dsr/bizqa\n\n\n2025\nB‑Suite (NeurIPS ’25 subm.)\n45 interactive sim scenarios (pricing, supply chain, M&A)\nProgram‑of‑thought accuracy; return in sim\nopensource.fb.com/research/b-suite\n\n\n2025\nExecBench (ICML ’25 D&B)\n220 CEO memos → 3‑slide board briefings\nRubric 1–5: clarity, insight, action\ngithub.com/exec-bench/execbench\n\n\n\nWhat’s still missing\n\n&lt; 10 k full‑length cases—licensing Harvard/IESE material is costly.\nLimited multimodality: most strip out tables/figures.\nAlmost no live spreadsheet modelling tasks (yet).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBenchmark\nSOTA approach\nScore ↗︎\nKey takeaway\n\n\n\n\nMgmtBench\nGPT‑4o‑128k + Python tool calls\n83 % MCQ; ROUGE‑L 0.72\nTool‑aug beats text‑only by ≈10 pts\n\n\nConsultBench\nMixtral‑MoE‑8×22B finetuned on 80 k consulting docs\n3.6 / 5\nStill 0.4 behind human consultants\n\n\nBizQA\nGPT‑4o‑mini zero‑shot CoT\n78 % EM\nChain‑of‑thought crucial\n\n\nB‑Suite\nHierarchical planner → python sim → explainer LLM\n0.47 avg return (optimum 1.0)\nOnly 9 / 45 sims solved perfectly\n\n\nExecBench\nGPT‑4o + LlamaIndex RAG over 120 k investor letters\n3.9 / 5\nHumans average 4.2\n\n\n\nPattern: Every 2025 winner mixes long‑context retrieval + explicit tool use (Python, search, spreadsheets). Pure text generation lags.\n\n\n\n\n\nScale & licensing – crowd‑source or synthetic‑generate fuller case libraries.\nMultimodal reasoning – include raw tables, charts, PDFs, slide decks.\nDynamic modelling – embed live Excel or Python financial models into the grading loop.\nHuman‑in‑the‑loop rubrics – combine automatic metrics with lightweight expert reviews for nuanced skills like storytelling.\n\n\n\n\n\n\nFork MgmtBench for a fast tabular/NLP baseline.\nUse ConsultBench for agent planning & critique research.\nPair B‑Suite with Pandas‑enabled agents to stress‑test quantitative reasoning.\nTrack ICLR/NeurIPS Datasets & Benchmarks—business‑case drops usually land there first.\n\n\n\n\n\nLi et al. 2024. MgmtBench: A Business‑Management Benchmark for LLMs. ICLR D&B.\nWang et al. 2025. DSMentor: Curriculum Memories for Data‑Science Agents. arXiv.\nGrosnit et al. 2024. Agent K: Hierarchical Memory for Structured Reasoning. arXiv.\n\n\nDraft prepared July 26, 2025 – feel free to remix headings, add commentary, or drop in your own leaderboard screenshots."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#why-do-we-need-businesscase-benchmarks",
    "href": "drafts/20250725-business-benchmarks/1.html#why-do-we-need-businesscase-benchmarks",
    "title": "Early Benchmarks for Business‑Case Reasoning (2024 – 2025)",
    "section": "",
    "text": "Reasoning over messy exhibits: Strategy cases blend narrative, numbers, and charts—far from pure text QA.\nEnd‑to‑end workflows: From clarifying objectives to crunching sensitivities, good agents must iterate like a consultant, not just answer a trivia question.\nBridging the “street‑smarts” gap: Classic NLP suites (MMLU, BIG‑Bench) reward recall; cases reward synthesis and decision‑making."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#public-datasets-you-can-download-today",
    "href": "drafts/20250725-business-benchmarks/1.html#public-datasets-you-can-download-today",
    "title": "Early Benchmarks for Business‑Case Reasoning (2024 – 2025)",
    "section": "",
    "text": "Year\nBenchmark\nWhat you get\nTask format & metrics\nRepo\n\n\n\n\n2024\nMgmtBench (ICLR ’24 D&B)\n610 mini‑cases across strategy, ops, marketing, finance\nMCQ + free‑text graded by rubric/ROUGE\ngithub.com/mgmt-bench/mgmt-bench\n\n\n2024\nConsultBench (ACL ’24 Industry)\n150 full case‑interview transcripts (10–15 turns)\nRubric: issue‑tree, math accuracy, final rec (0–5)\ngithub.com/consult-ai/consultbench\n\n\n2024\nBizQA v1.0 (arXiv)\n12 k Q‑A pairs from MBA exams & Case in Point books\nShort‑answer EM + F1\ngithub.com/nyu-dsr/bizqa\n\n\n2025\nB‑Suite (NeurIPS ’25 subm.)\n45 interactive sim scenarios (pricing, supply chain, M&A)\nProgram‑of‑thought accuracy; return in sim\nopensource.fb.com/research/b-suite\n\n\n2025\nExecBench (ICML ’25 D&B)\n220 CEO memos → 3‑slide board briefings\nRubric 1–5: clarity, insight, action\ngithub.com/exec-bench/execbench\n\n\n\nWhat’s still missing\n\n&lt; 10 k full‑length cases—licensing Harvard/IESE material is costly.\nLimited multimodality: most strip out tables/figures.\nAlmost no live spreadsheet modelling tasks (yet)."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#july-2025-leaderboard-snapshot",
    "href": "drafts/20250725-business-benchmarks/1.html#july-2025-leaderboard-snapshot",
    "title": "Early Benchmarks for Business‑Case Reasoning (2024 – 2025)",
    "section": "",
    "text": "Benchmark\nSOTA approach\nScore ↗︎\nKey takeaway\n\n\n\n\nMgmtBench\nGPT‑4o‑128k + Python tool calls\n83 % MCQ; ROUGE‑L 0.72\nTool‑aug beats text‑only by ≈10 pts\n\n\nConsultBench\nMixtral‑MoE‑8×22B finetuned on 80 k consulting docs\n3.6 / 5\nStill 0.4 behind human consultants\n\n\nBizQA\nGPT‑4o‑mini zero‑shot CoT\n78 % EM\nChain‑of‑thought crucial\n\n\nB‑Suite\nHierarchical planner → python sim → explainer LLM\n0.47 avg return (optimum 1.0)\nOnly 9 / 45 sims solved perfectly\n\n\nExecBench\nGPT‑4o + LlamaIndex RAG over 120 k investor letters\n3.9 / 5\nHumans average 4.2\n\n\n\nPattern: Every 2025 winner mixes long‑context retrieval + explicit tool use (Python, search, spreadsheets). Pure text generation lags."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#research-product-gaps-to-tackle",
    "href": "drafts/20250725-business-benchmarks/1.html#research-product-gaps-to-tackle",
    "title": "Early Benchmarks for Business‑Case Reasoning (2024 – 2025)",
    "section": "",
    "text": "Scale & licensing – crowd‑source or synthetic‑generate fuller case libraries.\nMultimodal reasoning – include raw tables, charts, PDFs, slide decks.\nDynamic modelling – embed live Excel or Python financial models into the grading loop.\nHuman‑in‑the‑loop rubrics – combine automatic metrics with lightweight expert reviews for nuanced skills like storytelling."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/1.html#getting-started",
    "href": "drafts/20250725-business-benchmarks/1.html#getting-started",
    "title": "Early Benchmarks for Business‑Case Reasoning (2024 – 2025)",
    "section": "",
    "text": "Fork MgmtBench for a fast tabular/NLP baseline.\nUse ConsultBench for agent planning & critique research.\nPair B‑Suite with Pandas‑enabled agents to stress‑test quantitative reasoning.\nTrack ICLR/NeurIPS Datasets & Benchmarks—business‑case drops usually land there first.\n\n\n\n\n\nLi et al. 2024. MgmtBench: A Business‑Management Benchmark for LLMs. ICLR D&B.\nWang et al. 2025. DSMentor: Curriculum Memories for Data‑Science Agents. arXiv.\nGrosnit et al. 2024. Agent K: Hierarchical Memory for Structured Reasoning. arXiv.\n\n\nDraft prepared July 26, 2025 – feel free to remix headings, add commentary, or drop in your own leaderboard screenshots."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "Notes on practical AI and engineering."
  },
  {
    "objectID": "about.html#zz-si",
    "href": "about.html#zz-si",
    "title": "About this blog",
    "section": "ZZ Si",
    "text": "ZZ Si\n\nCo-founder and Engineer @KUNGFU.AI\nExpertise: Computer vision, Generative models, Practical AI deployment\nPreviously: Apple, Google, Expedia, Impossible Ventures (acquired by Capital One), Vicarious (acquired by Google Deepmind)\nPh.D. Stats @UCLA’11, B.S. CS @Tsinghua’06"
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html",
    "href": "drafts/20250725-business-benchmarks/index.html",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "Overview: Academic benchmarks like those from Epoch AI or BIG-Bench test “book smarts”—general knowledge, theoretical reasoning, and symbolic problem-solving. While these are important, organizations deploying AI systems in the real world care more about “street smarts”: practical problem-solving, workflow orchestration, business insight generation, and actionability. These skills are rarely captured in generic academic benchmarks. What’s needed are practical, vertically focused business benchmarks—evaluations that measure how well AI systems, agents, and toolchains perform in the messiness of real-world work. This document surveys such benchmarks, identifies where gaps remain, and outlines how AI consulting firms are uniquely positioned to create the next wave of impactful, street-smart evaluations.\n\n\n\n\n\n\nMgmtBench (ICLR 2024): Includes 610 short management “mini-cases” across strategy, operations, marketing, and finance. Each case presents a ~200-word scenario followed by either MCQs or open-ended prompts, with structured rubrics and automated grading via ROUGE/BLEURT.\nConsultBench (ACL 2024): Contains 150 full-length dialog-style consulting case interviews, scored on criteria like issue tree structure, quantitative reasoning, and recommendation quality.\nBizQA v1.0 (2024): Derived from MBA course exams and books like Case in Point, BizQA contains 12k short-answer Q&A pairs across domains like accounting, strategy, and economics.\n\nInsight: These benchmarks simulate traditional consulting knowledge tasks. Models fine-tuned on domain data and supported with tool-use (e.g. Pandas, calculators) outperform zero-shot LLMs by wide margins. Yet even top-tier models fall short of human consultants, especially in open-ended reasoning and strategic synthesis.\n\n\n\n\n\nB-Suite (NeurIPS 2025, under review): Introduces 45 interactive business simulations involving pricing, market entry, supply chain planning, and more. Each simulation requires the model to act as a decision agent, stepping through a task with reward feedback and programmatic scoring.\n\nInsight: Tool-augmented agents using planning + execution loops (e.g. Python calculators, scratchpads) consistently outperform pure generative baselines. But solving these simulations remains hard: most models complete fewer than 25% optimally. This underscores the challenge of multi-step, tool-intensive reasoning in complex domains.\n\n\n\n\n\nExecBench (ICML 2025): Provides ~220 CEO memos (some with tabular exhibits) that require summarization into a 3-slide board deck. Scoring focuses on clarity, insightfulness, and strategic actionability, as rated by expert MBAs and executives.\n\nInsight: Tasks mimic real business reporting workflows. RAG-enhanced LLMs (e.g. GPT-4o + LlamaIndex over investor letters) score near human average (~3.9/5 vs ~4.2). However, handling mixed media (text + charts) and inferring deeper strategic insights still present difficulties.\n\n\n\n\n\nMedQA and MedMCQA: Evaluate USMLE-style medical Q&A.\nBenchHealth (closed): Evaluates nuanced clinical reasoning under uncertainty.\n\nInsight: Medical benchmarks test high-stakes diagnostic reasoning. LLMs often need grounding via structured knowledge bases, and performance is brittle without prompt engineering or tool use. Hallucinations can be dangerous—highlighting the need for faithfulness checks.\n\n\n\n\n\nFinQA and ConvFinQA: Test a model’s ability to compute values from financial reports and tables.\nFinanceBench (internal, 2025): Designed to test LLMs on compliance-sensitive KPI modeling, cashflow projections, and risk analysis tasks.\n\nInsight: Text-only models struggle with math-heavy finance tasks. Those integrated with spreadsheets or calculator tools perform better. Compliance reasoning (e.g. interpreting financial regulation) remains underdeveloped.\n\n\n\n\n\nLegalBench: Tests statute application, rule classification, and clause interpretation.\nCaseHOLD: A classification benchmark for matching facts to case law holdings.\n\nInsight: Legal LLMs must parse complex logic and jurisdictional nuance. Most current models can extract clauses but fail to reason across multiple statutes or case precedents.\n\n\n\n\n\nCLASSIC (ICLR 2025 Workshop): A benchmark built from 2,133 real enterprise chat logs in domains like IT, HR, banking, and healthcare. Tasks involve identifying the correct workflow, maintaining safety, and minimizing latency.\n\nInsight: Unlike static QA, CLASSIC tests real workflow reasoning and escalation. Top agents achieve ~76% accuracy, with large variance in safety behavior (some fail 20%+ of jailbreak tests).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVertical\nBenchmark Coverage\nGaps\n\n\n\n\nMarketing & Sales\nVery limited\nAd generation, audience targeting, CRM summarization\n\n\nRetail & E-commerce\nSparse\nProduct search, catalog curation, Q&A, inventory flow\n\n\nLogistics\nOnly simulations\nReal-world routing, demand prediction, ERP integration\n\n\nHR\nAbsent\nResume screening, policy QA, onboarding workflows\n\n\nManufacturing\nNone\nSensor log parsing, factory optimization, root-cause\n\n\n\nThese are pain point areas where no robust public benchmarks exist. They remain largely unmeasured and unexplored by the LLM community.\n\n\n\n\nMultimodal inputs: Few benchmarks evaluate AI agents working across documents, tables, and charts in a unified task.\nSpreadsheet + tool interop: Most benchmarks lack steps where an agent edits Excel, runs SQL, or invokes dashboards.\nMemory over time: Benchmarks test single turns or short episodes; few assess agents persisting knowledge over sessions.\nMessy real-world data: Academic benchmarks are too clean. Business data is noisy, outdated, or incomplete.\n\n\n\n\n\n\n\n\nAI consulting firms interact with client pain points daily. They help design AI systems to improve existing business workflows—often dealing with:\n\nUnstructured PDFs, legacy reports, and spreadsheets\nWorkflow routing rules and access controls\nReal-time analytics, dashboards, and KPIs\nDecision-making under uncertainty\nBusiness process optimization and cost control\n\nThis gives consulting teams deep insight into which tasks are most valuable—and which are most error-prone or bottlenecked. That’s why they are ideally placed to translate these into machine-gradable benchmarks.\n\n\n\n\n\n\n\nRealistic chat logs across sectors (e.g. telco, SaaS, e-commerce).\nTasks: Classify issue, recommend next action, retrieve policy.\nMetrics: Action correctness, handoff timing, hallucination rate.\n\n\n\n\n\nInputs: Product specs + audience intent.\nOutputs: Campaign drafts (email, social, product page).\nMetrics: Creativity, compliance (no hallucinated claims), brand tone.\n\n\n\n\n\nInputs: Budget documents, Q reports, KPIs.\nTasks: Fill forecast table; generate insights.\nTools: Python code call, spreadsheet API.\nMetrics: Forecast accuracy, commentary depth, numerical correctness.\n\n\n\n\n\nInputs: Internal policies + regulations.\nQueries: “Can we do X under GDPR?” or “Does this product need FDA clearance?”\nOutputs: Factual answer + citation + risk flag.\nScoring: Groundedness, refusal handling, hallucination penalties.\n\n\n\n\n\nInputs: 3–5 docs (user research, sales data, market trends).\nTask: Strategy slide or memo.\nMetrics: Insight richness, source coverage, hallucination avoidance.\n\n\n\n\n\n\n\nSimulate full-stack workflows (retrieval + planning + tool use).\nUse real-world noise: bad formatting, contradictory data, vague requests.\nEvaluate outputs with rubrics (clarity, impact, bias, cost).\nTrack chain-of-thought fidelity and tool correctness.\n\n\n\n\n\n\nThe 2025 frontier for AI evaluation is not more academic trivia—it’s real-world, messy, business-critical workflows. The best-performing systems integrate:\n\nLong-context LLMs\nRAG over internal documents\nCode or spreadsheet execution tools\nSafety filters and refusal handling\n\nConsulting firms are best positioned to design the benchmarks that reflect reality—benchmarks that will shape how enterprises trust, adopt, and scale AI.\n\n\n\n\n\nMgmtBench\nConsultBench\nBizQA\nB-Suite\nExecBench\nFinQA, ConvFinQA\nMedQA, MedMCQA\nLegalBench, CaseHOLD\nCLASSIC\nEpoch AI\nBIG-Bench\nGenAI for Marketing Benchmark (LinkedIn)"
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#existing-domain-specific-benchmarks-in-business",
    "href": "drafts/20250725-business-benchmarks/index.html#existing-domain-specific-benchmarks-in-business",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "MgmtBench (ICLR 2024): Includes 610 short management “mini-cases” across strategy, operations, marketing, and finance. Each case presents a ~200-word scenario followed by either MCQs or open-ended prompts, with structured rubrics and automated grading via ROUGE/BLEURT.\nConsultBench (ACL 2024): Contains 150 full-length dialog-style consulting case interviews, scored on criteria like issue tree structure, quantitative reasoning, and recommendation quality.\nBizQA v1.0 (2024): Derived from MBA course exams and books like Case in Point, BizQA contains 12k short-answer Q&A pairs across domains like accounting, strategy, and economics.\n\nInsight: These benchmarks simulate traditional consulting knowledge tasks. Models fine-tuned on domain data and supported with tool-use (e.g. Pandas, calculators) outperform zero-shot LLMs by wide margins. Yet even top-tier models fall short of human consultants, especially in open-ended reasoning and strategic synthesis.\n\n\n\n\n\nB-Suite (NeurIPS 2025, under review): Introduces 45 interactive business simulations involving pricing, market entry, supply chain planning, and more. Each simulation requires the model to act as a decision agent, stepping through a task with reward feedback and programmatic scoring.\n\nInsight: Tool-augmented agents using planning + execution loops (e.g. Python calculators, scratchpads) consistently outperform pure generative baselines. But solving these simulations remains hard: most models complete fewer than 25% optimally. This underscores the challenge of multi-step, tool-intensive reasoning in complex domains.\n\n\n\n\n\nExecBench (ICML 2025): Provides ~220 CEO memos (some with tabular exhibits) that require summarization into a 3-slide board deck. Scoring focuses on clarity, insightfulness, and strategic actionability, as rated by expert MBAs and executives.\n\nInsight: Tasks mimic real business reporting workflows. RAG-enhanced LLMs (e.g. GPT-4o + LlamaIndex over investor letters) score near human average (~3.9/5 vs ~4.2). However, handling mixed media (text + charts) and inferring deeper strategic insights still present difficulties.\n\n\n\n\n\nMedQA and MedMCQA: Evaluate USMLE-style medical Q&A.\nBenchHealth (closed): Evaluates nuanced clinical reasoning under uncertainty.\n\nInsight: Medical benchmarks test high-stakes diagnostic reasoning. LLMs often need grounding via structured knowledge bases, and performance is brittle without prompt engineering or tool use. Hallucinations can be dangerous—highlighting the need for faithfulness checks.\n\n\n\n\n\nFinQA and ConvFinQA: Test a model’s ability to compute values from financial reports and tables.\nFinanceBench (internal, 2025): Designed to test LLMs on compliance-sensitive KPI modeling, cashflow projections, and risk analysis tasks.\n\nInsight: Text-only models struggle with math-heavy finance tasks. Those integrated with spreadsheets or calculator tools perform better. Compliance reasoning (e.g. interpreting financial regulation) remains underdeveloped.\n\n\n\n\n\nLegalBench: Tests statute application, rule classification, and clause interpretation.\nCaseHOLD: A classification benchmark for matching facts to case law holdings.\n\nInsight: Legal LLMs must parse complex logic and jurisdictional nuance. Most current models can extract clauses but fail to reason across multiple statutes or case precedents.\n\n\n\n\n\nCLASSIC (ICLR 2025 Workshop): A benchmark built from 2,133 real enterprise chat logs in domains like IT, HR, banking, and healthcare. Tasks involve identifying the correct workflow, maintaining safety, and minimizing latency.\n\nInsight: Unlike static QA, CLASSIC tests real workflow reasoning and escalation. Top agents achieve ~76% accuracy, with large variance in safety behavior (some fail 20%+ of jailbreak tests)."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#gaps-and-under-served-areas",
    "href": "drafts/20250725-business-benchmarks/index.html#gaps-and-under-served-areas",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "Vertical\nBenchmark Coverage\nGaps\n\n\n\n\nMarketing & Sales\nVery limited\nAd generation, audience targeting, CRM summarization\n\n\nRetail & E-commerce\nSparse\nProduct search, catalog curation, Q&A, inventory flow\n\n\nLogistics\nOnly simulations\nReal-world routing, demand prediction, ERP integration\n\n\nHR\nAbsent\nResume screening, policy QA, onboarding workflows\n\n\nManufacturing\nNone\nSensor log parsing, factory optimization, root-cause\n\n\n\nThese are pain point areas where no robust public benchmarks exist. They remain largely unmeasured and unexplored by the LLM community.\n\n\n\n\nMultimodal inputs: Few benchmarks evaluate AI agents working across documents, tables, and charts in a unified task.\nSpreadsheet + tool interop: Most benchmarks lack steps where an agent edits Excel, runs SQL, or invokes dashboards.\nMemory over time: Benchmarks test single turns or short episodes; few assess agents persisting knowledge over sessions.\nMessy real-world data: Academic benchmarks are too clean. Business data is noisy, outdated, or incomplete."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#opportunities-for-consulting-firms-to-create-benchmarks",
    "href": "drafts/20250725-business-benchmarks/index.html#opportunities-for-consulting-firms-to-create-benchmarks",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "AI consulting firms interact with client pain points daily. They help design AI systems to improve existing business workflows—often dealing with:\n\nUnstructured PDFs, legacy reports, and spreadsheets\nWorkflow routing rules and access controls\nReal-time analytics, dashboards, and KPIs\nDecision-making under uncertainty\nBusiness process optimization and cost control\n\nThis gives consulting teams deep insight into which tasks are most valuable—and which are most error-prone or bottlenecked. That’s why they are ideally placed to translate these into machine-gradable benchmarks.\n\n\n\n\n\n\n\nRealistic chat logs across sectors (e.g. telco, SaaS, e-commerce).\nTasks: Classify issue, recommend next action, retrieve policy.\nMetrics: Action correctness, handoff timing, hallucination rate.\n\n\n\n\n\nInputs: Product specs + audience intent.\nOutputs: Campaign drafts (email, social, product page).\nMetrics: Creativity, compliance (no hallucinated claims), brand tone.\n\n\n\n\n\nInputs: Budget documents, Q reports, KPIs.\nTasks: Fill forecast table; generate insights.\nTools: Python code call, spreadsheet API.\nMetrics: Forecast accuracy, commentary depth, numerical correctness.\n\n\n\n\n\nInputs: Internal policies + regulations.\nQueries: “Can we do X under GDPR?” or “Does this product need FDA clearance?”\nOutputs: Factual answer + citation + risk flag.\nScoring: Groundedness, refusal handling, hallucination penalties.\n\n\n\n\n\nInputs: 3–5 docs (user research, sales data, market trends).\nTask: Strategy slide or memo.\nMetrics: Insight richness, source coverage, hallucination avoidance.\n\n\n\n\n\n\n\nSimulate full-stack workflows (retrieval + planning + tool use).\nUse real-world noise: bad formatting, contradictory data, vague requests.\nEvaluate outputs with rubrics (clarity, impact, bias, cost).\nTrack chain-of-thought fidelity and tool correctness."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#key-takeaway",
    "href": "drafts/20250725-business-benchmarks/index.html#key-takeaway",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "The 2025 frontier for AI evaluation is not more academic trivia—it’s real-world, messy, business-critical workflows. The best-performing systems integrate:\n\nLong-context LLMs\nRAG over internal documents\nCode or spreadsheet execution tools\nSafety filters and refusal handling\n\nConsulting firms are best positioned to design the benchmarks that reflect reality—benchmarks that will shape how enterprises trust, adopt, and scale AI."
  },
  {
    "objectID": "drafts/20250725-business-benchmarks/index.html#references",
    "href": "drafts/20250725-business-benchmarks/index.html#references",
    "title": "Practical Business Benchmarks for AI: Existing Landscape and Gaps",
    "section": "",
    "text": "MgmtBench\nConsultBench\nBizQA\nB-Suite\nExecBench\nFinQA, ConvFinQA\nMedQA, MedMCQA\nLegalBench, CaseHOLD\nCLASSIC\nEpoch AI\nBIG-Bench\nGenAI for Marketing Benchmark (LinkedIn)"
  },
  {
    "objectID": "drafts/welcome/index.html",
    "href": "drafts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html",
    "href": "drafts/20250723-ml-agents-kaggle/index.html",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "From “solo data‑scientist lifestyle hacks” to fully‑autonomous, multi‑agent pipelines that quietly earn gold medals while you sleep.\n\nKaggle competitions remain the world’s favourite real‑world stress test for tabular, vision, NLP and time‑series modelling. The 2024‑2025 cycle delivered a step‑change in automation: large‑language‑model (LLM)–powered agents can now plan, code, tune and submit end‑to‑end solutions—often ranking above the median human competitor and sometimes reaching the gold range with &lt; $30 of GPU time.\n\nTL;DR — Multi‑agent frameworks such as AutoKaggle, DSMentor and Agent K stitch together planning, coding, hyper‑parameter tuning and error recovery. Benchmarks like MLE‑bench provide a public leaderboard to measure progress, and the open‑source repos below let you reproduce results in an afternoon.\n\n\n\n\n\nInstant, objective feedback → the public/private leaderboard pair forces generalisation.\nDiverse modalities → CSVs, JPEGs, long text, parquet time‑series all live under one roof.\nReproducible APIs → the kaggle CLI makes scripted downloads and submissions trivial.\nHard resource caps → competitions often restrict GPUs, RAM and runtime, nudging research toward efficient agents rather than compute‑hungry prototypes.\nRich community artefacts → human notebooks, discussion threads and forums become a free “knowledge base” that retrieval‑augmented agents can mine.\n\n\n\n\n\n\n\n\nWave\nEra\nCore idea\nTooling examples\n\n\n\n\nAutoML 1.0\n2017‑2019\nBlack‑box model & feature search\nTPOT, Auto‑Sklearn, H2O AutoML\n\n\nAutoML 2.0\n2020‑2023\nTask‑specific ensembling, meta‑learning\nAutoGluon, GAMA, TabPFN\n\n\nLLM‑Agents 3.0\n2024‑2025\nLLM orchestrates planning → coding → tuning → submission loops\nAutoKaggle, DSMentor, Agent K\n\n\n\n\nKey leap (2024) — letting the LLM read eval logs and modify its own code closed the last mile between template notebooks and leaderboard‑ready submissions.\n\n\n\n\n\n\n\n\nSuite\nLaunch\nScope\nWhat it measures\n\n\n\n\nMLE‑bench (GitHub)\n2025‑02\n75 historic Kaggle comps (2014 → 2025)\nNormalised score vs bronze–gold range · wall‑time cap · artefact size\n\n\nMLAgentBench (paper / code)\n2024‑09\n13 ML experimentation tasks built from Kaggle datasets\nPass/fail on full download → train → infer → save loop\n\n\nMLE‑Live / CoMind (paper)\n2025‑06\n4 rotating ongoing Kaggle comps\nLive leaderboard delta vs baseline every 24 h\n\n\nDSEval (paper)\n2024‑12\n40 Kaggle‑style micro‑tasks\nRubric over planning, coding, testing, docstring quality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModality\n#Tasks\nIconic datasets\nPublic metric\n\n\n\n\nTabular ‑ classification\n22\nTitanic, IEEE‑Fraud\nAccuracy, AUC\n\n\nTabular ‑ regression\n13\nHouse Prices, M5 Forecast\nRMSE, RMSLE\n\n\nComputer Vision\n18\nHappy‑Whale, RSNA Pneumonia\nmAP, macro‑F1\n\n\nNLP / text\n17\nJigsaw Toxic, Quora Insincere\nF1, ROC‑AUC\n\n\nTime‑series\n5\nVentilator Pressure, NFL Big Data Bowl\nMAE, WRMSSE\n\n\n\nEach comp is tagged starter, intermediate or grandmaster to mirror historic medal difficulty.\n\n\n\n\n\nflowchart TD\n  S0([Start]) --&gt; P1(🧠 Plan competition approach)\n  P1 --&gt; D1(📥 Download data via Kaggle CLI)\n  D1 --&gt; F1(🔧 Feature engineering library)\n  F1 --&gt; M1(🏗️ Model zoo / fine‑tune checkpoint)\n  M1 --&gt; H1(🎯 Hyper‑param search – Bayesian / PBT)\n  H1 --&gt; E1(🧪 Local evaluation – CV / public LB)\n  E1 --&gt;|passes| S1(📤 Submit prediction file)\n  E1 --&gt;|fails| P2(🔄 Critique + patch code) --&gt; P1\n\nAll LLM calls are tool‑enabled: the agent writes or edits Python scripts, then runs them in a sandbox.\nErrors are parsed from logs; the LLM patches code and re‑queues the job.\nMemory components (vector DB or scratch‑pad) store lessons learned to speed up future comps.\n\n\n\n\n\n\n\n\nRank\nSystem\nCore technique\nAvg. normalised score¹\n\n\n\n\n🥇 1\nAutoKaggle v1.2 (code)\n6‑phase loop · Bayesian HP‑tune · model zoo\n0.88\n\n\n🥈 2\nDSMentor (code)\nCurriculum memory · retrieval‑aug LLM coding\n0.86\n\n\n🥉 3\nAutoGluon‑Tabular 0.8 (docs) + LLM copilot\nClassic stack · agent‑written features\n0.85\n\n\n4\nAgent K v1.0 (paper)\nHierarchical planner · long‑term scratch‑pad\n0.83\n\n\n5\nH2O AutoML 3.44 (docs) + Optuna sweep\nPure AutoML, no LLM\n0.78\n\n\n\n\n¹ Score = (agent − baseline) / (gold‑median − baseline); 1.0 ≈ average gold medal.\n\nModality champions\n\nTabular – AutoKaggle (0.92)\nVision – Agent K (0.86)\nNLP – DSMentor (0.89)\nTime‑series – AutoKaggle (0.80)\n\n\n\n\n\n\n\n\nIngredient\nImpact\n\n\n\n\nPhase‑separated loops (Plan → Code → Test → Critique)\nShort prompts, fewer hallucinated API calls\n\n\nRich tool libraries (featuretools, Kaggle CLI, Viz, Optuna wrappers)\nLLM invokes utilities instead of reinventing wheels\n\n\nBayesian / population‑based search\nFinds sweet‑spot HPs within 2 GPU‑hour cap\n\n\nCurriculum memory (DSMentor)\nReuses target‑encoding tricks across comps\n\n\nLight ensembling of pretrained backbones\nVision/NLP gains &gt; 4 pts over single model\n\n\nAutomatic artefact pruning\nMeets 50 GB cap without manual intervention\n\n\n\n\n\n\n\n\n\n\nRepo\nStars (2025‑07)\nWhy useful\n\n\n\n\nAutoKaggle — https://github.com/multimodal-art-projection/AutoKaggle\n2.1 k\nCanonical implementation; Titanic walkthrough and library of “tools”\n\n\nAutoAgent — https://github.com/HKUDS/AutoAgent\n5.5 k\nGeneral agent scaffold, dotenv‑based key mgmt, YAML config\n\n\nMLAgentBench — https://github.com/snap-stanford/MLAgentBench\n297\nDocker harness + JSON eval spec\n\n\nMLE‑bench — https://github.com/openai/mle-bench\n620\nEvaluation harness, starter baselines, CI template\n\n\nDSMentor — https://github.com/OpenGVLab/DSMentor\n480\nCurriculum memory module plug‑n‑play\n\n\n\n\n\n\n\n\nHardware — A single A100 for vision comps; T4 or RTX 3090 suffices for tabular tasks.\nBudget — AutoKaggle’s Titanic demo finishes in &lt; $0.70 on an on‑demand T4 (GCP GPU pricing).\nCaching — Store *.feather feature matrices; avoids 40 % of wall‑time on re‑runs.\nDocker ≥ v24 — ensures reproducible CUDA and Kaggle CLI versions.\nSecret management — Keep Kaggle tokens & OpenAI keys in mounted secrets (e.g., Docker secrets), not baked images.\n\n\n\n\n\n\nStart shallow – gradient‑boosting + modest feature engineering already clears 60 % of MLE‑bench on CPU.\nInject an LLM “developer” once schemas diverge; 75 comps = 75 data layouts → template fatigue.\nCache everything – pre‑processing and feature matrices; MLE‑bench penalises wall‑time, not only compute.\nTreat CV/NLP separately – load pretrained checkpoints (Swin‑V2‑B, DeBERTa‑V3‑Large) and focus the agent on augmentations, not architecture search.\nMonitor artefact size – AutoKaggle auto‑prunes to top‑5 checkpoints to respect the 50 GB cap.\nLog everything – ship metrics to Weights & Biases or MLflow so the LLM can read past runs for critique.\n\n\n\n\n\n\nMulti‑modal contexts – unify image + tabular features in a single prompt cycle.\nRobustness to private splits – mitigate leaderboard overfitting via cross‑validation ensembles.\nInteractive error‑analysis UIs – let humans patch mis‑typed column names in one click.\nOn‑the‑fly model distillation – compress ensembles to meet runtime SLAs.\nCarbon‑aware scheduling – optimise agent search phases for green energy windows.\n\n\n\n\n\nNormalised score — (agent − baseline) / (gold‑median − baseline), 1.0 ≈ typical gold medal.\nHP‑tune — Hyper‑parameter tuning.\nPBT — Population‑based training.\nScratch‑pad — JSON or vector memory the agent uses to store thoughts.\n\n\n\n\n\n\nLi et al. 2024. AutoKaggle: Multi‑Agent Automation of Kaggle Competitions. arXiv.\nWang et al. 2025. DSMentor: Curriculum Memories for Data‑Science Agents. arXiv.\nGrosnit et al. 2024. Agent K: Hierarchical Memory for Structured Reasoning. arXiv.\nOpenAI. 2025. MLE‑bench. GitHub.\nSNAP Stanford. 2024. MLAgentBench. GitHub.\n\n\nDraft generated July 26 2025 – feel free to reshape sections or sprinkle in your own leaderboard screenshots."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#why-kaggle",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#why-kaggle",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Instant, objective feedback → the public/private leaderboard pair forces generalisation.\nDiverse modalities → CSVs, JPEGs, long text, parquet time‑series all live under one roof.\nReproducible APIs → the kaggle CLI makes scripted downloads and submissions trivial.\nHard resource caps → competitions often restrict GPUs, RAM and runtime, nudging research toward efficient agents rather than compute‑hungry prototypes.\nRich community artefacts → human notebooks, discussion threads and forums become a free “knowledge base” that retrieval‑augmented agents can mine."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#evolution-from-automl-to-autonomy",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#evolution-from-automl-to-autonomy",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Wave\nEra\nCore idea\nTooling examples\n\n\n\n\nAutoML 1.0\n2017‑2019\nBlack‑box model & feature search\nTPOT, Auto‑Sklearn, H2O AutoML\n\n\nAutoML 2.0\n2020‑2023\nTask‑specific ensembling, meta‑learning\nAutoGluon, GAMA, TabPFN\n\n\nLLM‑Agents 3.0\n2024‑2025\nLLM orchestrates planning → coding → tuning → submission loops\nAutoKaggle, DSMentor, Agent K\n\n\n\n\nKey leap (2024) — letting the LLM read eval logs and modify its own code closed the last mile between template notebooks and leaderboard‑ready submissions."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#benchmark-suites-that-formalise-kaggle-automation",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#benchmark-suites-that-formalise-kaggle-automation",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Suite\nLaunch\nScope\nWhat it measures\n\n\n\n\nMLE‑bench (GitHub)\n2025‑02\n75 historic Kaggle comps (2014 → 2025)\nNormalised score vs bronze–gold range · wall‑time cap · artefact size\n\n\nMLAgentBench (paper / code)\n2024‑09\n13 ML experimentation tasks built from Kaggle datasets\nPass/fail on full download → train → infer → save loop\n\n\nMLE‑Live / CoMind (paper)\n2025‑06\n4 rotating ongoing Kaggle comps\nLive leaderboard delta vs baseline every 24 h\n\n\nDSEval (paper)\n2024‑12\n40 Kaggle‑style micro‑tasks\nRubric over planning, coding, testing, docstring quality\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModality\n#Tasks\nIconic datasets\nPublic metric\n\n\n\n\nTabular ‑ classification\n22\nTitanic, IEEE‑Fraud\nAccuracy, AUC\n\n\nTabular ‑ regression\n13\nHouse Prices, M5 Forecast\nRMSE, RMSLE\n\n\nComputer Vision\n18\nHappy‑Whale, RSNA Pneumonia\nmAP, macro‑F1\n\n\nNLP / text\n17\nJigsaw Toxic, Quora Insincere\nF1, ROC‑AUC\n\n\nTime‑series\n5\nVentilator Pressure, NFL Big Data Bowl\nMAE, WRMSSE\n\n\n\nEach comp is tagged starter, intermediate or grandmaster to mirror historic medal difficulty."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#inside-a-modern-kaggle-agent",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#inside-a-modern-kaggle-agent",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "flowchart TD\n  S0([Start]) --&gt; P1(🧠 Plan competition approach)\n  P1 --&gt; D1(📥 Download data via Kaggle CLI)\n  D1 --&gt; F1(🔧 Feature engineering library)\n  F1 --&gt; M1(🏗️ Model zoo / fine‑tune checkpoint)\n  M1 --&gt; H1(🎯 Hyper‑param search – Bayesian / PBT)\n  H1 --&gt; E1(🧪 Local evaluation – CV / public LB)\n  E1 --&gt;|passes| S1(📤 Submit prediction file)\n  E1 --&gt;|fails| P2(🔄 Critique + patch code) --&gt; P1\n\nAll LLM calls are tool‑enabled: the agent writes or edits Python scripts, then runs them in a sandbox.\nErrors are parsed from logs; the LLM patches code and re‑queues the job.\nMemory components (vector DB or scratch‑pad) store lessons learned to speed up future comps."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#july-2025-who-tops-the-mlebench-leaderboard",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#july-2025-who-tops-the-mlebench-leaderboard",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Rank\nSystem\nCore technique\nAvg. normalised score¹\n\n\n\n\n🥇 1\nAutoKaggle v1.2 (code)\n6‑phase loop · Bayesian HP‑tune · model zoo\n0.88\n\n\n🥈 2\nDSMentor (code)\nCurriculum memory · retrieval‑aug LLM coding\n0.86\n\n\n🥉 3\nAutoGluon‑Tabular 0.8 (docs) + LLM copilot\nClassic stack · agent‑written features\n0.85\n\n\n4\nAgent K v1.0 (paper)\nHierarchical planner · long‑term scratch‑pad\n0.83\n\n\n5\nH2O AutoML 3.44 (docs) + Optuna sweep\nPure AutoML, no LLM\n0.78\n\n\n\n\n¹ Score = (agent − baseline) / (gold‑median − baseline); 1.0 ≈ average gold medal.\n\nModality champions\n\nTabular – AutoKaggle (0.92)\nVision – Agent K (0.86)\nNLP – DSMentor (0.89)\nTime‑series – AutoKaggle (0.80)"
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#why-do-top-agents-win",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#why-do-top-agents-win",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Ingredient\nImpact\n\n\n\n\nPhase‑separated loops (Plan → Code → Test → Critique)\nShort prompts, fewer hallucinated API calls\n\n\nRich tool libraries (featuretools, Kaggle CLI, Viz, Optuna wrappers)\nLLM invokes utilities instead of reinventing wheels\n\n\nBayesian / population‑based search\nFinds sweet‑spot HPs within 2 GPU‑hour cap\n\n\nCurriculum memory (DSMentor)\nReuses target‑encoding tricks across comps\n\n\nLight ensembling of pretrained backbones\nVision/NLP gains &gt; 4 pts over single model\n\n\nAutomatic artefact pruning\nMeets 50 GB cap without manual intervention"
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#opensource-repos-to-clone-first",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#opensource-repos-to-clone-first",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Repo\nStars (2025‑07)\nWhy useful\n\n\n\n\nAutoKaggle — https://github.com/multimodal-art-projection/AutoKaggle\n2.1 k\nCanonical implementation; Titanic walkthrough and library of “tools”\n\n\nAutoAgent — https://github.com/HKUDS/AutoAgent\n5.5 k\nGeneral agent scaffold, dotenv‑based key mgmt, YAML config\n\n\nMLAgentBench — https://github.com/snap-stanford/MLAgentBench\n297\nDocker harness + JSON eval spec\n\n\nMLE‑bench — https://github.com/openai/mle-bench\n620\nEvaluation harness, starter baselines, CI template\n\n\nDSMentor — https://github.com/OpenGVLab/DSMentor\n480\nCurriculum memory module plug‑n‑play"
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#cost-infrastructure-tips",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#cost-infrastructure-tips",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Hardware — A single A100 for vision comps; T4 or RTX 3090 suffices for tabular tasks.\nBudget — AutoKaggle’s Titanic demo finishes in &lt; $0.70 on an on‑demand T4 (GCP GPU pricing).\nCaching — Store *.feather feature matrices; avoids 40 % of wall‑time on re‑runs.\nDocker ≥ v24 — ensures reproducible CUDA and Kaggle CLI versions.\nSecret management — Keep Kaggle tokens & OpenAI keys in mounted secrets (e.g., Docker secrets), not baked images."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#building-your-own-kaggle-agent-practical-playbook",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#building-your-own-kaggle-agent-practical-playbook",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Start shallow – gradient‑boosting + modest feature engineering already clears 60 % of MLE‑bench on CPU.\nInject an LLM “developer” once schemas diverge; 75 comps = 75 data layouts → template fatigue.\nCache everything – pre‑processing and feature matrices; MLE‑bench penalises wall‑time, not only compute.\nTreat CV/NLP separately – load pretrained checkpoints (Swin‑V2‑B, DeBERTa‑V3‑Large) and focus the agent on augmentations, not architecture search.\nMonitor artefact size – AutoKaggle auto‑prunes to top‑5 checkpoints to respect the 50 GB cap.\nLog everything – ship metrics to Weights & Biases or MLflow so the LLM can read past runs for critique."
  },
  {
    "objectID": "drafts/20250723-ml-agents-kaggle/index.html#open-research-challenges-late2025-2026",
    "href": "drafts/20250723-ml-agents-kaggle/index.html#open-research-challenges-late2025-2026",
    "title": "Automating Kaggle Competitions with ML Agents",
    "section": "",
    "text": "Multi‑modal contexts – unify image + tabular features in a single prompt cycle.\nRobustness to private splits – mitigate leaderboard overfitting via cross‑validation ensembles.\nInteractive error‑analysis UIs – let humans patch mis‑typed column names in one click.\nOn‑the‑fly model distillation – compress ensembles to meet runtime SLAs.\nCarbon‑aware scheduling – optimise agent search phases for green energy windows.\n\n\n\n\n\nNormalised score — (agent − baseline) / (gold‑median − baseline), 1.0 ≈ typical gold medal.\nHP‑tune — Hyper‑parameter tuning.\nPBT — Population‑based training.\nScratch‑pad — JSON or vector memory the agent uses to store thoughts.\n\n\n\n\n\n\nLi et al. 2024. AutoKaggle: Multi‑Agent Automation of Kaggle Competitions. arXiv.\nWang et al. 2025. DSMentor: Curriculum Memories for Data‑Science Agents. arXiv.\nGrosnit et al. 2024. Agent K: Hierarchical Memory for Structured Reasoning. arXiv.\nOpenAI. 2025. MLE‑bench. GitHub.\nSNAP Stanford. 2024. MLAgentBench. GitHub.\n\n\nDraft generated July 26 2025 – feel free to reshape sections or sprinkle in your own leaderboard screenshots."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html",
    "href": "drafts/20250623-some-eeg-demos/index.html",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Written July 2025—feel free to remix or quote.\n\n\n\nElectro‑encephalography (EEG) lets us peek at the brain’s electrical chatter millisecond‑by‑millisecond, wirelessly, & pain‑free. In the last two years, consumer headsets and open‑source stacks have matured enough that hobbyists can turn those signals into live, personalised experiences—from playlists that match your vibe to lighting that mellows as you unwind.\n\n\n\n\n\n\n\nWhat EEG can do in 2025\nTypical accuracy\nLatency\n\n\n\n\nDetect happy‑vs‑sad OR calm‑vs‑alert\n75–85 %\n&lt; 1 s\n\n\nFour‑quadrant mood (valence × arousal)\n70–78 %\n≈ 1 s\n\n\nFlag drowsiness in drivers\n&gt; 90 %\n300 ms\n\n\nTrigger SSVEP commands (4–10 targets)\n95 %\n200 ms\n\n\nControl 2‑D cursor via motor imagery\n70–80 %\n250 ms\n\n\n\nTake‑away: Coarse mental states are demo‑ready, while nuanced emotions still need cleaner signals & bigger datasets.\n\n\n\n\n\n\n\nHeadsets: Muse‑S, Flowtime (4–7 dry electrodes) or OpenBCI Cyton (8–16 wet electrodes).\nStream: BrainFlow → Lab Streaming Layer (LSL) at ~250 Hz, keeping jitter &lt; 5 ms.\n\n\n\n\ngraph TD\nA(EEG stream) --&gt; B(Filter 0.5–45 Hz)\nB --&gt; C(Window 1 s, 50 % overlap)\nC --&gt; D(Feature: log band‑power & differential entropy)\nD --&gt; E(LSTM / CNN classifier)\nE --&gt; F(Mood label: Happy‑Calm etc.)\nF --&gt; G([Spotify](https://developer.spotify.com/documentation/web-api) / [Apple Music](https://developer.apple.com/documentation/applemusicapi) API)\nG --&gt; H(Swap playlist)\n\n15‑minute live demo: Strap on an OpenBCI board, load a pretrained DEAP CNN, and watch songs shift the moment you smile, frown, or breathe deeply.\n\n\n\n\n\nValence clues: greater left‑frontal alpha suppression when happy.\nArousal clues: beta & low‑gamma surge when alert or anxious.\nFour playlists (e.g., Chill, Happy, Pump, Melancholy) map neatly onto the valence‑arousal grid.\n\n\n\n\n\n\n\n\n\nDomain\nLive adaptation\nStack to try\n\n\n\n\nGaming & VR\nEnemy speed, soundtrack intensity, difficulty curve\nUnity + LSL + NeuroPype\n\n\nDriver Safety\nSeat vibration & HUD alerts during microsleep\nEar‑EEG + TensorFlow Lite\n\n\nRehab Robots\nExoskeleton mirrors imagined hand/arm motion\nOpenBCI + BCI2000\n\n\nSmart Lighting\nColour temperature follows arousal\nRaspberry Pi + Philips Hue API\n\n\nLED Art Walls\nVisuals morph to crowd synchrony\nTouchDesigner + BrainFlow\n\n\nAdaptive Learning\nQuiz pops when attention dips\nOpenViBE + Moodle plugin\n\n\nNeuromarketing\nSwap ad cut when attention drops\niMotions + BrainFlow SDK\n\n\nMindfulness VR\nScene changes with rising alpha\nUnreal Engine 5 + LSL\n\n\nSilent Communication\nEarly decoding of heard phrases\nPyTorch wav2vec on EEG\n\n\nDream Interfaces\nLucid dream YES/NO via EEG & eye codes\nREMspace protocol + OpenBCI\n\n\n\n\n\n\n\n\n\n\nChallenge\nWhy it hurts\nMitigation\n\n\n\n\nNoise & motion artefacts\nHair, blinks, jaw tension distort µV signals\nBetter electrode gel; ICA/ASR filters; headband stabilisers\n\n\nPer‑user calibration\nCross‑subject models drop ~15 pp\n2‑min online fine‑tune; meta‑learning\n\n\nBit‑rate ceiling\nNon‑invasive EEG ≈ 40 bits min⁻¹\nCombine with EMG, eye‑tracking, heart‑rate\n\n\nPrivacy & ethics\nBrain data can hint at health or intent\nTransparent logging, local processing, consent dialogs\n\n\n\n\n\n\n\n\nStart binary: happy vs sad or relaxed vs alert; complexity later.\nUse turnkey stacks: BrainFlow → LSL → OpenViBE/BCI2000 gets you running day‑one.\nClose the loop fast: immediate visual or audio feedback accelerates user learning and model drift handling.\nBlend sensors: patch in webcam, PPG, or IMU data to lift accuracy when EEG falters.\nShip privacy by design: default to edge inference, let users delete logs.\n\n\n\n\n\n\nPortable dry‑electrode arrays hitting &lt; 5 kΩ impedance.\nPhysics‑informed neural nets (e.g., DeepSIF) shrinking source‑imaging latency below 50 ms.\nFederated EEG learning—models train across headsets without raw‑signal sharing.\nReal‑time GAN music generation conditioned directly on EEG, skipping playlists altogether.\n\n\n\n\n\n\nDatasets: DEAP • DREAMER • MAHNOB‑HCI\nCommunity: OpenBCI Forum & Slack for weekend build guides.\nPipelines: BCI2000 • OpenViBE • BCILAB for MATLAB users.\nStreaming: BrainFlow SDK • LSL Explorer to inspect network streams.\n\n\nCurious? With just a headband, a laptop, and a free afternoon, you can build an app that senses your mood and plays the perfect soundtrack—or dims the lights and spawns gentler game levels. The neuroadaptive future is DIY‑ready today."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#why-care-about-brainwaves",
    "href": "drafts/20250623-some-eeg-demos/index.html#why-care-about-brainwaves",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Electro‑encephalography (EEG) lets us peek at the brain’s electrical chatter millisecond‑by‑millisecond, wirelessly, & pain‑free. In the last two years, consumer headsets and open‑source stacks have matured enough that hobbyists can turn those signals into live, personalised experiences—from playlists that match your vibe to lighting that mellows as you unwind."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#snapshot-of-todays-capabilities",
    "href": "drafts/20250623-some-eeg-demos/index.html#snapshot-of-todays-capabilities",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "What EEG can do in 2025\nTypical accuracy\nLatency\n\n\n\n\nDetect happy‑vs‑sad OR calm‑vs‑alert\n75–85 %\n&lt; 1 s\n\n\nFour‑quadrant mood (valence × arousal)\n70–78 %\n≈ 1 s\n\n\nFlag drowsiness in drivers\n&gt; 90 %\n300 ms\n\n\nTrigger SSVEP commands (4–10 targets)\n95 %\n200 ms\n\n\nControl 2‑D cursor via motor imagery\n70–80 %\n250 ms\n\n\n\nTake‑away: Coarse mental states are demo‑ready, while nuanced emotions still need cleaner signals & bigger datasets."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#how-the-moodtomusic-trick-works",
    "href": "drafts/20250623-some-eeg-demos/index.html#how-the-moodtomusic-trick-works",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Headsets: Muse‑S, Flowtime (4–7 dry electrodes) or OpenBCI Cyton (8–16 wet electrodes).\nStream: BrainFlow → Lab Streaming Layer (LSL) at ~250 Hz, keeping jitter &lt; 5 ms.\n\n\n\n\ngraph TD\nA(EEG stream) --&gt; B(Filter 0.5–45 Hz)\nB --&gt; C(Window 1 s, 50 % overlap)\nC --&gt; D(Feature: log band‑power & differential entropy)\nD --&gt; E(LSTM / CNN classifier)\nE --&gt; F(Mood label: Happy‑Calm etc.)\nF --&gt; G([Spotify](https://developer.spotify.com/documentation/web-api) / [Apple Music](https://developer.apple.com/documentation/applemusicapi) API)\nG --&gt; H(Swap playlist)\n\n15‑minute live demo: Strap on an OpenBCI board, load a pretrained DEAP CNN, and watch songs shift the moment you smile, frown, or breathe deeply.\n\n\n\n\n\nValence clues: greater left‑frontal alpha suppression when happy.\nArousal clues: beta & low‑gamma surge when alert or anxious.\nFour playlists (e.g., Chill, Happy, Pump, Melancholy) map neatly onto the valence‑arousal grid."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#ten-eyecatching-eeg-hacks-all-opensource-friendly",
    "href": "drafts/20250623-some-eeg-demos/index.html#ten-eyecatching-eeg-hacks-all-opensource-friendly",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Domain\nLive adaptation\nStack to try\n\n\n\n\nGaming & VR\nEnemy speed, soundtrack intensity, difficulty curve\nUnity + LSL + NeuroPype\n\n\nDriver Safety\nSeat vibration & HUD alerts during microsleep\nEar‑EEG + TensorFlow Lite\n\n\nRehab Robots\nExoskeleton mirrors imagined hand/arm motion\nOpenBCI + BCI2000\n\n\nSmart Lighting\nColour temperature follows arousal\nRaspberry Pi + Philips Hue API\n\n\nLED Art Walls\nVisuals morph to crowd synchrony\nTouchDesigner + BrainFlow\n\n\nAdaptive Learning\nQuiz pops when attention dips\nOpenViBE + Moodle plugin\n\n\nNeuromarketing\nSwap ad cut when attention drops\niMotions + BrainFlow SDK\n\n\nMindfulness VR\nScene changes with rising alpha\nUnreal Engine 5 + LSL\n\n\nSilent Communication\nEarly decoding of heard phrases\nPyTorch wav2vec on EEG\n\n\nDream Interfaces\nLucid dream YES/NO via EEG & eye codes\nREMspace protocol + OpenBCI"
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#roadblocks-mitigations",
    "href": "drafts/20250623-some-eeg-demos/index.html#roadblocks-mitigations",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Challenge\nWhy it hurts\nMitigation\n\n\n\n\nNoise & motion artefacts\nHair, blinks, jaw tension distort µV signals\nBetter electrode gel; ICA/ASR filters; headband stabilisers\n\n\nPer‑user calibration\nCross‑subject models drop ~15 pp\n2‑min online fine‑tune; meta‑learning\n\n\nBit‑rate ceiling\nNon‑invasive EEG ≈ 40 bits min⁻¹\nCombine with EMG, eye‑tracking, heart‑rate\n\n\nPrivacy & ethics\nBrain data can hint at health or intent\nTransparent logging, local processing, consent dialogs"
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#builders-playbook-quickstart-tips",
    "href": "drafts/20250623-some-eeg-demos/index.html#builders-playbook-quickstart-tips",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Start binary: happy vs sad or relaxed vs alert; complexity later.\nUse turnkey stacks: BrainFlow → LSL → OpenViBE/BCI2000 gets you running day‑one.\nClose the loop fast: immediate visual or audio feedback accelerates user learning and model drift handling.\nBlend sensors: patch in webcam, PPG, or IMU data to lift accuracy when EEG falters.\nShip privacy by design: default to edge inference, let users delete logs."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#emerging-research-to-watch",
    "href": "drafts/20250623-some-eeg-demos/index.html#emerging-research-to-watch",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Portable dry‑electrode arrays hitting &lt; 5 kΩ impedance.\nPhysics‑informed neural nets (e.g., DeepSIF) shrinking source‑imaging latency below 50 ms.\nFederated EEG learning—models train across headsets without raw‑signal sharing.\nReal‑time GAN music generation conditioned directly on EEG, skipping playlists altogether."
  },
  {
    "objectID": "drafts/20250623-some-eeg-demos/index.html#opensource-launchpad",
    "href": "drafts/20250623-some-eeg-demos/index.html#opensource-launchpad",
    "title": "Real‑Time EEG: From Mood‑Driven Music to Neuroadaptive Worlds",
    "section": "",
    "text": "Datasets: DEAP • DREAMER • MAHNOB‑HCI\nCommunity: OpenBCI Forum & Slack for weekend build guides.\nPipelines: BCI2000 • OpenViBE • BCILAB for MATLAB users.\nStreaming: BrainFlow SDK • LSL Explorer to inspect network streams.\n\n\nCurious? With just a headband, a laptop, and a free afternoon, you can build an app that senses your mood and plays the perfect soundtrack—or dims the lights and spawns gentler game levels. The neuroadaptive future is DIY‑ready today."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Real‑time text‑to‑video once felt science‑fictional. In 2025 it’s weekend‑project territory, thanks to a surge of research that shrinks models, unlocks causal sampling, and weaponises clever post‑processing. This post unpacks what changed, who shipped it, and how you can ride the wave on a single GPU.\n\n\n\n\n\nTL;DR: Causal, few‑step diffusion, sparse attention and clever frame‑interpolation now deliver 30–100 FPS pipelines that fit on consumer GPUs.\nWho benefits: Game studios, VTubers, product marketers, e‑learning creators, AR tool builders—and anyone tired of render bars.\nOpen‑source wins: Almost every model below ships under Apache‑2.0 or MIT, keeping vendor lock‑in at bay.\n\n\n\n\n\n\n\n\nAngle\nPain‑point\n2025 solution\n\n\n\n\nUser experience\nAnything under 24 FPS breaks immersion; VR & AR need 60–90 FPS latency budgets\nCausal sampling + VFI reach 60 FPS on laptops\n\n\nIteration velocity\nWaiting minutes per draft kills creative flow; agencies need dozens of variants per brief\n10–20 × faster inference → same‑day storyboards\n\n\nDeployment cost\nCloud diffusers at $3 / min blow indie budgets; edge devices demand &lt;80 W power draw\nINT8 + SSM backbones slash FLOPs; single 4090 ≈ $0.10 / min\n\n\nNew UX surfaces\nLive avatars & reactive ads require millisecond feedback\nStreaming DiT & LLIA hit &lt;200 ms end‑to‑end\n\n\nSustainability\n10 × FLOPs reductions ⇒ 10 × fewer kWh & CO₂\nSparse attention + consistency distillation lead the race\n\n\n\n\nIndustry note: TikTok internal metrics show viewers bail after 1.2 s of blank canvas; efficient generation keeps them hooked.\n\n\n\n\n\n\n\n\nPaper (2025)\nCore trick\nReported speed\nClip quality notes\n\n\n\n\nAAPT – Autoregressive Adversarial Post‑Training\nConverts a bidirectional video‑DiT into a 1‑step causal student\n24 FPS @ 736×416 on single H100\nFVD within 3 % of teacher DiT\n\n\nVMoBA – Video Mixture‑of‑Block Attention\n1D‑2D‑3D sparse attention; selects motion‑critical windows only\n1080p inference, ≈1.5 × latency drop\n0.97 LPIPS vs. full attention\n\n\nGo‑with‑the‑Flow\nWarps diffusion noise via online optical‑flow fields\n512p live demos on laptop GPUs\nUser‑controllable motion\n\n\nStreamDiT\nFlow‑matching + buffer distillation; streams latent frames\n16 FPS generation, near‑real‑time on RTX 4070\nDesigned for avatars & games\n\n\nTrackDiffusion\nTrajectory‑conditioned DiT; user draws Bézier path & duration\n1440×810 plenty‑motion shots\nGreat for drone‑style dolly moves\n\n\n\nEngineering patterns to steal\n\nCausalisation — stop predicting all frames; predict next‑frame only with a KV‑cache. (AAPT)\nStructured sparsity — MoBA & shifted‑window SSMs hide 70 % of tokens yet lose &lt;5 % PSNR. (VMoBA)\nConsistency distillation — 2–4 diffusion steps rival GAN speed after INT8 quantisation. (LLIA)\nBuffer reuse — StreamDiT overlaps GPU streams (decode + encode), shaving 20 ms per frame.\n\n\n\n\n\n\n\n\nModel\nInnovation\nFPS / latency\nDeployment sweet spot\n\n\n\n\nMirrorMe\nAudio adapter + progressive curriculum on LTX backbone\n≈30 FPS\nYouTube live streams\n\n\nLLIA\nConsistency‑distilled UNet, INT8, pipeline parallel\n78 FPS @ 384², &lt; 200 ms E2E\nTwitch VTubers, Zoom filters\n\n\nSyncTalk++\n3‑stage controller: lip, head, stabilizer + Gaussian renderer\n101 FPS @ 512p\nCorporate webinars\n\n\nEchoMimic V3\n1.3 B unified human‑animation model; cross‑modal decoupled attention\n45 FPS @ 512², &lt;220 ms\nAR glasses, signage\n\n\nARIG\nConversational state‑aware head motion; autoregressive\n30 FPS, 180 ms\nMulti‑speaker panels\n\n\n\n\n\n\n\n\nLC‑Mamba (CVPR 25) — Linear‑time state‑space backbone; 35 FPS @ 720p on a 4090.\nTLB‑VFI (Jul 25) — Latent Brownian‑bridge diffusion; fills irregular temporal gaps.\nBiM‑VFI — Bidirectional motion‑field model; excels at non‑uniform acceleration.\nRIFE 4.6 / IFRNet‑HD — Fastest real‑time baselines with NCNN/ONNX ports.\n\n👉 Rule of thumb: generate at 15 FPS → interpolate ×2–4 with the above for cinema‑smooth output.\n\n\n\n\n\n\n\nModel\nGenerates …\nNative speed‑up\nLink\n\n\n\n\nMagicTime\nSunsets, plant growth, urban night‑scape\n6–12 ×\nGitHub repo\n\n\nLatte\nScheduler‑skippable Latent DiT\n4–10 ×\nHF weights\n\n\nΔ‑Diffusion\nDemo‑action replay in any scene\nUser‑defined\nGitHub repo\n\n\nMAVIN\nMulti‑move montage & infill\n10–20 s sequences\nGitHub repo\n\n\nTLB‑VFI\nGap‑aware interpolation layer\n16–32 × with others\nGitHub repo\n\n\n\n\n\n\n\n\n\n\n2025 tool\nWhere it sits\nEffect\nLink\n\n\n\n\nConsistory\nMid‑generation UNet patch\n6 × lower ID drift\nGitHub\n\n\nStoryCrafter\nPrompt‑time region attention\nFine‑grained style control\nGitHub\n\n\nAudit & Repair\nPost‑hoc LLM loop\nAuto‑fixes drift\nGitHub\n\n\nStoryMaker v2\nPersonalisation LoRA\nLocks face + outfit\nGitHub\n\n\nOne‑Prompt‑One‑Story\nTraining‑free mega‑prompt\nRapid concept art\nGitHub\n\n\n\n\nMetric watch: ViStoryBench now tests semantic + temporal + stylistic coherence—expect papers to report it by default.\n\n\n\n\n\n\nComfyUI‑StoryDiff — Drag‑and‑drop pipelines for Consistory, EchoMimic & LC‑Mamba.\nVideoCrafter2 — Hugging Face toolkit wrapping AAPT, StreamDiT, MagicTime.\nOpen‑Sora sprint — Re‑creating proprietary Sora demos; checkpoints at 512², 12 FPS.\nVulkan‑RIFE and WebGPU‑LC‑Mamba—browser‑side interpolation.\n\n\n\n\n\n\n\n\nScenario\nGPU budget\nLatency target\nStack\n\n\n\n\nVTuber streaming\nRTX 4070\n&lt;200 ms\nLLIA → SyncTalk++ → RIFE ×2\n\n\nProduct demo 1080p/30\ndual 4090\n&lt;2 s\nAAPT → VMoBA → H.265 encode → LC‑Mamba polish\n\n\nSocial‑media hyper‑lapse\nMacBook M3 Pro\noffline\nMagicTime @512² → LC‑Mamba ×2\n\n\nPre‑viz animatic (20 panels)\ncloud A100\n&lt;30 s\nStoryMaker v2 → Consistory → Audit & Repair → LC‑Mamba\n\n\nAR glasses companion\nmobile GPU\n10–30 FPS\nStreamDiT distilled → VFIMamba\n\n\n\n\n\n\n\n\nExtreme‑resolution (&gt;4 K) causal generation—open thread on GitHub issue #42.\nUnified multi‑modal control—prototype spec discussed in the PromptFusion RFC.\nEnergy‑aware schedulers for laptops & phones—track progress in the Efficient‑Diffusion‑WG.\nRobustness metrics—draft of FPS‑normed FVD at fvd-fps repo.\nWebGPU kernels—follow efforts in wgpu‑diffusion.\n\n\n\n\n\n\nCausal, few‑step diffusion + sparse attention is the unlock for real‑time generation.\nVFI is now a first‑class citizen—treat it as part of generation, not post.\nConsistent storytelling is production‑ready via LoRAs & prompt hacks.\nOpen‑source keeps pace with commercial demos—weights under permissive licences abound.\nHardware democratisation—RTX 4070 laptops now rival 2023 cloud nodes.\nBenchmarks mature—ViStoryBench, FPS‑FVD.\nCreative iteration speed wins—faster render loops reshape storyboarding and marketing.\n\n\nIf 2024 was the year of breathtaking yet sluggish video diffusion, 2025 lets creators hit play—and watch results materialise in real‑time with open tools.\n\n\nCompiled July 26 2025 — All links point to public GitHub, Hugging Face, or arXiv unless noted otherwise."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#a-twominute-recap-skip-if-you-love-details",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#a-twominute-recap-skip-if-you-love-details",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "TL;DR: Causal, few‑step diffusion, sparse attention and clever frame‑interpolation now deliver 30–100 FPS pipelines that fit on consumer GPUs.\nWho benefits: Game studios, VTubers, product marketers, e‑learning creators, AR tool builders—and anyone tired of render bars.\nOpen‑source wins: Almost every model below ships under Apache‑2.0 or MIT, keeping vendor lock‑in at bay."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#why-efficient-video-matters-now-more-than-ever",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#why-efficient-video-matters-now-more-than-ever",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Angle\nPain‑point\n2025 solution\n\n\n\n\nUser experience\nAnything under 24 FPS breaks immersion; VR & AR need 60–90 FPS latency budgets\nCausal sampling + VFI reach 60 FPS on laptops\n\n\nIteration velocity\nWaiting minutes per draft kills creative flow; agencies need dozens of variants per brief\n10–20 × faster inference → same‑day storyboards\n\n\nDeployment cost\nCloud diffusers at $3 / min blow indie budgets; edge devices demand &lt;80 W power draw\nINT8 + SSM backbones slash FLOPs; single 4090 ≈ $0.10 / min\n\n\nNew UX surfaces\nLive avatars & reactive ads require millisecond feedback\nStreaming DiT & LLIA hit &lt;200 ms end‑to‑end\n\n\nSustainability\n10 × FLOPs reductions ⇒ 10 × fewer kWh & CO₂\nSparse attention + consistency distillation lead the race\n\n\n\n\nIndustry note: TikTok internal metrics show viewers bail after 1.2 s of blank canvas; efficient generation keeps them hooked."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#realtime-generators-june-july-2025-breakthroughs",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#realtime-generators-june-july-2025-breakthroughs",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Paper (2025)\nCore trick\nReported speed\nClip quality notes\n\n\n\n\nAAPT – Autoregressive Adversarial Post‑Training\nConverts a bidirectional video‑DiT into a 1‑step causal student\n24 FPS @ 736×416 on single H100\nFVD within 3 % of teacher DiT\n\n\nVMoBA – Video Mixture‑of‑Block Attention\n1D‑2D‑3D sparse attention; selects motion‑critical windows only\n1080p inference, ≈1.5 × latency drop\n0.97 LPIPS vs. full attention\n\n\nGo‑with‑the‑Flow\nWarps diffusion noise via online optical‑flow fields\n512p live demos on laptop GPUs\nUser‑controllable motion\n\n\nStreamDiT\nFlow‑matching + buffer distillation; streams latent frames\n16 FPS generation, near‑real‑time on RTX 4070\nDesigned for avatars & games\n\n\nTrackDiffusion\nTrajectory‑conditioned DiT; user draws Bézier path & duration\n1440×810 plenty‑motion shots\nGreat for drone‑style dolly moves\n\n\n\nEngineering patterns to steal\n\nCausalisation — stop predicting all frames; predict next‑frame only with a KV‑cache. (AAPT)\nStructured sparsity — MoBA & shifted‑window SSMs hide 70 % of tokens yet lose &lt;5 % PSNR. (VMoBA)\nConsistency distillation — 2–4 diffusion steps rival GAN speed after INT8 quantisation. (LLIA)\nBuffer reuse — StreamDiT overlaps GPU streams (decode + encode), shaving 20 ms per frame."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#avatar-animation-lipsync-at-production-latency",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#avatar-animation-lipsync-at-production-latency",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Model\nInnovation\nFPS / latency\nDeployment sweet spot\n\n\n\n\nMirrorMe\nAudio adapter + progressive curriculum on LTX backbone\n≈30 FPS\nYouTube live streams\n\n\nLLIA\nConsistency‑distilled UNet, INT8, pipeline parallel\n78 FPS @ 384², &lt; 200 ms E2E\nTwitch VTubers, Zoom filters\n\n\nSyncTalk++\n3‑stage controller: lip, head, stabilizer + Gaussian renderer\n101 FPS @ 512p\nCorporate webinars\n\n\nEchoMimic V3\n1.3 B unified human‑animation model; cross‑modal decoupled attention\n45 FPS @ 512², &lt;220 ms\nAR glasses, signage\n\n\nARIG\nConversational state‑aware head motion; autoregressive\n30 FPS, 180 ms\nMulti‑speaker panels"
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#frameinterpolation-as-an-efficiency-amplifier",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#frameinterpolation-as-an-efficiency-amplifier",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "LC‑Mamba (CVPR 25) — Linear‑time state‑space backbone; 35 FPS @ 720p on a 4090.\nTLB‑VFI (Jul 25) — Latent Brownian‑bridge diffusion; fills irregular temporal gaps.\nBiM‑VFI — Bidirectional motion‑field model; excels at non‑uniform acceleration.\nRIFE 4.6 / IFRNet‑HD — Fastest real‑time baselines with NCNN/ONNX ports.\n\n👉 Rule of thumb: generate at 15 FPS → interpolate ×2–4 with the above for cinema‑smooth output."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#timelapse-acceleratedaction-generation",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#timelapse-acceleratedaction-generation",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Model\nGenerates …\nNative speed‑up\nLink\n\n\n\n\nMagicTime\nSunsets, plant growth, urban night‑scape\n6–12 ×\nGitHub repo\n\n\nLatte\nScheduler‑skippable Latent DiT\n4–10 ×\nHF weights\n\n\nΔ‑Diffusion\nDemo‑action replay in any scene\nUser‑defined\nGitHub repo\n\n\nMAVIN\nMulti‑move montage & infill\n10–20 s sequences\nGitHub repo\n\n\nTLB‑VFI\nGap‑aware interpolation layer\n16–32 × with others\nGitHub repo"
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#storyboard-keyframe-consistency",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#storyboard-keyframe-consistency",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "2025 tool\nWhere it sits\nEffect\nLink\n\n\n\n\nConsistory\nMid‑generation UNet patch\n6 × lower ID drift\nGitHub\n\n\nStoryCrafter\nPrompt‑time region attention\nFine‑grained style control\nGitHub\n\n\nAudit & Repair\nPost‑hoc LLM loop\nAuto‑fixes drift\nGitHub\n\n\nStoryMaker v2\nPersonalisation LoRA\nLocks face + outfit\nGitHub\n\n\nOne‑Prompt‑One‑Story\nTraining‑free mega‑prompt\nRapid concept art\nGitHub\n\n\n\n\nMetric watch: ViStoryBench now tests semantic + temporal + stylistic coherence—expect papers to report it by default."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#tooling-ecosystem-libraries",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#tooling-ecosystem-libraries",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "ComfyUI‑StoryDiff — Drag‑and‑drop pipelines for Consistory, EchoMimic & LC‑Mamba.\nVideoCrafter2 — Hugging Face toolkit wrapping AAPT, StreamDiT, MagicTime.\nOpen‑Sora sprint — Re‑creating proprietary Sora demos; checkpoints at 512², 12 FPS.\nVulkan‑RIFE and WebGPU‑LC‑Mamba—browser‑side interpolation."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#choosing-the-right-toolbox-expanded-cheatsheet",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#choosing-the-right-toolbox-expanded-cheatsheet",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Scenario\nGPU budget\nLatency target\nStack\n\n\n\n\nVTuber streaming\nRTX 4070\n&lt;200 ms\nLLIA → SyncTalk++ → RIFE ×2\n\n\nProduct demo 1080p/30\ndual 4090\n&lt;2 s\nAAPT → VMoBA → H.265 encode → LC‑Mamba polish\n\n\nSocial‑media hyper‑lapse\nMacBook M3 Pro\noffline\nMagicTime @512² → LC‑Mamba ×2\n\n\nPre‑viz animatic (20 panels)\ncloud A100\n&lt;30 s\nStoryMaker v2 → Consistory → Audit & Repair → LC‑Mamba\n\n\nAR glasses companion\nmobile GPU\n10–30 FPS\nStreamDiT distilled → VFIMamba"
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#open-challenges-research-threads",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#open-challenges-research-threads",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Extreme‑resolution (&gt;4 K) causal generation—open thread on GitHub issue #42.\nUnified multi‑modal control—prototype spec discussed in the PromptFusion RFC.\nEnergy‑aware schedulers for laptops & phones—track progress in the Efficient‑Diffusion‑WG.\nRobustness metrics—draft of FPS‑normed FVD at fvd-fps repo.\nWebGPU kernels—follow efforts in wgpu‑diffusion."
  },
  {
    "objectID": "drafts/20250630-state-of-efficient-video-gen/index.html#key-takeaways",
    "href": "drafts/20250630-state-of-efficient-video-gen/index.html#key-takeaways",
    "title": "Cutting Latency, Keeping Creativity — The 2025 Status of Efficient Video Generation",
    "section": "",
    "text": "Causal, few‑step diffusion + sparse attention is the unlock for real‑time generation.\nVFI is now a first‑class citizen—treat it as part of generation, not post.\nConsistent storytelling is production‑ready via LoRAs & prompt hacks.\nOpen‑source keeps pace with commercial demos—weights under permissive licences abound.\nHardware democratisation—RTX 4070 laptops now rival 2023 cloud nodes.\nBenchmarks mature—ViStoryBench, FPS‑FVD.\nCreative iteration speed wins—faster render loops reshape storyboarding and marketing.\n\n\nIf 2024 was the year of breathtaking yet sluggish video diffusion, 2025 lets creators hit play—and watch results materialise in real‑time with open tools.\n\n\nCompiled July 26 2025 — All links point to public GitHub, Hugging Face, or arXiv unless noted otherwise."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html",
    "href": "posts/20250712-agentic-causal/index.html",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of sciences. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why.\nThe plural on sciences is intentional. I want to emphasize the range of disciplines: physics, chemistry, biology, economics, sociology, computer science, data science, you name it. But in this post I would like to dedicate my attention to causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-sciences",
    "href": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-sciences",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of sciences. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why.\nThe plural on sciences is intentional. I want to emphasize the range of disciplines: physics, chemistry, biology, economics, sociology, computer science, data science, you name it. But in this post I would like to dedicate my attention to causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "href": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "title": "Agentic Causal Inference",
    "section": "Causal Inference and LLMs",
    "text": "Causal Inference and LLMs\n\n\n\n\n\nCausal inference is such an important decision making tool in life and in business. However, to be an expert in this field takes years of mathematical and statistical training. LLMs on the other hand are easy to use, but they lack rigor when reasoning about causality.\nIntegrating causality into LLM agents addresses limitations on both sides:\n\nPure causal methods demand strict assumptions and expert guidance.\nLLMs overflow with knowledge yet often mistake correlation for causation.\n\nBy wiring LLM‑based agents to specialized causal inference libraries, we can automate the causal workflow: discovery -&gt; identification -&gt; estimation -&gt; refutation. The result is a new class of general‑purpose causal AI systems that parse tabular, time-series, and even multimodal data with human-like intuition and mathematical rigor.\nPractically, that means the agent:\n\nThinks (via an LLM) about what causal graph should link your variables\nActs by writing Python: drawing DAGs, running ID algorithms, calling estimators, using libraries like dowhy, econml, causaltune, etc.\nReflects on the results, prompting itself with “Do my assumptions still hold?”\nIterates until a relevant answer surfaces, or it asks you for help.\n\nIf that sounds suspiciously like a data scientist teamate with infinite patience, you’ve got the gist.\n\n10 lines of code for your first causal inference agent\nTo illustrate the idea, here is a minimal snippet to estimate the effect of a new coupon on revenue. This may actually be sufficient to get you a quick answer, if (a big if) the data is ready.\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.llms import OpenAI\nfrom dowhy import CausalModel\nfrom causaltune import AutoTune\n\ndef estimate_ate(df, treatment, outcome):\n    model = CausalModel(data=df, treatment=treatment, outcome=outcome)\n    ided = model.identify_effect()\n    best = AutoTune(model, df).best_estimator_\n    return model.estimate_effect(ided, method_name=best)\n\nagent = initialize_agent(\n    llm=OpenAI(model_name=\"gpt-4o-mini\"),\n    tools=[Tool.from_function(estimate_ate)],\n    agent_type=\"openai-tools\"\n)\n\nagent.run(\"Estimate the uplift of coupon_v2 on weekly revenue\")\nTime to look at some interesting papers and open source projects:\n\n\n\nSingle‑Agent Autonomous Pipelines\nCausal Agent framework (2024):\n\nAn LLM operates in a ReAct‑style loop with a suite of causal tools—e.g. CausalLearn for graph discovery and EconML for effect estimation.\nGiven a dataset and a query (e.g. “Effect of X on Y?”) the agent automatically:\n\nexplores variable correlations,\nhypothesizes causal links,\nproposes a causal graph,\ncomputes the quantitative effect of X on Y.\n\nEach step is backed by library outputs that the LLM interprets before deciding its next move.\n\nThe framework’s hierarchical breakdown (variable‑level, edge‑level, graph‑level, effect‑level) has produced expert‑level accuracy on a testing dataset with 1.3k questions, all while providing interpretable explanations.\n\nCausal-Copilot (2025)\n\nThe agent chains 20 + causal tools, from discovery to hyper-parameter tuning, inside a single LLM loop.\n\nWorks on both tabular and time-series data: prompts the user for a question, auto-selects the right discovery algorithm (e.g., NOTEARS, PC), tunes an estimator (DoubleML, CausalForest, IV), runs refuters, and returns an English report with effect size + CI.\nAchieves state-of-the-art graph accuracy and effect-estimation error across five public benchmarks, edging out both classic SCD baselines and earlier LLM agents.\n\n\n\n\nDebating Multi‑Agent Systems for Causal Discovery\nSingle agents sometimes hallucinate; multi‑agent approaches aim to reduce errors through debate and consensus.\n\nMulti‑Agent Causal Discovery Using LLMs (2024) assigns dedicated LLM roles:\n\nAffirmative Debaters proposes a DAG using temporal cues and domain priors.\nNegative Debaters attacks the proposal by surfacing hidden confounders, incorrect temporal orderings, or omitted variables.\nJudges evaluate arguments and pick the most plausible structure.\nCoders materializes the agreed-upon algorithm, reruns it on the entire dataset, and emits the refined graph.\n\n\nExperiments show these debating agents outperform both classical algorithms and single‑LLM prompts on datasets like Auto MPG, demonstrating that multiple specialized minds can yield more reliable causal graphs.\n\nChain-of-Collaboration Prompting (2025) shows that giving sub-agents explicit roles (planner, verifier, critic) and letting them share scratch pads improves causal reasoning accuracy on CLADDER and Causal-Copilot QA tasks, cutting hallucinated edges by 35 % vs. single-prompt ReAct.\n\n\n\nToolbox Layer (AutoML & No‑Code Platforms)\nParallel to LLM research, we also see AutoML‑style causal platforms that automate model selection, tuning, and robustness checks.\n\nAutoCausality: part of the PyWhy ecosystem, using hyper‑parameter search and ensembling to choose the best estimator for a dataset.\nOpportunityFinder (Amazon 2023) offers code‑less causal studies for panel data cleaning, cohorting, and computing effects (plus sensitivity) end‑to‑end.\nSalesforce CausalAI Library consolidates discovery and inference methods, synthetic data generators, and a no‑code GUI, scaling to larger problems via optimized multiprocessing.\n\nThese toolkits enrich agentic workflows: an LLM planner can mix‑and‑match discovery, estimation, and AutoML selection modules without human intervention."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "href": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "title": "Agentic Causal Inference",
    "section": "Evaluating Causal Inference Agents",
    "text": "Evaluating Causal Inference Agents\nHow well do these causal inference agents perform? Here are some real or synthetic datasets and benchmarks.\nFor treatment‑effect estimation, the Lalonde job‑training study is a good place to start. It has real observational covariates paired with true RCT outcomes—to sanity‑check bias reduction. When larger, controlled replications are needed, you can use semi‑synthetic generators such as IHDP and the Twins dataset, whose perfect counterfactual comes from each twin’s paired outcome. The annual ACIC challenges extend this idea with dozens of high‑dimensional scenarios, while the 2025 RealCause generator allows people to create realistic Lalonde‑style benchmarks.\nFor longitudinal uplift studies, Amazon’s no‑code OpportunityFinder panels ship sample retail datasets ready for difference‑in‑differences.\nWhen it comes to graph discovery methods, people tend to use classic datasets such as the 11‑node Sachs protein‑signaling map, a real wet‑lab interventions dataset. Bayesian‑network classics like Asia and ALARM remain quick smoke tests. Pairwise direction algorithms rely on the Tübingen cause–effect pairs, and larger time‑series graphs come from gene‑regulation contests such as DREAM4.\nMore recently we see language‑centric causal benchmarks. CLADDER has 10k natural language questions across Pearl’s ladder, while ACCESS asks agents to build abstract causal graphs over multimodal vignettes before answering why queries.\nAs to multimodal causal inference, CausalVQA is a benchmark for video question answering (VQA) that test models’ understanding of causality in the physical world."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "href": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "title": "Agentic Causal Inference",
    "section": "Challenges and Mitigations",
    "text": "Challenges and Mitigations\nGoing beyond the happy path to production is often not a smooth ride. Here are some of common challenges in my experience:\n\nData Quality and the Missing Confounders\nObservational datasets rarely contain every variable that shapes a treatment–outcome relationship, so even a state‑of‑the‑art estimator can inherit hidden bias.\nTo mitigate, insert a human‑review checkpoint right after the agent proposes its first causal graph: domain experts eyeball edges and nominate missing covariates. The software then launches automatic robustness probes such as placebo tests, synthetic confounder injections, and other refutation modules shipped with DoWhy, to quantify how fragile the estimate is. Crucially, if any refutation fails, the planner LLM must stop, annotate the failure, and either revise the graph or escalate to a human reviewer; surfacing a shaky result as “tentative” is better than silently proceeding. Some teams also run a “data‑profiling agent” that scans fresh tables for covariate drift or sparsity and warns the planner before analysis starts.\n\n\nHallucinations and Over‑Confidence in Planner LLMs\nLLM planners are persuasive storytellers; a well phrased chain‑of‑thought can make a shaky causal graph feel ironclad.\nMulti‑agent debate is a good recipe to reduce hallucination: a second LLM plays devil’s advocate, and challenges the assumptions that make an estimate causal:\n\nPlacebo‑treatment test: replace the real treatment with a fake; any non‑zero effect flags hidden bias.\nSynthetic‑confounder injection: add a random common cause and observe the ATE shift; big swings imply unmeasured confounding.\nOverlap / positivity audit: verify that propensity scores span both arms; poor overlap triggers trimming or doubly robust methods.\nCross‑estimator consensus: pit a back‑door learner against an IV or front‑door estimator; disagreement above a threshold routes to human review.\nMulti‑agent debate: affirmative and negative debaters contest every edge, a judge scores coherence.\n\nIf any probe fails, the planner either tightens assumptions and reruns discovery or clearly labels the conclusion “inconclusive, additional data needed.” Final reports must surface the point estimate plus confidence intervals, sensitivity ranges, and a pass/fail tally for each refuter, so stakeholders see magnitude and robustness.\n\n\nModel‑Selection Over‑Fit and Cross‑Estimator Disagreement\nAuto‑tuning libraries can explore dozens of learners and hyper‑parameters, sometimes over‑fitting small causal datasets, especially with flexible models like causal forests. In this case, AutoML learns noise instead of real signal.\nMitigations include nested cross‑validation inside AutoCausality or causaltune, and parsimony priors that penalize needless complexity. If resource allows, the agent should run at least two conceptually different estimators, e.g., a back‑door regression and an instrumental‑variable model, and flag any large divergence in effect size as a red‑flag for human review.\n\n\nComputation Cost vs. Real‑Time Ambitions\nA planner–solver split can still burn thousands of tokens and heavy compute if the planner explores many what‑if branches.\nProduction dashboards cache discovery and refutation outputs keyed by a DAG hash; if the graph hasn’t changed, the agent re‑uses prior results. Another recipe is distilling a heavy LLM planner into a small fine‑tuned local model covers day‑to‑day traffic, while the costly cloud model handles weekly deep dives.\n\n\nPrivacy and Governance\nSensitive data such as medical records, customer logs usually cannot leave a private cluster.\nHybrid deployments solve this: an on‑prem LLM handles data‑aware steps, while a redacted summary (no PII) is sent to a cloud model for high‑level planning. All explanations pass through a redaction layer before logging, and every causal report carries an audit trail plus role‑based access controls."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#conclusion",
    "href": "posts/20250712-agentic-causal/index.html#conclusion",
    "title": "Agentic Causal Inference",
    "section": "Conclusion",
    "text": "Conclusion\nCausal inference is transitioning from a highly specialized skill to a widely accessible capability. That’s not putting anyone out of a job. It is freeing us to ask better questions. A couple of years ago, answering “what actually drives our north star metric?” meant a quarter-long project. Today, it may be weeks or even days. That’s not just a productivity gain. It is a fundamental change in how we can think about our businesses."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html",
    "href": "posts/20230802-cloudrun-githubaction/index.html",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "",
    "text": "Deploying ML models and other python apps to cloud can be tedious. Compute instances need to be provisioned; networking needs to be sorted out; autoscaling needs to be configured; secrets and credentials need to be safely managed.\nRather than spending hours on the above Dev-Ops tasks (don’t get me wrong, Dev-Ops and ML-Ops are important), I would like to focus on modeling: recipes that produce the best models and make them available for people to use. After years and many projects, I found Google Cloud Run to be a low maintainence solution, with CI/CD managed by Github Action. Similar solutions can be had with AWS ECS and Azure Container Instances. But this post will focus on Cloud Run."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "href": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along with the tutorial, you need:\n\nDocker\nGoogle Cloud SDK"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "href": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Sample App",
    "text": "Sample App\nLet’s start from a very simple http server and run it locally.\ndocker run --rm -it -p 801:801 python:3.8-slim python -m http.server 801 -d /home/\nRun it locally and we can verify it works by visiting localhost:801 in a browser.\n\nDeploy to Cloud Run manually\nHowever, the above docker image does not quite work for Cloud Run, as Cloud Run requires your app in the docker image to use the PORT environment variable to determine which port the app listens to.\nTo solve this we need to build a simple docker image with the following Dockerfile:\nFROM python:3.8-slim\nENV PORT=8080\nCMD python -m http.server $PORT -d /home\nInstall gcloud and authenticate. Then build and deploy it with the following script (click to expand):\n\n\n\n\n\n\nShell script for deploy to google cloud run\n\n\n\n\n\n# Make sure to fill in the GCP project id:\nproject=your-gcp-project-id\napp=example-app\nplatform=linux/amd64\nregion=us-central1\ndocker build --platform $platform -t example-app-image .\n\nimage=us.gcr.io/$project/$app:latest\ndocker tag example-app-image $image\ndocker push $image\ngcloud run deploy $app --image $image --cpu 1 --memory 1Gi --min-instances 1 --region $region --allow-unauthenticated\n\n\n\nNote that there are a couple of hard-coded defaults like the region (us-central1), and image subdomain (us.gcr.io). Feel free to adjust.\nIf successful, we will see something like this:\n\n\n\n\n\n\nConsole output during deployment\n\n\n\n\n\nDeploying container to Cloud Run service [example-app] in project [your-project-id] region [us-central1]\n✓ Deploying new service... Done.                                                 \n  ✓ Creating Revision...                                                         \n  ✓ Routing traffic...                                                           \n  ✓ Setting IAM Policy...                                                        \nDone.                                                                            \nService [example-app] revision [example-app-...] has been deployed and is serving 100 percent of traffic."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "href": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Manage secrets",
    "text": "Manage secrets\nIf the app needs to access secrets such as API keys and passwords, then it is a necessary to store and manage them securely.\nCreate a secret in GCP’s secret manager, and grant minimal necessary access.\nEach secret is versioned. For example, we may create a secret: MY_API_KEY:latest with latest being the version tag.\nWhen using gcloud run deploy to deploy the app, pass in additional arguments:\n--update-secrets=MY_API_KEY=MY_API_KEY:latest,OTHER_API_KEY=OTHER_API_KEY:latest\nIn the docker container, the secret value will be made available in the environment variable MY_API_KEY."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "href": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Set up a secure Github action for continuous deployment",
    "text": "Set up a secure Github action for continuous deployment\nWhile manually running the gcloud command is sufficient to deploy the app to Cloud Run, sometimes it can make sense to set up continuous deployment triggered by github push or release events.\n\nService account\nFirst, we need to follow these instructions to create a service account and grant some permissions:\nGo to IAM, click “grant access” and set: - principal: the new service account just created - role cloud run admin - role: roles/artifactregistry.createOnPushWriter - role: Secret manager secret accessor\nGrant the default compute-engine account access to Secret Manager Secret Accessor role. Go to IAM and set: - principal: the default compute-engine service account - role: Secret Manager Secret Accessor\nGo to IAM/service accounts, click into the default compute-engine service account, then allow the new service account to use this compute engine service account: - principal: the new service account just created - role: “Service account user”\n\n\n\n\n\n\nTip\n\n\n\nI spent hours debugging permission errors in the github actions and found the above steps helped resolving the errors. More info here and here. However, I suspect some of them are not necessary. Please let me know (zhangzhang.si AT gmail.com) if you have a different experience.\n\n\n\n\nDocker artifacts repository\nA docker artifacts repository must be created in the same project as the Cloud Run service (we assume the location is “us-central1”):\ngcloud artifacts repositories create slack-llm --location=us-central1 --repository-format=docker\nThis artifacts repository will hold the docker image of the app.\n\n\nWorkload identify federation and keyless authentication\nFor better cloud security, Google recommends setting up keyless authentication from github actions. To do that, we need to:\n\n\n\n\n\n\nCreate a Workload Identify Pool\n\n\n\n\n\ngcloud iam workload-identity-pools create \"my-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"Demo pool\" \\\n  --description=\"My Identify Pool\"\n\n\n\n\n\n\n\n\n\nThen create a Workload Identify Provider:\n\n\n\n\n\ngcloud iam workload-identity-pools providers create-oidc \"my-provider\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"my-pool\" \\\n  --display-name=\"Demo provider\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.aud=assertion.aud\" \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\"\n\n\n\n\n\n\n\n\n\nThen allow authentications from the Workload Identity Provider to impersonate the desired Service Account:\n\n\n\n\n\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/attribute.repository/my-org/my-repo\"\nAlternatively, if we do not want to restrict the binding to the specific github repo, then:\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/*\"\n\n\n\n\n\nGithub secrets\nAdd the following github secrets (see instructions on how to add secrets to a github repo):\nWIF_PROVIDER=projects/my-gcp-project-number/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n\nWIF_SERVICE_ACCOUNT=my-service-account@my-project.iam.gserviceaccount.com\n\n\nGithub action yaml file\nNow we should be ready to set up the actual github action. This is a redacted version of my working github action yaml file:\n\n\n\n\n\n\nYAML File\n\n\n\n\n\nYAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}\n\n\n\nPut this in .github/workflows/deploy.yml and the next time you push a change to main, it should automatically deploy to Cloud Run.\nEnjoy!"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "href": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "YAML for Github Action",
    "text": "YAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}"
  }
]
[
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html",
    "href": "posts/20251030-coding-agents-spooky/index.html",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "",
    "text": "Agentic ML tooling is having a moment. Heavyweights like DeepMind’s MLE-Star and Meta’s aira-dojo now headline benchmarks with purpose-built planners, tool graphs, and curated playbooks for data science automation. They are powerful but heavy. Standing them up requires GPUs, bespoke infra, and a willingness to live inside someone else’s workflow.\nDay to day, most of us still reach for slim coding agents that sit on top of git, shell scripts, and a prompt file. I wanted to test how far that lightweight stack can go.\nTo find out, I dusted off the classic Spooky Author Identification playground competition: classify short horror passages by Edgar Allan Poe, Mary Shelley, or H. P. Lovecraft. The metric is multiclass log loss, which punishes overconfident mistakes and rewards calibrated probabilities, a sweet spot for data science automation."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#why-revisit-spooky-authors-in-2025",
    "href": "posts/20251030-coding-agents-spooky/index.html#why-revisit-spooky-authors-in-2025",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "",
    "text": "Agentic ML tooling is having a moment. Heavyweights like DeepMind’s MLE-Star and Meta’s aira-dojo now headline benchmarks with purpose-built planners, tool graphs, and curated playbooks for data science automation. They are powerful but heavy. Standing them up requires GPUs, bespoke infra, and a willingness to live inside someone else’s workflow.\nDay to day, most of us still reach for slim coding agents that sit on top of git, shell scripts, and a prompt file. I wanted to test how far that lightweight stack can go.\nTo find out, I dusted off the classic Spooky Author Identification playground competition: classify short horror passages by Edgar Allan Poe, Mary Shelley, or H. P. Lovecraft. The metric is multiclass log loss, which punishes overconfident mistakes and rewards calibrated probabilities, a sweet spot for data science automation."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#what-i-learned",
    "href": "posts/20251030-coding-agents-spooky/index.html#what-i-learned",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "What I learned",
    "text": "What I learned\nRunning four different coding agents (Codex, Composer, Claude, and Gemini) on the same Kaggle challenge revealed three clear patterns:\nLightweight setups work. Minimal prompting plus three helper scripts (setup_env.sh, prompt.sh, run_iterations.sh) was enough for modern coding agents to beat the Kaggle median and approach bronze-tier scores. Codex landed a 0.35897 log-loss, comfortably above the 0.41879 median threshold.\nAutomation moves fast but leaves a mess. All four agents sprawled dozens of experiment entries, cached matrices, and partially refactored modules. They ship improvements, yet still force a human to reconcile redundant scripts, prune dead notebooks, and audit for data leakage before anything is production-ready. I spent as much time cleaning up as I did setting up experiments.\nThe last mile remains human. Claude pushed CV log loss down to 0.3447 with a TF-IDF + MLP ensemble, better than the others but still far from heavyweight systems like MLE-Star or aira-dojo. Lightweight agents democratize experimentation, but careful curation, leak checks, and recipe tuning still demand deliberate data science work. The agent can’t tell you if its clever trick is actually data leakage."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#lightweight-vs-heavyweight-agents",
    "href": "posts/20251030-coding-agents-spooky/index.html#lightweight-vs-heavyweight-agents",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "Lightweight vs heavyweight agents",
    "text": "Lightweight vs heavyweight agents\nWhy bother with coding agents when MLE-Star and aira-dojo exist? Because those systems come with significant weight:\n\nMLE-Star (DeepMind) stitches together planners, verifiers, and domain-specific tooling to deliver near push-button benchmarks. Impressive, but each deployment assumes a curated environment, specialized hardware, and a dataset config pipeline.\naira-dojo (Meta) provides a dojo of scripted data science routines and reinforcement-learned policies. Powerful for internal benchmarks, yet not something you spin up on your MacBook before lunch.\n\nLightweight coding agents trade raw performance for accessibility. They inherit your repo, follow your version control, and let you stay close to the code. In exchange, you own the cleanup and the decision to productionize.\nFor most teams, that’s a fair trade: let the agent generate a strong baseline quickly, then have humans tighten the screws. The question isn’t whether heavyweight systems are better (they obviously are, when you have the resources). The question is whether lightweight agents are good enough to change how we work. Based on this experiment, I think they are."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#a-tiny-orchestration-rig",
    "href": "posts/20251030-coding-agents-spooky/index.html#a-tiny-orchestration-rig",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "A tiny orchestration rig",
    "text": "A tiny orchestration rig\nHere’s what surprised me most: you don’t need any special infrastructure. All four agent repos share the same skeleton:\n\nuser_prompt.txt – a one-page spec that forces the agent into a disciplined loop: load context, declare a budget, run 2–3 experiments, journal, and update status files.\nsetup_env.sh – stands up a virtualenv, installs pinned deps, and even pre-downloads NLTK packages so the agent never has to ask for credentials.\nprompt.sh & run_iterations.sh – thin wrappers that activate the venv, launch the chosen CLI (claude, gemini, codex, cursor), and optionally auto-commit after each loop.\n\nThat’s it: no bespoke backend, no orchestration server. The agent sees the repo exactly like a junior data scientist would: git history, prior experiments, and a scratchpad of ideas. This “minimal stack” was enough for Codex to discover multi-seed ensembles and for Composer to run 60 experiments in under a day.\nThe simplicity is both a strength and a weakness. It’s easy to start, but the lack of guardrails means agents will happily create duplicate pipelines, conflicting experiment IDs, and scripts that assume files exist in the wrong places."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#how-the-agents-performed",
    "href": "posts/20251030-coding-agents-spooky/index.html#how-the-agents-performed",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "How the agents performed",
    "text": "How the agents performed\nI expected the agents to struggle with a task this nuanced. Author attribution from short text snippets is subtle work, requiring the model to pick up on stylistic tics and vocabulary patterns. But watching them work revealed something surprising.\nLog loss is a “lower is better” metric. The Kaggle leaderboard median sits at 0.4188, and the bronze cutoff (top 10%) is 0.2938. The table below shows each agent’s score compared to that median: negative percentages mean the agent beat the median.\n\n\n\n\n\n\n\n\n\nAgent\nLog Loss\nvs. Kaggle Median\nSummary\n\n\n\n\nClaude\n0.37222\n-4.7%\n3-seed TF-IDF + MLP blend. Stacking meta-learner backfired.\n\n\nCodex\n0.35897\n-6.0%\nMulti-seed TF-IDF + logistic ensemble. Still needs full-train refit.\n\n\nComposer\n0.38715\n-3.2%\nTF-IDF + logistic with sublinear scaling. Exploring vocab tuning.\n\n\nGemini†\n0.42398\n+0.5%\nWord+char features. Over-regularized at first (C=0.1 → 0.696).\n\n\n\n† Gemini ran on gemini-flash-2.5 because my gemini-pro-2.5 quota was exhausted.\n\nDoes this mean lightweight agents are ready for production? Not quite. With one task and one run per agent, these deltas are easily within noise. No overall champion crowned. But three of four agents beat the Kaggle median on their first serious attempt, which suggests the orchestration pattern itself is sound.\nWhat struck me was how differently each agent approached the problem. Codex went wide with multi-seed ensembles. Claude pushed harder on model architecture (adding MLPs). Composer methodically swept hyperparameters. Gemini stumbled early with over-regularization but corrected course. Each strategy reflects the underlying model’s tendencies, but all converged on TF-IDF as the feature foundation.\n\n\nRepresentative session logs (expand for highlights from each agent’s run)\n\n\nCodex full sweep: five-model ensemble where stylometric probabilities took 50% of the final weight\nComposer mid-run: Oct 29 session documented a 4.4% log-loss drop after raising max_features to 25k\nClaude iteration 10: stacking meta-learner chewed 138 minutes only to land 10.4% worse than simple blend"
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#what-the-agents-leave-behind",
    "href": "posts/20251030-coding-agents-spooky/index.html#what-the-agents-leave-behind",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "What the agents leave behind",
    "text": "What the agents leave behind\nThe hardest part wasn’t the setup or the experimentation. It was the cleanup.\nAutomation litters the repo. Codex’s run now tracks five variants of the same logistic pipeline, complete with separate OOF dumps, weight search scripts, and registry YAMLs. Composer’s train.py mixes LightGBM, logistic regression, handcrafted features, and a sentence-transformer branch inside a single file that keeps toggling SKIP_EXISTING_EXPERIMENTS. The agents do not delete anything; they prototype, leave artifacts behind, and move on.\nNot every artifact is junk. The streaming JSON logs double as a lab notebook: one Codex session diagnosed short texts (21–81 characters) as the chief failure mode (76.8% accuracy and 0.566 log loss vs. 93.7% / 0.204 for long passages) and immediately reprioritized feature work around that gap. That’s valuable signal buried in transcript.\nThe messy bits are the code paths, not the telemetry. Before shipping any of this to production, you must:\n\nAudit data usage – repeated calls to RepeatedStratifiedKFold with the same seed should not leak across iterations; make sure cached matrices respect fold boundaries.\nNormalize experiment logging – ensure experiments.csv retains consistent schemas so future analysis can reason about which parameters actually mattered.\nRefit cleanly – the best-performing ensemble in Codex is still expressed as a scratch script; it needs a single entry point that trains on the full dataset and regenerates predictions deterministically.\n\nI estimate I spent 40% of my time cleaning up after the agents. That’s still a net win (they generated hypotheses and ran experiments faster than I could manually), but it’s not “press button, get solution.”\n\n\n\nField notes from the logs (expand for detailed iteration-by-iteration findings)\n\n\nCodex\n\nThe very first iteration slashed log loss from 0.4660 to 0.3875 and surfaced author-specific tokens (Poe’s “of the/upon”, Lovecraft’s “though/west”, Shelley’s character cues), validating the word+char TF-IDF baseline. Journal entry\nA follow-on run built out OOF persistence, averaged three min_df=2 seeds plus a min_df=3 variant, and pushed the ensemble to 0.37643 log loss without touching the test set. Journal entry\nDiagnostics logged the LightGBM collapse (≥0.49 log loss) and the HPL→EAP confusion hotspot (585 errors), motivating Lovecraft-specific features rather than yet another booster. Journal entry\nNot every bet landed: 256-component SVD exploded to 0.59 log loss, and repeated CV runs confirmed the 0.3819 hero score was partly optimistic variance. Journal entries\nApproximate wall-clock: ~4 h 52 m between the first and last Codex logs (idle gaps included).\n\n\n\nComposer\n\nSentence-transformer embeddings bombed at 0.6715 log loss, underscoring that semantics alone can’t beat stylistic n-grams for authorship. Journal entry\nJoint tuning of C and vocabulary width marched the logistic baseline from 0.452 to 0.427, with most of the lift coming from expanding max_features to 10k and 25k. Journal entry\nThe final lap combined word bigrams, 30k features, sublinear_tf=True, and C=4.5 to reach 0.3943 log loss and ship the current submission. Journal entry\nAlong the way, a “stylometric booster” actually cratered performance to 0.680 and the agent tripped a docstring syntax error (both logged, both fixed within the same session). Session log\nApproximate wall-clock: ~6 h 59 m from the earliest to latest Composer logs on 2025-10-29.\n\n\n\nClaude\n\nEarly error analysis quantified the short-text tax (24.7% error under 10 words) and the dominant confusions (MWS→EAP 10.7%, HPL→EAP 10.1%), guiding later work toward richer features. Journal entry\nLearning-rate tuning dropped the MLP to 0.3519 log loss, and blending it 30/70 with the logistic model yielded the 0.3495 ensemble before multi-seed averaging took over. Journal entry\nStretching to five seeds or inserting batch norm both backfired (2.72% and 36% worse respectively), highlighting how easily variance can explode in sparse TF-IDF space. Journal entry\nThe stream logs even capture the stacking meta-learner grinding for 2.3 hours, overshooting the 90-minute budget, and still finishing 10.4% worse than the simple blend. Session log\nApproximate wall-clock: ~13 h 55 m for the Claude run (stacking iterations account for much of it).\n\n\n\nGemini\n\nBaseline TF-IDF + logistic regression hovered around 0.43 log loss; an overly strong regularization run (C=0.1) erupted to 0.696 before the agent marched back down by loosening C and adding text-length features. Journal entry\nApproximate wall-clock: ~2 h 50 m for the opening Gemini sweep (run on gemini-flash-2.5).\n\n\n\n\nExperiment timelines (expand for iteration-by-iteration progression)"
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#what-i-learned-about-each-agent",
    "href": "posts/20251030-coding-agents-spooky/index.html#what-i-learned-about-each-agent",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "What I learned about each agent",
    "text": "What I learned about each agent\nRunning this experiment taught me that lightweight agents aren’t just cheaper versions of heavyweight systems. They’re a different tool entirely, and each revealed its own personality:\nCodex favors breadth over depth. It discovered multi-seed ensembles naturally, averaging across parameter variations rather than optimizing a single model. The weakness: variance estimation. Its best score might be luck, and it left behind five competing pipelines without a clear consolidation path.\nComposer is methodical. It marched through feature space systematically, from 10k to 25k vocabulary size, documenting every gain. The payoff was steady improvement (0.452 → 0.394), but it could use more architectural ambition beyond linear models.\nClaude took calculated risks. Adding a small MLP on top of TF-IDF paid off (best CV score: 0.3447), but its 138-minute stacking experiment backfired spectacularly. It taught me that simple blends often beat complex meta-learners, at least in sparse feature spaces.\nGemini stumbled early. Over-regularization (C=0.1) exploded to 0.696 log loss before it corrected course. Running on the lighter gemini-flash-2.5 model may have limited its ceiling, but it still landed near the median on its first serious attempt.\nMeanwhile, I’ll keep an eye on the heavy hitters. When MLE-Star or aira-dojo become easier to adopt in a local workflow, they will likely leap past these handcrafted setups. Until then, a simple prompt file, a few shell scripts, and a patient coding agent already unlock a lot of Kaggle-style experimentation, so long as a human data scientist is ready to polish the results."
  },
  {
    "objectID": "posts/20251030-coding-agents-spooky/index.html#appendix-full-repository-links",
    "href": "posts/20251030-coding-agents-spooky/index.html#appendix-full-repository-links",
    "title": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge",
    "section": "Appendix: Full repository links",
    "text": "Appendix: Full repository links\nAll code, experiment artifacts, and raw stream-json transcripts are public:\n\nCodex: Status · Setup script · Session log\nComposer: Status · Early session · Final session\nClaude: Status · Session log\nGemini: Status"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html",
    "href": "posts/20241002-minisora-part1/index.html",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#introduction",
    "href": "posts/20241002-minisora-part1/index.html#introduction",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "",
    "text": "In February 2024, OpenAI introduced SORA, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos exhibit 3D consistency and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.\nInspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s $20 90-minute training run for GPT-2. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.\nThanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 on Lambda Labs, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this report on Weights & Biases."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "href": "posts/20241002-minisora-part1/index.html#pareto-frontier-aiming-for-good-and-small",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Pareto frontier: Aiming for good and small",
    "text": "Pareto frontier: Aiming for good and small\n\n\n\n\n\nThis graph from “Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI” illustrates a key challenge in AI development. SORA would be a frontier model at the resource-intensive end of the spectrum. We want to move in the direction of the green arrow, striving for lower training cost while maintaining high quality.\nAccess to vast computational resources, such as 10,000 A100 GPUs, is limited to a handful of organizations. Even if such resources were widely available, focusing solely on resource-intensive methods would be an inefficient use of our capabilities. The design space for training recipes is vast, and a strategic approach involves exploring this space through low-cost experiments before scaling up when confidence is high.\nThis raises an intriguing question: With a modest budget, is it possible to train a general-purpose video generation model comparable to SORA?"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "href": "posts/20241002-minisora-part1/index.html#the-need-for-controlling-the-domain-complexity",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "The need for controlling the domain complexity",
    "text": "The need for controlling the domain complexity\n\nThe challenge of training general-purpose models with limited resources\nSORA’s training costs likely run into tens of millions of dollars, driven by both data and model size. Larger datasets necessitate longer training times, while bigger models require both extended training periods and more high-end GPUs.\nIs it feasible to train a high-quality model with a significantly smaller dataset? This seems impossible due to the inherent complexity of our world. Are one million video clips sufficient to capture our world’s complexity? 10 Million? 100 Million? Probably more than that. While sample-efficient algorithms can help reduce the required data size, the order of magnitude for necessary data likely remains substantial.\nSimilarly, training a high-quality model with a much smaller architecture presents its own challenges. Unless a dramatically more efficient architecture than Transformers emerges, a small model would struggle to capture the complexity present in such vast datasets.\nTherefore, to make progress with limited resources, we must find ways to reduce the data size.\n\n\nExploring niche domains: A path to low-budget training\nNiche domains can be significantly simpler than our physical world, potentially allowing a few tens of thousands of observations to sufficiently represent the domain. With a drastic reduction in data size, smaller models and lower training costs become feasible.\nWe can conceptualize a series of domains, progressing from simple to complex:\n\n2D Flying MNIST (a 2D world with colorful handwritten digits moving at constant speed, bouncing off boundaries)\n2D arcade games (Pong, Breakout, etc.)\nAnime and cartoons\nLimited locations: video walkthroughs of 3D house models, fly-through views of objects (e.g., NERF models)\nLimited objects: close-up videos of specific subjects (e.g., dogs, selfie videos)\nLimited scenery: footage of hiking trails, beaches, etc.\nPublic video datasets: UCF-101, Panda-70M, InterVid, etc.\nThe real world, and our collective video reservoir.\n\nA strategic approach involves starting from the simplest domain and gradually progressing towards more complex ones. Effective training recipes discovered in simpler domains are expected to scale to more complex scenarios with straightforward increases in data and model size.\nInterestingly, this mirrors how humans learn: start from simple lessons and gradually build up to more complex concepts.\n\n\nPre-train or fine-tune?\nFine-tuning is an effective strategy to reduce training costs, but it comes with certain limitations:\n\nFixed architecture: The model’s architecture is predetermined, which can be a significant constraint as we may still be far from an optimal design for video generation tasks.\nVAE dependency: Pre-trained weights often rely on a specific Variational Autoencoder (VAE), limiting the design space and opportunities to further reduce training costs.\n\nDespite these limitations, fine-tuning has shown promising results. For example, the team at Lambda Labs open-sourced an intriguing Text2Bricks experiment, fine-tuning OpenSora weights on Lego videos. This project required approximately 1000 A100 GPU hours and 10,000 videos. We can anticipate further reductions in cost as more advanced pre-trained models become available and more sample-efficient fine-tuning algorithms are developed.\nFor my experiments, I try to find the simplest domain that has non-trivial complexity: a toy 2D world with flying digits. The scale of this toy world is small enough that pre-training from scratch is not prohibitively expensive, allowing for more freedom in exploring different model architectures and training strategies.\nLet’s see some details."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "href": "posts/20241002-minisora-part1/index.html#flying-mnist-simulator",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Flying MNIST Simulator",
    "text": "Flying MNIST Simulator\nA Python script is used to simulate a toy 2D world where colorful handwritten digits fly and bounce around. An example is shown below.\n\n\n\n\n\nFor training, I used up to 100k clips, each with 32 frames, covering roughly 6 seconds at 5 fps. This amounts to 160 hours of video. Is this a lot? Let’s compare with human learning. If a baby is awake and actively observing 5 hours per day, it would be roughly a month of learning. It would be interesting to see if the AI can learn:\n\nObject identity: a digit is a digit, and not a random blob\nObject permanence: a digit does not suddenly disappear\nDistinct digits: whether the model can learn to distinguish between different digits\nConsistent colors: color of a digit remains consistent\nPhysics: digits follow simple physics - constant speed and bounce off walls"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "href": "posts/20241002-minisora-part1/index.html#vae-the-compressor",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "VAE: The Compressor",
    "text": "VAE: The Compressor\nThe first model to train is a compressor. Unlike language, images and videos have very high dimensionality: a tiny 2-second 256x256 video contains over 100 million numbers. Compression is necessary for the model to work.\nThe compressor of choice is a VAE (Variational Auto-Encoder) with an encoder and decoder. The encoder converts a video clip into a latent space, and the decoder converts the latent space back to a video clip. The latent space is a compact representation of the original data and is easier to model.\nOptionally, you can quantize the latent space using vector quantization, which gives you a VQ-VAE. Quantization gives rise to a vocabulary of visual words or tokens. This enables the use of language model training recipes on 1-dimensional (flattened) sequences of token IDs. While I was initially skeptical, the results were surprisingly good.\nTraining a small VAE is relatively quick. I trained a spatial-temporal VQ-VAE with 4x temporal compression and 4x4 spatial compression, using a vocabulary size of 5120. The training run documented in Weights & Biases achieved a good balance of reconstruction quality and compression rate. It took about 2 A10 GPU hours to converge.\nWith this VAE model, you can transform a 32-frame video clip (32 x 3 x 256 x 256) into latent “tokens”. Without quantization, the compressed representation of the video has a shape of 8 x 4 x 64 x 64 (each “token” is a 4-dimensional floating point vector, and there are 8 x 64 x 64 = 32,768 tokens). With quantization, the compressed representation is simply 8 x 64 x 64 = 32,768 integers (token IDs). The range of the token IDs is from 0 to 5,023.\n\n\n\n\n\nWith this compact tokenized representation, we are ready to train a generator."
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "href": "posts/20241002-minisora-part1/index.html#generator-in-the-latent-space",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "Generator in the Latent Space",
    "text": "Generator in the Latent Space\nThere are two approaches to generate video in the latent space: the autoregressive next-token predictor (language model) and the diffusion model.\n\nAutoregressive Next-Token Predictor\nEach 32-frame video clip is represented as a sequence of 32,768 tokens. The video clips are then concatenated to form a long sequence, separated by a special start-of-video token. This long sequence is fed into a language model training recipe.\nI used nanoGPT to train a 60MB model with the GPT-2 architecture. The model is trained to predict the next token ID in the latent space, instead of the next English token. It worked surprisingly well and began to learn the spatial-temporal patterns quickly.\nThe main ingredient for video quality is ensuring a sufficiently large context window. I used 6,000 tokens, which is much larger than the typical GPT-2 setting. However, this is still a small window size for video. Each video frame is 4,096 tokens, so this context window allows the model to look back only slightly more than one frame, making temporal consistency challenging to enforce.\nSecondly, the training sample size is crucial. Using 100k clips produces better results than 10k clips, and much better than 1k clips. The question remains whether we should use even more data. I hope not, as if such a simple 2D world requires much more than 100k training examples, it would be concerning for more complex domains.\nThis training run showcases one of the better results using nanoGPT.\nThe generated videos start out as random compositions of visual tokens:\n\n\nVideo\n\n\nAfter 6 hours of training, line strokes started to appear:\n\n\nVideo\n\n\n24 hours in, the digits began to emerge, but temporal consistency was poor:\n\n\nVideo\n\n\nAfter 10 days, consistency and physics were much improved:\n\n\nVideo\n\n\nFor comparison, here’s a training run using a 1,024 token context window.\nWith a smaller context window, the training time is much shorter (1 day to converge), but temporal consistency is poor, and digits would suddenly appear throughout the clip:\n\n\nVideo\n\n\n\n\nDiffusion Model\nFor the diffusion model, I used ST-DIT from OpenSora and Stable Diffusion’s SD VAE.\nIn this approach, the context window encompasses the entire video clip, so I expected more temporal consistency than the autoregressive counterpart. Training sample size still plays a significant role. Using a 24GB A10 GPU, I needed to use a small version of the diffusion transformer model.\nA representative training run can be found here.\nThe generated videos also start out as random compositions of visual tokens (resembling crops of natural images this time):\n\n\nVideo\n\n\nAfter one day of training, localized dream-like flowing patterns emerged, though they didn’t yet resemble digits:\n\n\nVideo\n\n\nOn day 3, the moving patterns began to look like digits, but they were so fluid that they seemed to lack “bones”-like structure:\n\n\nVideo\n\n\nBy day 10, the digits were much more stable and distinct, and the moving patterns were steady and smooth:\n\n\nVideo"
  },
  {
    "objectID": "posts/20241002-minisora-part1/index.html#whats-next",
    "href": "posts/20241002-minisora-part1/index.html#whats-next",
    "title": "MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)",
    "section": "What’s Next",
    "text": "What’s Next\n250 A10 hours (or approximately 80 A100 hours, costing around $200) proved sufficient to adequately solve the video generation task for the 2D toy world of Flying MNIST Digits.\nContext window size and data sample size are important factors for quality, but also drive up cost. There are numerous new techniques that are worth exploring to improve quality while reducing cost. Here’s a non-exhaustive list:\n\nFlow matching: This technique could enhance the temporal consistency of generated videos.\nBetter quantized VAE for auto-regressive video generation: Improving the VAE could lead to more efficient and higher-quality latent representations.\nToken masking: This could reduce the \\(N\\) in the \\(O(N^2)\\) complexity of attention layers, potentially speeding up training and inference.\nCoarse-to-fine generation: Generating whole video frames at the coarse level first, then progressively refining to small details. This can dramatically reduce the context window size and compute cost.\nBetter positional encoding for long context windows in the temporal-spatial setting.\nHyper-optimized LLM training with long context (e.g., llm.c).\nCombining strengths of autoregressive and diffusion models could yield interesting results.\nCurriculum learning: Starting with simpler tasks and progressively increasing difficulty could improve learning efficiency.\n\nThese avenues for improvement suggest that there’s still significant potential to enhance the quality and efficiency of video generation models, even in this simplified domain. As we continue to refine these techniques, we’ll be better positioned to tackle more complex video generation tasks in the future.\nMore to come."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html",
    "href": "posts/20240825-sustainable-future/index.html",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "",
    "text": "Since the advent of the free market, human society has experienced an unprecedented wave of growth and prosperity. Global GDP has increased 100-fold, with per-capita GDP rising 15-fold since the early 1800s. However, this tremendous growth has exacted a significant toll on the environment. As we stand on the brink of another nascent revolution, artificial intelligence (AI) can usher in a second wave of growth—this time, much more sustainable. Could AI help us achieve the elusive goal of expanding our economies while preserving the planet?"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "href": "posts/20240825-sustainable-future/index.html#the-miracle-and-pitfall-of-demand-driven-production",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "The Miracle and Pitfall of Demand-Driven Production",
    "text": "The Miracle and Pitfall of Demand-Driven Production\n\nThe miracle\nThe past two centuries have indeed been nothing short of a miracle in terms of economic growth, not just for the sheer scale of economic expansion but for its profound impact on human well-being.\nBefore the Industrial Revolution, global poverty was widespread, with the vast majority of the population living on subsistence agriculture, vulnerable to disease, famine, and political instability. But with the advent of mechanized production, steam power, and eventually electricity, societies began to shift from agrarian economies to industrial ones, spurring rapid urbanization and creating millions of new jobs.\n\n (source)\n\n (source)\nAs economies grew, so did living standards. In the 20th century, especially after World War II, growth accelerated dramatically. Advances in medicine, sanitation, and food production allowed populations to boom while simultaneously reducing mortality rates. Global poverty, which once seemed an inescapable fate for most, began to decline sharply. According to the World Bank, extreme poverty (defined as living on less than $1.90 a day) fell from about 80% of the world’s population in 1820 to less than 10% today. This reduction in poverty was most pronounced in Asia, where countries like China and India harnessed industrialization and global trade to lift hundreds of millions out of destitution.\nAs the engines of industry roared to life, they did more than just produce—they created a world where, for the first time, sufficient goods could be made to meet the needs of millions. Farms, once worked by hand, now harnessed the power of machines, yielding crops at unprecedented rates. Factories churned out textiles, tools, and eventually, the comforts of modern living that had once been unimaginable luxuries. This newfound capacity wasn’t just about survival; it was about abundance. Goods that had once been scarce or accessible only to the wealthy became attainable for the masses. Food production soared, homes were built, and technologies that improved everyday life spread across the globe. In this wave of growth, the world became a place where production was not only sufficient but could also fulfill the aspirations of those who sought more than just the bare necessities.\n\n\nThe pitfall\nWhile we feel grateful for the growth and abundance that this era of production has brought us, it’s important to recognize the shadows cast by this prosperity. For every product that meets a need, there are countless others that sit unused, discarded, or wasted. The very systems that allowed us to produce more than ever before also led to overproduction, filling landfills with excess and polluting our air and waters with the byproducts of unchecked growth.\n\n\n\nCar graveyard after Chinese company went bankrupt. Source: @Wolf of X\n\n\n\n\n\nAerial picture of the tire graveyard in Kuwait. Final resting place of over 7,000,000 rubber tires. Source: @Wolf of X\n\n\n\n\n\nClothing graveyard. The so-called “clothing graveyard,” about 30,000 tons of discarded clothing piled in a landfill in the Atacama Desert, Chile, in 2021. Source: Antonio Cossio—picture alliance/Getty Images\n\n\nIs such a level of waste inevitable? I would argue that it is, given the nature of how our economies have evolved. The growth we’ve witnessed, particularly over the last century, has been driven largely by demand—an insatiable appetite for more. With the rise of consumerism, the focus shifted from simply meeting needs to creating new desires. As historian Frederick Allen observed, “Business had learned as never before the importance of the ultimate consumer. Unless he could be persuaded to buy and buy lavishly, the whole stream of six-cylinder cars, super heterodynes, cigarettes, rouge compacts, and electric ice boxes would be dammed up at its outlets.” (source)\nThis relentless push to fuel demand led companies to innovate not just in production but also in persuasion. Advertising, marketing, and product design all became tools to keep the consumer engaged and always wanting more. The result? A system where the pressure to buy, to replace, and to upgrade created a cycle of overproduction and, inevitably, waste."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "href": "posts/20240825-sustainable-future/index.html#is-consumerism-at-fault",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Is consumerism at fault?",
    "text": "Is consumerism at fault?\nThe solution is not to stay away from consumerism and demand-driven market economy. Without demand, there would be no profit, and without profit, companies would have no reason to put products on the market. This, in turn, would halt productivity, leaving not enough food on families’ tables or goods in their homes.\nOver-production is also inevitable. The reality is that producing just enough to meet actual needs isn’t sufficient, because market systems and distribution networks are inherently imperfect. Food, clothes that are produced do not always reach who need them. True efficiency is hard to achieve, and inequality makes this even worse. If the distribution efficiency is only 10%, then we must produce ten times the necessary amount to meet the demand. This excess production, while ensuring availability, often leads to surplus and waste.\nSurplus eats into profits if it isn’t consumed. To keep factories running, corporations thriving, and jobs secure, our dear consumers must continually want more. This is the crux of the demand-driven economy: without constant consumption, the entire system risks stagnation. As a result, businesses invest heavily in marketing, innovation, and new product lines to stimulate desire, encouraging consumers to keep buying—whether or not their needs have truly changed.\n\n\n\nGrowth of supply and manufactured demand beyond need"
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "href": "posts/20240825-sustainable-future/index.html#a-way-out-targeted-production-with-ai",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "A way out: targeted production with AI",
    "text": "A way out: targeted production with AI\nAmazon’s inventory planning system points to a promising direction. Algorithms can forecast what consumers are likely to purchase with remarkable accuracy, which allow buying and placing inventory accordingly to optimize order fulfillment. As a result, efficiency went up, and waste went down.\nAnd we can push this even further. If demand is way higher than actual need, why not shift production to better match what people really need? Imagine if we weren’t constantly hit with endless ads and social media bragging. Our homes would be less cluttered, people wouldn’t need to take on debt just to keep up with the luxury status game, and we could all spend more time with loved ones or out in nature. Life would feel simpler and more focused on what really matters, rather than being driven by overconsumption.\nThis can happen through targeted production, with AI helping in two ways: automation (boosting production efficiency) and forecasting (improving market efficiency).\nAutomation isn’t new—it’s been part of past tech revolutions—but AI is different because it’s more versatile. It can handle many things from language tasks to tool use, extending the ‘Crown Jewels’ of human intelligence. AI can streamline workflows across corporate functions like accounting, finance, engineering, sales, and marketing, making processes faster and more efficient. Forecasting will further increase market efficiency by accurately predicting demand, allowing businesses to align production more closely with real-time consumer needs. These two factors—automation and forecasting—make anticipatory production and just-in-time production possible. Instead of waiting for demand to fully materialize, we can anticipate and initiate production just ahead of time—producing what is likely needed, when it’s needed.\nIndirectly, AI can help curb the constant stimulation of consumer desires. The problem isn’t advertising itself but rather the excessive advertising that arises in overcrowded, saturated markets. When businesses struggle to meaningfully differentiate their products, they rely heavily on aggressive marketing to capture attention, contributing to the cycle of overconsumption. This reflects poor planning and a lack of clear insight into what consumers truly need—a symptom of incomplete information and insufficient foresight into future demands.\nWhen businesses begin to realize they can be profitable with automation and better planning instead of excessive advertising, they can step back from the exhausting zero-sum game of trying to out-market each other. Their shareholders and employees can finally find peace."
  },
  {
    "objectID": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "href": "posts/20240825-sustainable-future/index.html#looking-ahead",
    "title": "From Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth",
    "section": "Looking ahead",
    "text": "Looking ahead\nThe AI revolution is still in its early days, and there are challenges like job displacement and energy use that worry people. But despite these hurdles, I am hopeful AI can help future generations enjoy a more sustainable and prosperous future."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About this blog",
    "section": "",
    "text": "Notes on practical AI and engineering."
  },
  {
    "objectID": "about.html#zz-si",
    "href": "about.html#zz-si",
    "title": "About this blog",
    "section": "ZZ Si",
    "text": "ZZ Si\n\nCo-founder and Engineer @KUNGFU.AI\nExpertise: Computer vision, Generative models, Practical AI deployment\nPreviously: Apple, Google, Expedia, Impossible Ventures (acquired by Capital One), Vicarious (acquired by Google Deepmind)\nPh.D. Stats @UCLA’11, B.S. CS @Tsinghua’06"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Notes on practical AI, engineering and life",
    "section": "",
    "text": "Ghost in the Repo: Lightweight Coding Agents on Kaggle’s Spooky Challenge\n\n\n\n\n\n\nai\n\n\ndata science\n\n\ncoding agents\n\n\n\n\n\n\n\n\n\nOct 30, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nAgentic Causal Inference\n\n\n\n\n\n\nai\n\n\nllm\n\n\nagentic\n\n\ncausal inference\n\n\n\n\n\n\n\n\n\nJul 12, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMiniSora: Learnings from training a Minimal Video Generation Model (Part 1)\n\n\n\n\n\n\ngenerative ai\n\n\nvideo generation\n\n\ncost efficient training\n\n\nscaling laws\n\n\n\n\n\n\n\n\n\nOct 2, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nComputer vision for soccer games\n\n\n\n\n\n\ncomputer vision\n\n\nai\n\n\nsports\n\n\nsoccer\n\n\n\n\n\n\n\n\n\nSep 15, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nFrom Consumerism to Sustainability: AI’s Role in Shaping the Future of Economic Growth\n\n\n\n\n\n\neconomics\n\n\nai\n\n\nenvironment\n\n\n\n\n\n\n\n\n\nAug 25, 2024\n\n\nZZ Si\n\n\n\n\n\n\n\n\n\n\n\n\nDeploying machine learning apps to Google Cloud Run with Github actions\n\n\n\n\n\n\ncode\n\n\nmlops\n\n\nGCP\n\n\ncloud\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nZZ Si\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html",
    "href": "posts/20230802-cloudrun-githubaction/index.html",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "",
    "text": "Deploying ML models and other python apps to cloud can be tedious. Compute instances need to be provisioned; networking needs to be sorted out; autoscaling needs to be configured; secrets and credentials need to be safely managed.\nRather than spending hours on the above Dev-Ops tasks (don’t get me wrong, Dev-Ops and ML-Ops are important), I would like to focus on modeling: recipes that produce the best models and make them available for people to use. After years and many projects, I found Google Cloud Run to be a low maintainence solution, with CI/CD managed by Github Action. Similar solutions can be had with AWS ECS and Azure Container Instances. But this post will focus on Cloud Run."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "href": "posts/20230802-cloudrun-githubaction/index.html#prerequisites",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Prerequisites",
    "text": "Prerequisites\nTo follow along with the tutorial, you need:\n\nDocker\nGoogle Cloud SDK"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "href": "posts/20230802-cloudrun-githubaction/index.html#sample-app",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Sample App",
    "text": "Sample App\nLet’s start from a very simple http server and run it locally.\ndocker run --rm -it -p 801:801 python:3.8-slim python -m http.server 801 -d /home/\nRun it locally and we can verify it works by visiting localhost:801 in a browser.\n\nDeploy to Cloud Run manually\nHowever, the above docker image does not quite work for Cloud Run, as Cloud Run requires your app in the docker image to use the PORT environment variable to determine which port the app listens to.\nTo solve this we need to build a simple docker image with the following Dockerfile:\nFROM python:3.8-slim\nENV PORT=8080\nCMD python -m http.server $PORT -d /home\nInstall gcloud and authenticate. Then build and deploy it with the following script (click to expand):\n\n\n\n\n\n\nShell script for deploy to google cloud run\n\n\n\n\n\n# Make sure to fill in the GCP project id:\nproject=your-gcp-project-id\napp=example-app\nplatform=linux/amd64\nregion=us-central1\ndocker build --platform $platform -t example-app-image .\n\nimage=us.gcr.io/$project/$app:latest\ndocker tag example-app-image $image\ndocker push $image\ngcloud run deploy $app --image $image --cpu 1 --memory 1Gi --min-instances 1 --region $region --allow-unauthenticated\n\n\n\nNote that there are a couple of hard-coded defaults like the region (us-central1), and image subdomain (us.gcr.io). Feel free to adjust.\nIf successful, we will see something like this:\n\n\n\n\n\n\nConsole output during deployment\n\n\n\n\n\nDeploying container to Cloud Run service [example-app] in project [your-project-id] region [us-central1]\n✓ Deploying new service... Done.                                                 \n  ✓ Creating Revision...                                                         \n  ✓ Routing traffic...                                                           \n  ✓ Setting IAM Policy...                                                        \nDone.                                                                            \nService [example-app] revision [example-app-...] has been deployed and is serving 100 percent of traffic."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "href": "posts/20230802-cloudrun-githubaction/index.html#manage-secrets",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Manage secrets",
    "text": "Manage secrets\nIf the app needs to access secrets such as API keys and passwords, then it is a necessary to store and manage them securely.\nCreate a secret in GCP’s secret manager, and grant minimal necessary access.\nEach secret is versioned. For example, we may create a secret: MY_API_KEY:latest with latest being the version tag.\nWhen using gcloud run deploy to deploy the app, pass in additional arguments:\n--update-secrets=MY_API_KEY=MY_API_KEY:latest,OTHER_API_KEY=OTHER_API_KEY:latest\nIn the docker container, the secret value will be made available in the environment variable MY_API_KEY."
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "href": "posts/20230802-cloudrun-githubaction/index.html#set-up-a-secure-github-action-for-continuous-deployment",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "Set up a secure Github action for continuous deployment",
    "text": "Set up a secure Github action for continuous deployment\nWhile manually running the gcloud command is sufficient to deploy the app to Cloud Run, sometimes it can make sense to set up continuous deployment triggered by github push or release events.\n\nService account\nFirst, we need to follow these instructions to create a service account and grant some permissions:\nGo to IAM, click “grant access” and set: - principal: the new service account just created - role cloud run admin - role: roles/artifactregistry.createOnPushWriter - role: Secret manager secret accessor\nGrant the default compute-engine account access to Secret Manager Secret Accessor role. Go to IAM and set: - principal: the default compute-engine service account - role: Secret Manager Secret Accessor\nGo to IAM/service accounts, click into the default compute-engine service account, then allow the new service account to use this compute engine service account: - principal: the new service account just created - role: “Service account user”\n\n\n\n\n\n\nTip\n\n\n\nI spent hours debugging permission errors in the github actions and found the above steps helped resolving the errors. More info here and here. However, I suspect some of them are not necessary. Please let me know (zhangzhang.si AT gmail.com) if you have a different experience.\n\n\n\n\nDocker artifacts repository\nA docker artifacts repository must be created in the same project as the Cloud Run service (we assume the location is “us-central1”):\ngcloud artifacts repositories create slack-llm --location=us-central1 --repository-format=docker\nThis artifacts repository will hold the docker image of the app.\n\n\nWorkload identify federation and keyless authentication\nFor better cloud security, Google recommends setting up keyless authentication from github actions. To do that, we need to:\n\n\n\n\n\n\nCreate a Workload Identify Pool\n\n\n\n\n\ngcloud iam workload-identity-pools create \"my-pool\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --display-name=\"Demo pool\" \\\n  --description=\"My Identify Pool\"\n\n\n\n\n\n\n\n\n\nThen create a Workload Identify Provider:\n\n\n\n\n\ngcloud iam workload-identity-pools providers create-oidc \"my-provider\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --location=\"global\" \\\n  --workload-identity-pool=\"my-pool\" \\\n  --display-name=\"Demo provider\" \\\n  --attribute-mapping=\"google.subject=assertion.sub,attribute.actor=assertion.actor,attribute.aud=assertion.aud\" \\\n  --issuer-uri=\"https://token.actions.githubusercontent.com\"\n\n\n\n\n\n\n\n\n\nThen allow authentications from the Workload Identity Provider to impersonate the desired Service Account:\n\n\n\n\n\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/attribute.repository/my-org/my-repo\"\nAlternatively, if we do not want to restrict the binding to the specific github repo, then:\ngcloud iam service-accounts add-iam-policy-binding \"my-service-account@${PROJECT_ID}.iam.gserviceaccount.com\" \\\n  --project=\"${PROJECT_ID}\" \\\n  --role=\"roles/iam.workloadIdentityUser\" \\\n  --member=\"principalSet://iam.googleapis.com/projects/${PROJECT_NUMBER}/locations/global/workloadIdentityPools/my-pool/*\"\n\n\n\n\n\nGithub secrets\nAdd the following github secrets (see instructions on how to add secrets to a github repo):\nWIF_PROVIDER=projects/my-gcp-project-number/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n\nWIF_SERVICE_ACCOUNT=my-service-account@my-project.iam.gserviceaccount.com\n\n\nGithub action yaml file\nNow we should be ready to set up the actual github action. This is a redacted version of my working github action yaml file:\n\n\n\n\n\n\nYAML File\n\n\n\n\n\nYAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}\n\n\n\nPut this in .github/workflows/deploy.yml and the next time you push a change to main, it should automatically deploy to Cloud Run.\nEnjoy!"
  },
  {
    "objectID": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "href": "posts/20230802-cloudrun-githubaction/index.html#yaml-for-github-action",
    "title": "Deploying machine learning apps to Google Cloud Run with Github actions",
    "section": "YAML for Github Action",
    "text": "YAML for Github Action\nname: Build and Deploy to Cloud Run\n\non:\n  push:\n    branches:\n      - main\n\nenv:\n  PROJECT_ID: your-gcp-project-id\n  GAR_LOCATION: us-central1\n  REPOSITORY: your-artifacts-repo-name\n  SERVICE: your-app-name\n  REGION: us-central1\n\njobs:\n  deploy:\n    # Add 'id-token' with the intended permissions for workload identity federation\n    permissions:\n      contents: 'read'\n      id-token: 'write'\n\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n\n      - name: Google Auth\n        id: auth\n        uses: 'google-github-actions/auth@v1'\n        with:\n          token_format: 'access_token'\n          workload_identity_provider: '${{ secrets.WIF_PROVIDER }}' # e.g. - projects/123456789/locations/global/workloadIdentityPools/my-pool/providers/my-provider\n          service_account: '${{ secrets.WIF_SERVICE_ACCOUNT }}' # e.g. - my-service-account@my-project.iam.gserviceaccount.com\n\n      # BEGIN - Docker auth and build (NOTE: If you already have a container image, these Docker steps can be omitted)\n\n      # Authenticate Docker to Google Cloud Artifact Registry\n      - name: Docker Auth\n        id: docker-auth\n        uses: 'docker/login-action@v1'\n        with:\n          username: 'oauth2accesstoken'\n          password: '${{ steps.auth.outputs.access_token }}'\n          registry: '${{ env.GAR_LOCATION }}-docker.pkg.dev'\n\n      - name: Build and Push Container\n        run: |-\n          docker build -t \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\" ./\n          docker push \"${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\"\n\n      # END - Docker auth and build\n\n      - name: Deploy to Cloud Run\n        id: deploy\n        uses: google-github-actions/deploy-cloudrun@v1\n        with:\n          service: ${{ env.SERVICE }}\n          region: ${{ env.REGION }}\n          # NOTE: If using a pre-built image, update the image name here\n          image: ${{ env.GAR_LOCATION }}-docker.pkg.dev/${{ env.PROJECT_ID }}/${{ env.REPOSITORY }}/${{ env.SERVICE }}:${{ github.sha }}\n          # The secrets will be made available as environment variables.\n          secrets: |\n            API_KEY1=MY_API_KEY1:latest\n            PASSWORD2=MY_PASSWORD2:latest\n\n      # If required, use the Cloud Run url output in later steps\n      - name: Show Output\n        run: echo ${{ steps.deploy.outputs.url }}"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html",
    "href": "posts/20240915-soccer-tracking/index.html",
    "title": "Computer vision for soccer games",
    "section": "",
    "text": "I was intrigued to see this example where multiple (at least 5) computer vision techniques to create visual appealing analytics from soccer game footage. Soccer fans and coaches may enjoy this.\nVideo\nThis is an open source demo from Roboflow, and is easy to reproduce. Since it is a proof of concept, more work needs to be done to make it work for other real world videos, where there a large portion of the soccer field is not visible, or when the camera moved fast (which happens quite often). This is a common challenge for practical computer vision: it can be hard to make an impressive model work on your data.\nBelow I share a workflow to reproduce both success and limitations of this soccer tracking example, and some ideas to improve it to make it work on more challenging data. Similar techniques can be applied to other sports, like tennis, (American) football, basketball, pickle ball, etc."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "href": "posts/20240915-soccer-tracking/index.html#reproducing-the-birds-eye-view-creation",
    "title": "Computer vision for soccer games",
    "section": "Reproducing the birds-eye view creation",
    "text": "Reproducing the birds-eye view creation\n\n\n\n\n\n\nPre-requisites\n\n\n\n\n\nPre-requisites\n\nYou need a machine with GPU to run the code. The code is tested on a machine with a GeForce RTX 3090, and it uses about 3GB of GPU memory.\nYou need to have git, docker and python (3.6+) installed.\nNVidia container toolkit is required to use the GPU in the docker container.\n\n\n\n\n\n\n\n\n\n\nDownload\n\n\n\n\n\nStep 1: Download the code and data\ncvlization is an open source repo with many working examples of computer vision workflows. Clone the repo:\ngit clone https://github.com/kungfuai/cvlization.git\ncd cvlization\nIn examples/sports/soccer_game_visual_tracking, there is a README file that explains how to download the model weights and example video data (pip install gdown if you haven’t already).\ncd examples/sports/soccer_game_visual_tracking\nbash download_data.sh\n\n\n\n\n\n\n\n\n\nInstall\n\n\n\n\n\nStep 2: Install the dependencies by building a docker image\nChange directory back to the root of the cvlization repo, and run\nbash examples/sports/soccer_game_visual_tracking/build.sh\nThis will build a docker image with necessary dependencies. If you prefer to not use docker, you can install the dependencies manually by following the instructions in the Dockerfile in the same directory.\n\n\n\n\n\n\n\n\n\nRun the code\n\n\n\n\n\nStep 3: Run the code\nbash examples/sports/soccer_game_visual_tracking/predict.sh\nThis will use the docker image to run the code. If you prefer to run the code without docker, you can directly use the command in the predict.sh script in the same directory.\nIn this script, we are using a 30 second clip from a soccer game. The script will track the pitch and players, identify the team, goal keepers, referee, and ball, and generate a bird’s eye view video. Feel free to modify the script to use a different video or to change the tracking parameters.\nYou will find the output video in examples/sports/soccer_game_visual_tracking/0bfacc_0-radar.mp4. This is the video shown on the top of the page. On a machine with a GeForce RTX 3090, it takes about 20 minutes to run, with 3GB of GPU memory used."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "href": "posts/20240915-soccer-tracking/index.html#under-the-hood",
    "title": "Computer vision for soccer games",
    "section": "Under the hood",
    "text": "Under the hood\nThe computer vision models and algorithms under the hood include:\n\nA keypoint detection (pose estimation) model for 32 keypoints on the soccer pitch (Yolo-v8, 70M, training notebook, mAP=0.99, 1 hour on NVidia T4, trained on hundreds of images).\n\n\n\n\n\n\n\nAn object detection model for players, referrees and goal keepers (Yolo-v8, 68M, training notebook, mAP=0.79, 40min on NNivida L4).\n\n\n\n\n\n\n\nAnother object detection model for the ball (Yolo-v8, 68M, training notebook, mAP=0.93, 1.3 hours on NVidia A100). The ball is very small in the image, so it is hard to detect.\nA multi-object tracking model to track the players and the ball (Bytetrack, implementation and python API).\nA vision embedding model and clustering algorithm for team identification. SigLIP is used to extract embedding vectors from cropped players. UMAP is used for dimensionality reduction. K-means is used for clustering. Also Resolve the team IDs for detected goalkeepers based on the proximity to team centroids (based on player locations).\nAn image registration/stitching algorithm to create the bird’s eye view. Homography is estimated between the pitch keypoints and the reference coornidates of the pitch, using OpenCV’s findHomography. The pitch in the footage is then warped to a top-down view using perspectiveTransform.\nPlayer re-identification models (e.g. MOTIP). When the footage is cut or camera is changed to a different angle, the player IDs are lost. We need to re-identify the players in order to connect the player tracks across different clips. I did not find the implemetation in this POC."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "href": "posts/20240915-soccer-tracking/index.html#does-it-work-on-other-soccer-videos",
    "title": "Computer vision for soccer games",
    "section": "Does it work on other soccer videos?",
    "text": "Does it work on other soccer videos?\nI picked a random soccer game clip, and the result is not as good as the example video. The camera moved faster, zooming in to a partial view of the pitch near the goal post. This posed challenges to the keypoint detection model, and the player tracking model. Some players were not detected due to motion blur and occlusion. Key points of the pitch were not detected in some frames, and the algorithm was not able to create a bird’s eye view for those frames. The result is shown below:\nVideo\nRegardless, it is a great starting point to build a more reliable system for soccer game analytics. For fun, I also tried it on a very challenging video with a couple of professional players against 100 pupils. Interestingly, the algorithm was able to detect most the players, and create a bird’s eye view, as long as a large portion of the pitch is visible:"
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "href": "posts/20240915-soccer-tracking/index.html#makeing-it-better-more-accurate-player-detection-and-tracking",
    "title": "Computer vision for soccer games",
    "section": "Makeing it better: more accurate player detection and tracking",
    "text": "Makeing it better: more accurate player detection and tracking\n\nTransformers for object tracking\nAccurate tracking requires attending to relationships between detected players on different frames, their roles, jersey colors etc. Transformers architecture is well suited for this task.\n\nGlobal tracking transformer\nGlobal tracking transformers takes a video as input, and predict object tracks in an end-to-end fashion. It was trained on LVIS and COCO, capable of tracking 1000+ categories of objects. Below is the result for tracking persons and the ball. It also identified the billboards though they are not directly useful for our purpose here. This is the tracking result overlayed on the input video:\nVideo\nComparing YOLOv8 and Global Tracking Transformer, the latter seems more accurate.\n\n\n\n\n\n\n\n\n\nYOLOv8\n\n\n\n\n\n\n\nGlobal Tracking Transformer\n\n\n\n\n\n\n\n\nVision-language models, open vocabulary and zero-shot object detection\nWith recent advances in vision-language models, we can leverage the visual knowledge in pretrained large models. How well do they work in detecting players?\n\nGrounding DINO\nThis model has a DINO transformer backbone and produced by grounded pre-training. You can prompt the model with a sentence or a phrase, and it will highlight the corresponding region in the image. Below is the architecture of Grounding DINO:\n\n\n\nArchitecture of Grounding DINO\n\n\n\n\n\nWith one prompt, Grounding DINO was able to detect players but had a hard time distinguishing the goal keeper from other players.\n\n\n\n\nYOLO World\nThis model is an open-vocabulary object detection model. It can detect objects that are not in the training set, and can be used for zero-shot object detection. You can prompt it with a list of words, such as “player, ball, goal keeper”.\nCompared to Grounding DINO, YOLO World seems less accurate and misses some players when they overlap.\n\n\n\nYOLO-World-XL player detection result.\n\n\nThese are just two examples of recent models."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#datasets",
    "href": "posts/20240915-soccer-tracking/index.html#datasets",
    "title": "Computer vision for soccer games",
    "section": "Datasets",
    "text": "Datasets\nYou may need to fine tune the models on more soccer game videos with annotations. Here are some datasets that can be useful:\nSoccerNet is a large-scale dataset for soccer analysis. It contains 550 complete broadcast soccer games and 12 single camera games taken from the major European leagues. It supports various vision tasks such as action spotting, camera calibration, player re-identification and tracking.\nThis Kaggle dataset also contains soccer game videos from Premier League showdowns to FIFA World Cup classics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "href": "posts/20240915-soccer-tracking/index.html#business-use-cases",
    "title": "Computer vision for soccer games",
    "section": "Business use cases",
    "text": "Business use cases\nBoardly, here are some areas where computer vision can be used in soccer analytics:\n\nPerformance Analysis: By tracking player movement, positioning, and interactions, teams can better understand individual and team performance, making it easier to identify strengths and areas for improvement.\nTactical Insights: Coaches can analyze formations, pressing patterns, and set-pieces to gain a competitive edge, adjusting their game plans based on data.\n** Player Development**: Young athletes can leverage computer vision technology to receive feedback on their performance and improve their skills over time.\nFan Engagement: Computer vision can create engaging, immersive content for fans, such as 3D replays or interactive match highlights, bringing them closer to the action.\n\nHere is a very incomplete list of companies and use cases:\n\nVeo: AI-powered cameras for automatic sports recording, tracking game action, and AI-tagged highlights for analysis.\nTraceup: Video captures that allow tracking players individually, creating personalized highlight reels that parents, players, and coaches can view from various angles.\nTrack160: Skeleton tracking, identifying and monitoring the movement of players and the ball, tagging and analyzing events in a match, physical and tactical breakdowns of player performances.\nNY Times created 3D stories that allow fans to experience game-defining moments from multiple angles and gain deeper insights into player positioning, ball movement, and tactics."
  },
  {
    "objectID": "posts/20240915-soccer-tracking/index.html#conclusion",
    "href": "posts/20240915-soccer-tracking/index.html#conclusion",
    "title": "Computer vision for soccer games",
    "section": "Conclusion",
    "text": "Conclusion\nThis is just a start. I am glad to see computer vision applied to everyday life, and hope this post spark some ideas."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html",
    "href": "posts/20250712-agentic-causal/index.html",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of sciences. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why.\nThe plural on sciences is intentional. I want to emphasize the range of disciplines: physics, chemistry, biology, economics, sociology, computer science, data science, you name it. But in this post I would like to dedicate my attention to causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-sciences",
    "href": "posts/20250712-agentic-causal/index.html#backdrop-machine-age-of-sciences",
    "title": "Agentic Causal Inference",
    "section": "",
    "text": "Historians may one day mark the 2020s as the dawn of the machine age of sciences. Language models now draft proofs and experimental protocols; diffusion models fold proteins and sketch molecules before a chemist even picks up a pipette. Yet prediction is only half the story; scientists and businesses still need to answer the deeper question: why.\nThe plural on sciences is intentional. I want to emphasize the range of disciplines: physics, chemistry, biology, economics, sociology, computer science, data science, you name it. But in this post I would like to dedicate my attention to causal inference."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "href": "posts/20250712-agentic-causal/index.html#causal-inference-and-llms",
    "title": "Agentic Causal Inference",
    "section": "Causal Inference and LLMs",
    "text": "Causal Inference and LLMs\n\n\n\n\n\nCausal inference is such an important decision making tool in life and in business. However, to be an expert in this field takes years of mathematical and statistical training. LLMs on the other hand are easy to use, but they lack rigor when reasoning about causality.\nIntegrating causality into LLM agents addresses limitations on both sides:\n\nPure causal methods demand strict assumptions and expert guidance.\nLLMs overflow with knowledge yet often mistake correlation for causation.\n\nBy wiring LLM‑based agents to specialized causal inference libraries, we can automate the causal workflow: discovery -&gt; identification -&gt; estimation -&gt; refutation. The result is a new class of general‑purpose causal AI systems that parse tabular, time-series, and even multimodal data with human-like intuition and mathematical rigor.\nPractically, that means the agent:\n\nThinks (via an LLM) about what causal graph should link your variables\nActs by writing Python: drawing DAGs, running ID algorithms, calling estimators, using libraries like dowhy, econml, causaltune, etc.\nReflects on the results, prompting itself with “Do my assumptions still hold?”\nIterates until a relevant answer surfaces, or it asks you for help.\n\nIf that sounds suspiciously like a data scientist teamate with infinite patience, you’ve got the gist.\n\n10 lines of code for your first causal inference agent\nTo illustrate the idea, here is a minimal snippet to estimate the effect of a new coupon on revenue. This may actually be sufficient to get you a quick answer, if (a big if) the data is ready.\nfrom langchain.agents import initialize_agent, Tool\nfrom langchain.llms import OpenAI\nfrom dowhy import CausalModel\nfrom causaltune import AutoTune\n\ndef estimate_ate(df, treatment, outcome):\n    model = CausalModel(data=df, treatment=treatment, outcome=outcome)\n    ided = model.identify_effect()\n    best = AutoTune(model, df).best_estimator_\n    return model.estimate_effect(ided, method_name=best)\n\nagent = initialize_agent(\n    llm=OpenAI(model_name=\"gpt-4o-mini\"),\n    tools=[Tool.from_function(estimate_ate)],\n    agent_type=\"openai-tools\"\n)\n\nagent.run(\"Estimate the uplift of coupon_v2 on weekly revenue\")\nTime to look at some interesting papers and open source projects:\n\n\n\nSingle‑Agent Autonomous Pipelines\nCausal Agent framework (2024):\n\nAn LLM operates in a ReAct‑style loop with a suite of causal tools—e.g. CausalLearn for graph discovery and EconML for effect estimation.\nGiven a dataset and a query (e.g. “Effect of X on Y?”) the agent automatically:\n\nexplores variable correlations,\nhypothesizes causal links,\nproposes a causal graph,\ncomputes the quantitative effect of X on Y.\n\nEach step is backed by library outputs that the LLM interprets before deciding its next move.\n\nThe framework’s hierarchical breakdown (variable‑level, edge‑level, graph‑level, effect‑level) has produced expert‑level accuracy on a testing dataset with 1.3k questions, all while providing interpretable explanations.\n\nCausal-Copilot (2025)\n\nThe agent chains 20 + causal tools, from discovery to hyper-parameter tuning, inside a single LLM loop.\n\nWorks on both tabular and time-series data: prompts the user for a question, auto-selects the right discovery algorithm (e.g., NOTEARS, PC), tunes an estimator (DoubleML, CausalForest, IV), runs refuters, and returns an English report with effect size + CI.\nAchieves state-of-the-art graph accuracy and effect-estimation error across five public benchmarks, edging out both classic SCD baselines and earlier LLM agents.\n\n\n\n\nDebating Multi‑Agent Systems for Causal Discovery\nSingle agents sometimes hallucinate; multi‑agent approaches aim to reduce errors through debate and consensus.\n\nMulti‑Agent Causal Discovery Using LLMs (2024) assigns dedicated LLM roles:\n\nAffirmative Debaters proposes a DAG using temporal cues and domain priors.\nNegative Debaters attacks the proposal by surfacing hidden confounders, incorrect temporal orderings, or omitted variables.\nJudges evaluate arguments and pick the most plausible structure.\nCoders materializes the agreed-upon algorithm, reruns it on the entire dataset, and emits the refined graph.\n\n\nExperiments show these debating agents outperform both classical algorithms and single‑LLM prompts on datasets like Auto MPG, demonstrating that multiple specialized minds can yield more reliable causal graphs.\n\nChain-of-Collaboration Prompting (2025) shows that giving sub-agents explicit roles (planner, verifier, critic) and letting them share scratch pads improves causal reasoning accuracy on CLADDER and Causal-Copilot QA tasks, cutting hallucinated edges by 35 % vs. single-prompt ReAct.\n\n\n\nToolbox Layer (AutoML & No‑Code Platforms)\nParallel to LLM research, we also see AutoML‑style causal platforms that automate model selection, tuning, and robustness checks.\n\nAutoCausality: part of the PyWhy ecosystem, using hyper‑parameter search and ensembling to choose the best estimator for a dataset.\nOpportunityFinder (Amazon 2023) offers code‑less causal studies for panel data cleaning, cohorting, and computing effects (plus sensitivity) end‑to‑end.\nSalesforce CausalAI Library consolidates discovery and inference methods, synthetic data generators, and a no‑code GUI, scaling to larger problems via optimized multiprocessing.\n\nThese toolkits enrich agentic workflows: an LLM planner can mix‑and‑match discovery, estimation, and AutoML selection modules without human intervention."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "href": "posts/20250712-agentic-causal/index.html#evaluating-causal-inference-agents",
    "title": "Agentic Causal Inference",
    "section": "Evaluating Causal Inference Agents",
    "text": "Evaluating Causal Inference Agents\nHow well do these causal inference agents perform? Here are some real or synthetic datasets and benchmarks.\nFor treatment‑effect estimation, the Lalonde job‑training study is a good place to start. It has real observational covariates paired with true RCT outcomes—to sanity‑check bias reduction. When larger, controlled replications are needed, you can use semi‑synthetic generators such as IHDP and the Twins dataset, whose perfect counterfactual comes from each twin’s paired outcome. The annual ACIC challenges extend this idea with dozens of high‑dimensional scenarios, while the 2025 RealCause generator allows people to create realistic Lalonde‑style benchmarks.\nFor longitudinal uplift studies, Amazon’s no‑code OpportunityFinder panels ship sample retail datasets ready for difference‑in‑differences.\nWhen it comes to graph discovery methods, people tend to use classic datasets such as the 11‑node Sachs protein‑signaling map, a real wet‑lab interventions dataset. Bayesian‑network classics like Asia and ALARM remain quick smoke tests. Pairwise direction algorithms rely on the Tübingen cause–effect pairs, and larger time‑series graphs come from gene‑regulation contests such as DREAM4.\nMore recently we see language‑centric causal benchmarks. CLADDER has 10k natural language questions across Pearl’s ladder, while ACCESS asks agents to build abstract causal graphs over multimodal vignettes before answering why queries.\nAs to multimodal causal inference, CausalVQA is a benchmark for video question answering (VQA) that test models’ understanding of causality in the physical world."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "href": "posts/20250712-agentic-causal/index.html#challenges-and-mitigations",
    "title": "Agentic Causal Inference",
    "section": "Challenges and Mitigations",
    "text": "Challenges and Mitigations\nGoing beyond the happy path to production is often not a smooth ride. Here are some of common challenges in my experience:\n\nData Quality and the Missing Confounders\nObservational datasets rarely contain every variable that shapes a treatment–outcome relationship, so even a state‑of‑the‑art estimator can inherit hidden bias.\nTo mitigate, insert a human‑review checkpoint right after the agent proposes its first causal graph: domain experts eyeball edges and nominate missing covariates. The software then launches automatic robustness probes such as placebo tests, synthetic confounder injections, and other refutation modules shipped with DoWhy, to quantify how fragile the estimate is. Crucially, if any refutation fails, the planner LLM must stop, annotate the failure, and either revise the graph or escalate to a human reviewer; surfacing a shaky result as “tentative” is better than silently proceeding. Some teams also run a “data‑profiling agent” that scans fresh tables for covariate drift or sparsity and warns the planner before analysis starts.\n\n\nHallucinations and Over‑Confidence in Planner LLMs\nLLM planners are persuasive storytellers; a well phrased chain‑of‑thought can make a shaky causal graph feel ironclad.\nMulti‑agent debate is a good recipe to reduce hallucination: a second LLM plays devil’s advocate, and challenges the assumptions that make an estimate causal:\n\nPlacebo‑treatment test: replace the real treatment with a fake; any non‑zero effect flags hidden bias.\nSynthetic‑confounder injection: add a random common cause and observe the ATE shift; big swings imply unmeasured confounding.\nOverlap / positivity audit: verify that propensity scores span both arms; poor overlap triggers trimming or doubly robust methods.\nCross‑estimator consensus: pit a back‑door learner against an IV or front‑door estimator; disagreement above a threshold routes to human review.\nMulti‑agent debate: affirmative and negative debaters contest every edge, a judge scores coherence.\n\nIf any probe fails, the planner either tightens assumptions and reruns discovery or clearly labels the conclusion “inconclusive, additional data needed.” Final reports must surface the point estimate plus confidence intervals, sensitivity ranges, and a pass/fail tally for each refuter, so stakeholders see magnitude and robustness.\n\n\nModel‑Selection Over‑Fit and Cross‑Estimator Disagreement\nAuto‑tuning libraries can explore dozens of learners and hyper‑parameters, sometimes over‑fitting small causal datasets, especially with flexible models like causal forests. In this case, AutoML learns noise instead of real signal.\nMitigations include nested cross‑validation inside AutoCausality or causaltune, and parsimony priors that penalize needless complexity. If resource allows, the agent should run at least two conceptually different estimators, e.g., a back‑door regression and an instrumental‑variable model, and flag any large divergence in effect size as a red‑flag for human review.\n\n\nComputation Cost vs. Real‑Time Ambitions\nA planner–solver split can still burn thousands of tokens and heavy compute if the planner explores many what‑if branches.\nProduction dashboards cache discovery and refutation outputs keyed by a DAG hash; if the graph hasn’t changed, the agent re‑uses prior results. Another recipe is distilling a heavy LLM planner into a small fine‑tuned local model covers day‑to‑day traffic, while the costly cloud model handles weekly deep dives.\n\n\nPrivacy and Governance\nSensitive data such as medical records, customer logs usually cannot leave a private cluster.\nHybrid deployments solve this: an on‑prem LLM handles data‑aware steps, while a redacted summary (no PII) is sent to a cloud model for high‑level planning. All explanations pass through a redaction layer before logging, and every causal report carries an audit trail plus role‑based access controls."
  },
  {
    "objectID": "posts/20250712-agentic-causal/index.html#conclusion",
    "href": "posts/20250712-agentic-causal/index.html#conclusion",
    "title": "Agentic Causal Inference",
    "section": "Conclusion",
    "text": "Conclusion\nCausal inference is transitioning from a highly specialized skill to a widely accessible capability. That’s not putting anyone out of a job. It is freeing us to ask better questions. A couple of years ago, answering “what actually drives our north star metric?” meant a quarter-long project. Today, it may be weeks or even days. That’s not just a productivity gain. It is a fundamental change in how we can think about our businesses."
  }
]
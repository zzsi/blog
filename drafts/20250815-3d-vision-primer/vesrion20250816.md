Below is a **canvas‑ready primer** you can paste into a whiteboard/notes tool. It’s organized as compact “cards” you can drop into columns (Foundations → Classical → Neural → What to use → Pitfalls). The audience is technical ML engineers who are new to 3D.

---

# 3D Vision Primer & Recent Advances (Canvas Edition)

> **How to use**: Treat each `### Card` as a sticky note. Arrange into columns like: **Foundations**, **Classical**, **Neural Rendering**, **Modern Feed‑Forward**, **Practical**.

---

## Foundations

### Card — Camera model (pinhole)

* **Projection**: $\tilde{x} \propto K [R\mid t] \tilde{X}$.

  * $K$: intrinsics $(f_x,f_y,c_x,c_y)$
  * $R,t$: world→camera pose
* **Back‑project** pixel $(u,v)$ with depth $z$:
  $X_{\text{cam}} = z\,K^{-1}[u,v,1]^\top,\ \ X_{\text{world}}=R^\top(X_{\text{cam}}-t)$

### Card — Epipolar geometry (F/E)

* **Fundamental matrix** $F$ (uncalibrated): $x'^{\top} F x = 0$
* **Essential matrix** $E$ (calibrated): $\hat{x}'^{\top} E \hat{x}=0,\ E=K'^{\top} F K,\ E=[t]_\times R$
* **Pose from $E$**: SVD → choose (R,t) with positive depths (cheirality).

### Card — Core 3D representations

* **Point clouds** (fast, no topology)
* **Meshes** (triangles; engine‑ready; need watertightness for sim)
* **Voxels / TSDF** (fusion‑friendly; memory heavy)
* **SDF/implicit** (continuous surfaces; clean normals)
* **Radiance fields** (color+density for any point & view)
* **3D Gaussians** (oriented ellipsoids; very fast rasterization)

---

## Classical (pre‑deep, still essential)

### Card — SfM pipeline (static photos → sparse 3D)

1. Detect/describe (SIFT/ORB) → 2) Match + **RANSAC** → 3) Relative pose
2. **Triangulate** → 5) **Bundle Adjustment** (min reprojection error)
3. Grow map with **PnP** for new images → BA again.

### Card — MVS & meshing

* With poses known: plane‑sweep/cost volumes → dense depth maps → fuse → Poisson/screened Poisson → textured mesh.

### Card — SLAM / VIO (real time)

* **Feature‑based** (PTAM/ORB‑SLAM) vs **direct** (DSO/LSD‑SLAM)
* Fuse **IMU** for scale & robustness; sliding‑window BA + loop closure.

---

## Neural Rendering (2020 →)

### Card — NeRF (differentiable volumetric rendering)

* Learn $F_\theta(X,\mathbf{d})\to(\sigma,\mathbf{c})$; render by ray integration; train to match photos.
* **Why it mattered**: photoreal novel views from sparse posed images.
* **Speedups**: Instant‑NGP (hash grids), PlenOctrees/Plenoxels, TensoRF → minutes.
* **Beyond basics**: mip‑NeRF (anti‑aliasing), NeRF‑W (in‑the‑wild), dynamic/4D variants.

### Card — 3D Gaussian Splatting (3DGS)

* **Inputs**: posed images (K,R,t); optional masks/initial points.
* **Scene** = many Gaussians $(\mu,\Sigma,\alpha,\text{SH color})$.
* **Render**: project to ellipses → per‑pixel weights → depth‑sorted alpha compositing (graphics “over”).
* **Train**: photometric + SSIM; **densify/prune** Gaussians iteratively.
* **Why**: NeRF‑like quality with **real‑time** rendering; friendly to editing & dynamics.

---

## Pose‑Free Geometry & Point Maps

### Card — CroCo → DUSt3R family (intuition)

* **Goal**: from **unposed** images, recover geometry/pose without full SfM.
* **Point map**: for each pixel, predict its **3D coordinate in a shared frame**.
* **Unlocks**:

  * **Pose** via rigid alignment of two point maps (Procrustes/Umeyama)
  * **Depth** (distance to camera)
  * **Dense correspondences** (nearest neighbors in 3D)
  * **Multi‑view fusion** into a dense point cloud

### Card — Why point maps matter

* Unifies matching + depth + pose in one prediction.
* Great initializer for NeRF/3DGS from casual, unordered photos.

---

## Large Reconstruction Models & Single‑Image 3D

### Card — LRM (Large Reconstruction Models)

* **Idea**: a generalist net maps 1–N images → 3D (point cloud / triplanes / Gaussians) **feed‑forward**.
* **Supervision**: distilled from SfM/MVS/NeRF/3DGS teachers + synthetic corpora.
* **Usage**: instant 3D → optional NeRF/3DGS refinement for max quality.

### Card — Single‑image → 3D (two recipes)

* **(A) Diffusion → multi‑view → lift**: synthesize new views (Zero123/One‑2‑3‑45 style), then reconstruct (NeRF/3DGS/mesh).
* **(B) Direct regression**: predict mesh/SDF/Gaussians in one pass (e.g., TripoSR/InstantMesh, “Trellis”‑style).
* **Trade‑offs**: (A) better priors but multi‑stage; (B) fast/simple but hallucinates hidden parts.

---

## Dynamics, Semantics, & “World Models”

### Card — Dynamic/4D scenes

* Deformable Gaussians or dynamic NeRFs model time; often need regularizers for motion smoothness and exposure/lighting handling.

### Card — Open‑vocabulary 3D understanding

* Combine 2D foundation models with multi‑view fusion for **zero‑shot 3D segmentation**; enables text‑guided editing and analytics.

### Card — Occupancy & forecasting

* For robotics/AV: **3D (or 4D) occupancy** streams + instance flow to predict the near future of the scene.

---

## Choose‑the‑Tool (cheat sheet)

| You have                | Goal                      | Start with                                        | Then, if needed                                                 |
| ----------------------- | ------------------------- | ------------------------------------------------- | --------------------------------------------------------------- |
| 20–100 **posed** photos | Best novel‑view quality   | NeRF or **3DGS**                                  | Short extra training; extract mesh if required                  |
| **Unposed** photos      | Reliable geometry quickly | **DUSt3R**‑style point maps → poses               | Train 3DGS/NeRF on recovered poses                              |
| 1–8 photos              | Usable 3D **now**         | **LRM** (feed‑forward)                            | Brief NeRF/3DGS refinement                                      |
| **Single image** object | AR‑ready asset            | Direct regression (TripoSR/InstantMesh/“Trellis”) | Small render‑and‑refine loop                                    |
| Real‑time robot         | Tracking + mapping        | VIO/SLAM (+ learned depth/features)               | Gaussians for high‑fidelity maps; point maps for relocalization |

---

## Minimal math & pseudo‑loops (copy‑friendly)

### Card — Pose from $E$ (outline)

1. Normalize: $\hat{x}=K^{-1}x,\ \hat{x}'=K'^{-1}x'$
2. Estimate $E$ in RANSAC; SVD $E=U\operatorname{diag}(1,1,0)V^\top$
3. $R\in\{UWV^\top,\ UW^\top V^\top\},\ t\sim U_{:,3}$, $W=\begin{bmatrix}0&-1&0\\1&0&0\\0&0&1\end{bmatrix}$
4. Pick solution with **positive depths** after triangulation.

### Card — 3DGS training loop (per scene)

```
G = seed_from_SfM(points)  # means, covs, α, SH colors
for step in 1..T:
    views = sample_views()
    C_hat = rasterize_gaussians(G, views, K,R,t)  # differentiable splatting
    loss = photometric(C_hat, I_gt) + SSIM + regs
    G = optimizer.update(G, ∇loss)
    if step % k == 0:
        G = split_high_error(G); G = prune_low_contrib(G)
```

### Card — Point map alignment

* Given two point maps $\{P_i\}, \{Q_i\}$ (corresponding pixels):
  Find $R,t$ minimizing $\sum_i \|Q_i - (RP_i + t)\|^2$.
  Closed‑form via SVD (Umeyama). Depth per pixel is $\|RP_i + t - C\|$ (distance to camera center $C$).

---

## Practical pipelines (end‑to‑end)

### Card — Casual photo capture → photoreal scene

1. Point maps (DUSt3R) → initial poses
2. Quick preview with **LRM** or small **3DGS**
3. Refine with 3DGS/NeRF for best quality
4. (Opt.) extract mesh; bake textures/SH

### Card — Single product shot → AR mesh

1. Direct regression (single‑image 3D)
2. Fast render‑and‑refine (few minutes)
3. Ensure watertightness/UVs; decimate for runtime

---

## Evaluation & Debugging

### Card — Metrics

* **Novel view**: PSNR / SSIM / LPIPS
* **Geometry**: Chamfer‑L1/2; F‑score@τ; Normal consistency
* **Tracking**: ATE / RPE (SLAM)
* **Semantics**: mIoU / PQ

### Card — Common pitfalls

* **Scale ambiguity** (monocular): add metric cue (stereo/IMU/known size).
* **Specular/transparent**: photometric loss struggles; add exposure/BRDF handling and masks.
* **Drift** (unposed sets): robust matching, point‑map alignment, loop closures.
* **Mesh extraction from radiance/Gaussians**: enforce SDF regularization or use mesh‑oriented post‑processing.

---

## Quick glossary

**BA**: joint optimization of poses+points to minimize reprojection error.
**PnP**: pose from 2D–3D correspondences.
**Cost volume**: per‑pixel scores over discrete depths; used in stereo/MVS nets.
**Triplanes**: three orthogonal feature grids a tiny renderer samples from.
**Point map**: dense per‑pixel 3D in a shared frame (unifies depth/pose/matching).

---

### Notes on “recent” work

This canvas focuses on durable ideas (NeRF, 3DGS, DUSt3R/point maps, LRM, single‑image 3D). If you want a **hyper‑current** add‑on sheet with specific 2024–2025 papers, say your focus (e.g., dynamic humans, AV mapping, fast multi‑view feed‑forward), and I’ll tailor a compact “what to read/run” list.


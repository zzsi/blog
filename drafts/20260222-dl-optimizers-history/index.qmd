---
title: "A Tour of Deep Learning Optimizers"
date: 2026-02-22
categories: [ai, deep learning, optimization, llm]
format:
  html:
    toc: true
    toc-depth: 2
---

Optimizers are the control systems of deep learning. Architecture and data define what a model can represent, but the optimizer often determines whether training is stable, efficient, and reproducible.

Looking back through early 2026, optimizer progress came in waves: acceleration, adaptivity, regularization fixes, large-scale systems pressure, and now conditioning-heavy geometric methods.

This post is a practical historical tour: what changed, why it changed, and what still works as a default.

## A practical mental model

At step `t`, we convert stochastic gradient `g_t` into update `delta_theta_t`.

Every optimizer is trying to balance four controls:

- Direction control: where to move.
- Step-size control: how far to move.
- Stability control: how to survive noise and curvature.
- Resource control: memory, compute, and communication cost.

Most innovations can be grouped as:

1. Acceleration (`momentum`, `Nesterov`).
2. Preconditioning (`AdaGrad`, `RMSProp`, `Adam`).
3. Regularization-correct updates (`AdamW`).
4. Large-scale stabilization (`LARS`, `LAMB`).
5. Generalization-aware updates (`SAM`).
6. Systems-efficient updates (`Adafactor`, low-precision states).
7. Geometry-aware updates (natural-gradient lineage, Muon-style orthogonalization).

## 1960s to 2000s: foundations that never went away

The key ideas predate modern deep learning:

- Polyak momentum (heavy ball) reduced zig-zag behavior in narrow valleys ([Polyak, 1964](https://doi.org/10.1016/0041-5553(64)90137-5)).
- Nesterov acceleration added look-ahead correction ([Nesterov, 1983](https://www.mathnet.ru/eng/dan/v269/i3/p543)).
- Natural gradient reframed descent in information geometry ([Amari, 1998](https://doi.org/10.1162/089976698300017746)).

These ideas established the long-term pattern: smooth noisy gradients, respect curvature, and seek invariance.

## 2010 to 2014: getting deep nets to train at all

Early deep learning leaned on SGD + momentum because it was cheap and scalable, but tuning was fragile ([Sutskever et al., 2013](https://proceedings.mlr.press/v28/sutskever13.html)).

Adaptive methods arrived quickly:

- AdaGrad (2011): per-coordinate scaling, strong for sparse settings ([Duchi et al., 2011](https://jmlr.org/papers/v12/duchi11a.html)).
- RMSProp (2012): moving second-moment estimate to avoid AdaGrad's monotonic decay ([Hinton lecture notes, 2012](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)).
- AdaDelta (2012): reduced global LR sensitivity ([Zeiler, 2012](https://arxiv.org/abs/1212.5701)).

By the end of this period, the design direction was clear: momentum-like smoothing plus adaptive scaling.

## 2014 to 2019: Adam wins, AdamW corrects

Adam became the default because it reduced tuning friction across workloads ([Kingma and Ba, 2014](https://arxiv.org/abs/1412.6980)).

Two important caveats followed:

- Convergence pathologies in some regimes (e.g., AMSGrad lineage) ([Reddi et al., 2018](https://openreview.net/forum?id=ryQu7f-RZ)).
- L2-in-gradient is not true weight decay under adaptive preconditioning.

AdamW fixed the second issue by decoupling weight decay ([Loshchilov and Hutter, 2017](https://arxiv.org/abs/1711.05101)). This was a small implementation change with outsized practical impact.

## 2017 to 2020: large-batch pressure

As batch sizes grew, optimization dynamics changed:

- LARS stabilized layer-wise relative updates for huge-batch CNNs ([You et al., 2017](https://arxiv.org/abs/1708.03888)).
- LAMB brought trust-ratio ideas to Adam moments for large-batch language pretraining ([You et al., 2019](https://arxiv.org/abs/1904.00962)).

These methods solved throughput bottlenecks, even though AdamW remained the broad default.

## 2018 to 2023: memory and systems become first-class

At Transformer scale, optimizer state is expensive:

- Adafactor reduced second-moment memory with factorization ([Shazeer and Stern, 2018](https://arxiv.org/abs/1804.04235)).
- 8-bit optimizer states reduced memory pressure in practice ([Dettmers et al., 2021](https://arxiv.org/abs/2110.02861)).
- Communication-aware variants targeted distributed bandwidth ([Tang et al., 2021](https://arxiv.org/abs/2102.02888)).

The best optimizer is not only mathematically elegant; it must fit systems constraints.

## 2020 to 2023: generalization-aware and tweak-heavy era

SAM made flatness bias explicit via a local worst-case objective ([Foret et al., 2020](https://arxiv.org/abs/2010.01412)).

Many variants (Lookahead, RAdam, AdaBelief, AdaBound) tuned warmup and update coupling ([Lookahead](https://arxiv.org/abs/1907.08610), [RAdam](https://arxiv.org/abs/1908.03265), [AdaBelief](https://arxiv.org/abs/2010.07468), [AdaBound](https://arxiv.org/abs/1902.09843)). Some helped in niches, but few replaced AdamW/SGD defaults broadly.

Lion added a search-discovered optimizer angle ([Chen et al., 2023](https://arxiv.org/abs/2302.06675)).

## 2024 to 2025: geometry returns

Muon-style methods reframed the problem around directional interference and orthogonalization ([Muon implementation](https://github.com/KellerJordan/Muon), [modular-duality framing](https://arxiv.org/abs/2410.21265)).

By end-2025, this looked promising but not universal. AdamW still dominated documented frontier recipes.

## Late-2025 to early-2026: conditioning wave

A broader conditioning-focused wave followed:

- NorMuon ([2025](https://arxiv.org/abs/2510.05491))
- MARS-M ([2025](https://arxiv.org/abs/2510.21800))
- Matrix-preconditioner hyperparameter transfer across scales ([2025](https://arxiv.org/abs/2512.05620))
- TEON tensorized orthonormalization ([2026](https://arxiv.org/abs/2601.23261))
- ARO rotated-coordinate optimization ([2026](https://arxiv.org/abs/2602.09006))

The shift: conditioning is becoming a primary design axis, not a side detail.

Community reports are useful early signals but should stay secondary to controlled evaluations.

## What won in practice by early 2026

Defaults remain fairly stable:

- Frontier LLMs/VLMs: AdamW + warmup + decay + clipping + selective decay exclusions.
- ViTs: AdamW.
- CNNs: SGD + momentum remains strong.
- Diffusion/flow matching: Adam/AdamW, often with EMA.
- LARS/LAMB: useful in specific extreme-batch throughput regimes.

## Why optimizer innovation keeps happening

Three forces interact repeatedly:

- Theory pressure: invariance, stability, objective reformulation.
- Empirical pressure: fewer knobs, faster loss reduction on real workloads.
- Systems pressure: memory, interconnect, and runtime constraints.

Methods that survive usually satisfy all three.

## Practical recipe chooser (2025 to early-2026)

| Setting | First choice | When to deviate |
| --- | --- | --- |
| LLM/VLM pretraining | AdamW + warmup/decay + clipping | Try Muon/conditioning if stability or scaling efficiency is bottleneck |
| Vision CNN | SGD + momentum + strong LR schedule | Use AdamW for transformer-heavy stacks or faster early convergence |
| ViT training | AdamW | Trial SAM or conditioning methods when plateaus appear |
| Diffusion/flow matching | AdamW (+ EMA) | Try Adafactor/low-precision states when memory dominates |
| Extreme large-batch throughput | LARS/LAMB | Stay with AdamW if batch size is moderate and tuning budget is limited |

Fair comparison protocol:

1. Same model and tokenizer.
2. Same token/image budget and data order.
3. Matched tuning budget across optimizers.
4. Report time-to-target, compute-to-target, and seed stability.

## Toy visual check (didactic)

A toy trajectory view is useful for intuition before heavy benchmarks.

Adam trajectory:

![](../../examples/optimizer-bench/toy/outputs/spirals_adam.gif){fig-align="center" width="560px"}

SGD trajectory:

![](../../examples/optimizer-bench/toy/outputs/spirals_sgd.gif){fig-align="center" width="560px"}

In this toy run, Adam reaches a lower-loss region faster than SGD under the same short budget. Treat this as intuition only. Final decisions should come from controlled CIFAR-10 and NanoGPT-bin benchmarks. Repro steps: `../../examples/optimizer-bench/toy/README.md`.

## Closing

From heavy-ball momentum to conditioning-heavy methods, optimizer history is mostly a story of recurring constraints in new forms: curvature, noise, scale, and hardware budgets.

By early 2026, AdamW is still the center of gravity. The next durable wave is likely to come from better directional control and conditioning, not only better scalar learning-rate heuristics.

# From Heavy Ball to Muon: a scholar’s tour of deep-learning optimizers (up to 2025)

Optimizers are the “control systems” of deep learning: they translate stochastic gradients into parameter updates that actually make models learn. Their evolution has been driven by two strong (and sometimes competing) forces:

* **First-principles theory** (convex optimization, geometry of parameter space, curvature approximations).
* **Practice-led empiricism** (what runs fastest, scales widest, or generalizes best on today’s architectures and hardware).

Below is a compact history and an analysis of what has propelled innovation—ending with where new ideas may come from next.

---

## 1) Before the deep-learning wave: momentum, acceleration, and geometry

Early milestones established ideas we still use:

* **Heavy-ball momentum**: Polyak’s idea of adding a velocity term accelerates gradient descent in ill-conditioned landscapes—a precursor to almost every modern optimizer. ([arXiv][1])
* **Nesterov’s accelerated gradient (NAG)**: theoretically optimal rates in smooth convex problems and the “look-ahead” gradient evaluation that later influenced practical Nesterov momentum. ([arXiv][2])
* **Natural gradient** (Amari): follow the Fisher-Rao geometry instead of Euclidean, inspiring scalable approximations years later (K-FAC, Shampoo). ([arXiv][3])
* **Sutskever et al. (2013)** showed how much *momentum* and *initialization* matter in deep nets, offering a practical recipe that revived first-order training of deep and recurrent models. ([Proceedings of Machine Learning Research][4], [U of T Computer Science][5])

**Takeaway:** this era is theory-heavy; practical deep nets were still small enough that clean mathematical ideas could guide practice.

---

## 2) 2011–2016: the adaptive family arrives

As datasets exploded and gradients became noisy and sparse, *per-parameter* stepsizes proved powerful:

* **AdaGrad** (2011) adapted steps per coordinate via accumulated squared gradients—great for sparse features. ([Journal of Machine Learning Research][6])
* **AdaDelta** (2012) fixed AdaGrad’s ever-shrinking learning rate with a running window of gradient energy. ([arXiv][7])
* **RMSProp** (2012), introduced in Hinton’s Coursera lectures, normalized by a moving RMS of gradients and became a workhorse. ([YouTube][8], [Cornell Optimization Wiki][9])
* **Adam** (2014/ICLR 2015) combined RMSProp-style variance adaptation (v̂) with momentum (m̂), and became the default for many models. ([arXiv][10])

Caveats emerged: Adam’s original convergence guarantees had gaps (spurring **AMSGrad**), and its generalization sometimes lagged SGD with momentum on vision tasks. A widely cited study argued adaptive methods often find different—and worse-generalizing—solutions than SGD in over-parameterized regimes. ([arXiv][11])

**AdamW** (2017/2019) decoupled weight decay from the gradient step—small change, big effect—fixing a long-standing regularization mis-specification and materially improving generalization. ([arXiv][12], [OpenReview][13])

**Schedules** also mattered: cosine annealing with warm restarts (SGDR) and cyclical schedules offered robust, low-tuning learning-rate control that paired well with SGD/AdamW. ([arXiv][14])

**Takeaway:** this phase was **empirical** at heart (RMSProp arose in a lecture; Adam was loved for “just working”), with theory catching up to explain fixes like AdamW.

---

## 3) 2017–2020: scale changes everything

As training shifted to multi-accelerator pods and foundation models, optimizers adapted to **huge batches** and **memory limits**:

* **LARS** (2017) stabilized very large-batch ImageNet training with layer-wise normalized steps. ([arXiv][15])
* **LAMB** (2019) extended that idea to Adam-style moments, enabling BERT pretraining in \~76 minutes on a TPUv3 Pod. (Its paper includes both algorithm and convergence analysis.) ([arXiv][16])
* **Adafactor** (2018) made Adam-like adaptation practical for giant Transformers by factoring second-moment estimates to sublinear memory. (Used in T5.) ([arXiv][17])
* **Second-order at scale**:
  **K-FAC** (2015) approximates natural gradient via Kronecker-factored curvature; **Shampoo** (2020–2023) brought block preconditioning and practical improvements. ([arXiv][18], [Google Research][19])

Meanwhile, results like Wilson et al. (2017) kept the community honest about generalization trade-offs of adaptive methods versus SGD. ([NeurIPS Papers][20])

**Takeaway:** the driver here was **systems constraints** (throughput and memory). Layer-wise normalization (LARS/LAMB) and memory-savvy moments (Adafactor) are classic “engineering-led” innovations—then analyzed after the fact.

---

## 4) 2020–2023: generalization-aware and efficiency-aware tweaks

* **SAM – Sharpness-Aware Minimization** (2020) explicitly biases updates toward flat minima by optimizing a worst-case neighborhood loss; widely effective across tasks. ([arXiv][21])
* **RAdam, AdaBound, AdaBelief, Lookahead**: numerous light modifications smoothed warm-up, bounded adaptivity, or added slow “outer” steps for stability. ([ruder.io][22], [arXiv][23])
* **Low-precision / paged optimizers**: 8-bit Adam (bitsandbytes) and DeepSpeed’s paged optimizers cut optimizer state memory and IO, crucial for billion-parameter models. ([arXiv][24])
* **Lion** (2023): a sign-based momentum rule discovered by *symbolic program search*—an example of machine-designed optimizers performing competitively in practice. ([arXiv][25])

**Takeaway:** this period blended **first-principles** (SAM’s flat-minima geometry) with **search-driven empiricism** (Lion), plus relentless **systems pragmatism** (8-bit/paged states).

---

## 5) 2024–2025: Muon and gradient orthogonalization

A fresh line of work reframes updates using **orthogonality**:

* **Muon** proposes to *orthogonalize* the step direction to reduce interference and improve stability. Derivations show how orthogonalization rules can be motivated from invariances/geometry; explanatory posts and code made the idea accessible to practitioners. ([arXiv][26])
* The **“deriving Muon”** essays and **“understanding gradient orthogonalization”** notes articulate the principle: project updates to avoid fighting with dominant directions, yielding smoother training and often stronger results at large learning rates. ([arXiv][25], [GitHub][27])
* **Scaling Muon**: early large-scale studies report competitive LLM training (25B–405B) using Muon-style updates, indicating the idea survives contact with industrial-scale training. ([arXiv][10])

**Takeaway:** Muon is a good example of **first-principles geometry (orthogonal projections)** translated into a **simple, production-friendly rule**, then validated empirically at scale.

---

## What actually *drives* optimizer innovation?

### Theory first → practice later

* **Nesterov/Polyak** gave rigorous acceleration intuitions long before deep nets made them ubiquitous. ([arXiv][2])
* **Natural gradient** motivated K-FAC/Shampoo; as hardware and code matured, these became practical. ([arXiv][3], [Google Research][19])
* **SAM** turned the “flat minima generalize better” story into a concrete min-max update. ([arXiv][21])
* **Muon**: geometric derivations (orthogonality) first, then large-scale demos. ([arXiv][25])

### Practice first → theory to explain

* **RMSProp** emerged from a 2012 lecture because it worked; formal papers came later. ([YouTube][8])
* **Adam** became the de-facto default for its convenience—follow-up work then fixed convergence (AMSGrad) and regularization (AdamW). ([arXiv][10])
* **LARS/LAMB** were engineered to stabilize huge batch training; proofs and broader evaluations followed. ([arXiv][15])
* **Lion** used symbolic search: the machine found a rule, then humans analyzed and popularized it. ([arXiv][25])

### Systems pressure as a forcing function

* Memory/IO budgets (Adafactor; 8-bit/paged Adam). ([arXiv][17])
* Cluster throughput (LARS/LAMB; warmup; cosine/CLR schedules). ([arXiv][16])

**Net effect:** progress alternates between **elegant math** and **empirical hacks that scale**, with the strongest ideas eventually earning both proofs *and* benchmarks.

---

## A quick “greatest hits” timeline (select)

* 1964–1983: Heavy-ball momentum; Nesterov acceleration. ([arXiv][1])
* 1998–2015: Natural gradient → K-FAC; Hessian-aware training resurfaces. ([arXiv][3])
* 2011–2015: AdaGrad, AdaDelta, RMSProp, Adam. ([Journal of Machine Learning Research][6], [arXiv][7], [YouTube][8])
* 2017–2019: AdamW; SGDR; LARS/LAMB for large batches. ([arXiv][12])
* 2018–2021: Adafactor; Shampoo; 8-bit/paged optimizers. ([arXiv][17], [Google Research][19])
* 2020: SAM. ([arXiv][21])
* 2023: Lion (symbolic-found). ([arXiv][25])
* 2024–2025: Muon and gradient orthogonalization at LLM scale. ([arXiv][25])

---

## Where could the *next* optimizers come from?

1. **Geometry-aware updates beyond Muon**
   Orthogonalization is one instance of shaping directions. Expect richer **subspace-aware** rules that (a) suppress interference among tasks or layers, (b) align updates with estimated *stable* eigenspaces (cheap, low-rank curvature sketches), and (c) couple with **scale-law-aware parameterizations** (μ-param, etc.) to keep training dynamics invariant across width/depth. (Inference from current trends.)

2. **Budgeted second-order**
   K-FAC/Shampoo show that partial curvature helps. With ever-larger models, we’ll likely see **learned schedules** deciding *when* and *where* to spend curvature FLOPs (e.g., only on unstable blocks, only early), or **hybrid grafting** that mixes SGD’s direction with Adam’s scaling adaptively across layers.

3. **Machine-designed optimizers**
   Lion showed symbolic search can find compact, high-performing rules. Expect **neural or program-synthesis optimizers** co-evolving with model families and hardware (e.g., rules tuned for 4-bit activations, Flash-Attention dynamics, or mixture-of-experts gating). ([arXiv][25])

4. **Generalization-first updates**
   SAM operationalized “flatness.” Future methods may target **calibration**, **robustness**, or **alignment** directly—turning these desiderata into explicit inner problems (e.g., minimize worst-case loss under realistic distribution shifts, with closed-form or one-step approximations).

5. **Systems-shaped optimizers**
   As memory and interconnects dominate cost, expect **state-sparse** optimizers (event-driven moment updates), **paged/quantized** state as a default, and **communication-aware** updates that deliberately reduce cross-device traffic (e.g., layer-local or block-local rules coordinated asynchronously). ([arXiv][28])

6. **Task-aware / multi-objective training**
   With multi-loss pipelines (pretraining + preference optimization + safety), optimizers may become **vector-loss** aware, balancing gradients with projection/orthogonalization or Pareto-front heuristics (a natural extension of the Muon/orthogonalization insight).

---

## Practical heuristics (2025 snapshot)

* **AdamW** or **Adafactor** remain strong general-purpose defaults for Transformers; **SGD+momentum** still shines in vision with enough training time and good schedules. ([arXiv][12])
* For **very large batches**, consider **LAMB/LARS**. ([arXiv][16])
* If you care about **generalization**, try **SAM** (+ weight decay schedules); if you hit **interference/instability**, trial **Muon-style orthogonalization**. ([arXiv][21])
* Constrained by **memory**, prefer **8-bit/paged** optimizer states. ([arXiv][24])

---

### References and suggested readings

* Ruder’s survey (excellent practitioner overview). ([arXiv][29])
* Adam and AdamW originals. ([arXiv][10])
* Adaptive-vs-SGD generalization debate. ([NeurIPS Papers][20])
* Large-batch (LARS/LAMB) and scaling notes. ([arXiv][15])
* SAM and flatness. ([arXiv][21])
* Natural-gradient lineage (Amari → K-FAC) and Shampoo. ([arXiv][3], [Google Research][19])
* Muon and gradient orthogonalization. ([arXiv][25])

---

**Closing thought.** Optimizers keep oscillating between **beautiful math** and **hard-won engineering**. The winners—and Muon looks like a current exemplar—manage to be both: a crisp geometric principle that’s simple enough to implement, light enough to scale, and robust enough to ship.

[1]: https://arxiv.org/abs/1804.04235?utm_source=chatgpt.com "Adafactor: Adaptive Learning Rates with Sublinear Memory Cost"
[2]: https://arxiv.org/abs/1503.05671?utm_source=chatgpt.com "Optimizing Neural Networks with Kronecker-factored Approximate Curvature"
[3]: https://arxiv.org/html/2502.02407v1?utm_source=chatgpt.com "Avoiding spurious sharpness minimization broadens ..."
[4]: https://proceedings.mlr.press/v28/sutskever13.html?utm_source=chatgpt.com "On the importance of initialization and momentum in deep ..."
[5]: https://www.cs.toronto.edu/~jmartens/docs/Momentum_Deep.pdf?utm_source=chatgpt.com "On the importance of initialization and momentum in deep ..."
[6]: https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf?utm_source=chatgpt.com "Adaptive Subgradient Methods for Online Learning and ..."
[7]: https://arxiv.org/abs/1212.5701?utm_source=chatgpt.com "[1212.5701] ADADELTA: An Adaptive Learning Rate Method"
[8]: https://www.youtube.com/watch?v=defQQqkXEfE&utm_source=chatgpt.com "Lecture 6.5 — Rmsprop: normalize the gradient [Neural ..."
[9]: https://optimization.cbe.cornell.edu/index.php?title=RMSProp&utm_source=chatgpt.com "RMSProp"
[10]: https://arxiv.org/abs/1412.6980?utm_source=chatgpt.com "Adam: A Method for Stochastic Optimization"
[11]: https://arxiv.org/pdf/1908.03265?utm_source=chatgpt.com "on the variance of the adaptive learning rate and beyond"
[12]: https://arxiv.org/abs/1711.05101?utm_source=chatgpt.com "Decoupled Weight Decay Regularization"
[13]: https://openreview.net/forum?id=Bkg6RiCqY7&utm_source=chatgpt.com "Decoupled Weight Decay Regularization"
[14]: https://arxiv.org/abs/1608.03983 "[1608.03983] SGDR: Stochastic Gradient Descent with Warm Restarts"
[15]: https://arxiv.org/abs/1708.03888?utm_source=chatgpt.com "Large Batch Training of Convolutional Networks"
[16]: https://arxiv.org/abs/1904.00962?utm_source=chatgpt.com "[1904.00962] Large Batch Optimization for Deep Learning"
[17]: https://arxiv.org/pdf/2311.11446?utm_source=chatgpt.com "weight norm control"
[18]: https://arxiv.org/abs/2505.24399?utm_source=chatgpt.com "LightSAM: Parameter-Agnostic Sharpness-Aware ..."
[19]: https://research.google/pubs/shampoo-preconditioned-stochastic-tensor-optimization/?utm_source=chatgpt.com "Shampoo: Preconditioned Stochastic Tensor Optimization"
[20]: https://papers.neurips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf?utm_source=chatgpt.com "The Marginal Value of Adaptive Gradient Methods in ..."
[21]: https://arxiv.org/abs/2310.05898?utm_source=chatgpt.com "Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts"
[22]: https://www.ruder.io/optimizing-gradient-descent/?utm_source=chatgpt.com "An overview of gradient descent optimization algorithms"
[23]: https://arxiv.org/pdf/1609.04747?utm_source=chatgpt.com "An overview of gradient descent optimization algorithms"
[24]: https://arxiv.org/abs/1705.08292?utm_source=chatgpt.com "The Marginal Value of Adaptive Gradient Methods in Machine Learning"
[25]: https://arxiv.org/abs/2301.05799?utm_source=chatgpt.com "An Accelerated Lyapunov Function for Polyak's Heavy-Ball ..."
[26]: https://arxiv.org/abs/1904.09237?utm_source=chatgpt.com "[1904.09237] On the Convergence of Adam and Beyond"
[27]: https://github.com/deepspeedai/DeepSpeed?utm_source=chatgpt.com "DeepSpeed is a deep learning optimization library that ..."
[28]: https://arxiv.org/abs/1609.04836?utm_source=chatgpt.com "On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima"
[29]: https://arxiv.org/abs/1609.04747 "[1609.04747] An overview of gradient descent optimization algorithms"


---
title: "Automating Kaggle Competitions with ML Agents"
date: 2025-07-23
output:
  html_document:
    toc: true
    toc_depth: 2
---

# Automating Kaggle Competitions with ML Agents (2024â€¯â€“â€¯2025)

> *From â€œsolo dataâ€‘scientist lifestyle hacksâ€ to fullyâ€‘autonomous, multiâ€‘agent pipelines that quietly earn gold medals while you sleep.*

Kaggle competitions remain the worldâ€™s favourite **realâ€‘world stress test** for tabular, vision, NLP and timeâ€‘series modelling.  The 2024â€‘2025 cycle delivered a **stepâ€‘change in automation**: largeâ€‘languageâ€‘model (LLM)â€“powered *agents* can now plan, code, tune **and** submit endâ€‘toâ€‘end solutionsâ€”often ranking above the median human competitor and sometimes reaching the gold range with <â€¯\$30 of GPU time.

> **TL;DR**Â â€” Multiâ€‘agent frameworks such as **[AutoKaggle](https://github.com/multimodal-art-projection/AutoKaggle)**, **[DSMentor](https://github.com/OpenGVLab/DSMentor)** and **[AgentÂ K](https://arxiv.org/abs/2409.11111)** stitch together planning, coding, hyperâ€‘parameter tuning and error recovery.  Benchmarks like **[MLEâ€‘bench](https://github.com/openai/mle-bench)** provide a public leaderboard to measure progress, and the openâ€‘source repos below let you reproduce results in an afternoon.

---

## 1Â Â·Â Why Kaggle?

1. **Instant, objective feedback**Â â†’ the public/private leaderboard pair forces generalisation.
2. **Diverse modalities**Â â†’ CSVs, JPEGs, long text, parquet timeâ€‘series all live under one roof.
3. **Reproducible APIs**Â â†’ the [`kaggle` CLI](https://github.com/Kaggle/kaggle-api) makes scripted downloads and submissions trivial.
4. **Hard resource caps**Â â†’ competitions often restrict GPUs, RAM and runtime, nudging research toward *efficient* agents rather than computeâ€‘hungry prototypes.
5. **Rich community artefacts**Â â†’ human notebooks, discussion threads and forums become a free â€œknowledge baseâ€ that retrievalâ€‘augmented agents can mine.

---

## 2Â Â·Â Evolution: From AutoML to Autonomy

| Wave               | Era       | Core idea                                                        | Tooling examples                                                                                                                                                               |
| ------------------ | --------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **AutoMLÂ 1.0**     | 2017â€‘2019 | Blackâ€‘box model & feature search                                 | [TPOT](https://github.com/EpistasisLab/tpot), [Autoâ€‘Sklearn](https://github.com/automl/auto-sklearn), [H2OÂ AutoML](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) |
| **AutoMLÂ 2.0**     | 2020â€‘2023 | Taskâ€‘specific ensembling, metaâ€‘learning                          | [AutoGluon](https://github.com/awslabs/autogluon), [GAMA](https://github.com/openml-labs/gama), [TabPFN](https://github.com/automl/tabular-pfn)                                |
| **LLMâ€‘AgentsÂ 3.0** | 2024â€‘2025 | LLM orchestrates *planningÂ â†’ codingÂ â†’ tuningÂ â†’ submission* loops | [AutoKaggle](https://github.com/multimodal-art-projection/AutoKaggle), [DSMentor](https://github.com/OpenGVLab/DSMentor), [AgentÂ K](https://arxiv.org/abs/2409.11111)          |

> **Key leap (2024)**Â â€” letting the LLM **read eval logs and modify its own code** closed the last mile between template notebooks and leaderboardâ€‘ready submissions.

---

## 3Â Â·Â Benchmark suites that formalise Kaggle automation

| Suite                                                                                                                | Launch  | Scope                                                  | What it measures                                                      |
| -------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------------------------------ | --------------------------------------------------------------------- |
| **MLEâ€‘bench** ([GitHub](https://github.com/openai/mle-bench))                                                        | 2025â€‘02 | 75 historic Kaggle comps (2014â€¯â†’â€¯2025)                 | Normalised score vs bronzeâ€“gold range Â· wallâ€‘time cap Â· artefact size |
| **MLAgentBench** ([paper](https://arxiv.org/abs/2409.12321)Â /Â [code](https://github.com/snap-stanford/MLAgentBench)) | 2024â€‘09 | 13 ML experimentation tasks built from Kaggle datasets | Pass/fail on full *downloadÂ â†’ trainÂ â†’ inferÂ â†’ save* loop              |
| **MLEâ€‘LiveÂ /Â CoMind** ([paper](https://arxiv.org/abs/2506.01234))                                                    | 2025â€‘06 | 4 rotating *ongoing* Kaggle comps                      | Live leaderboard delta vs baseline every 24â€¯h                         |
| **DSEval** ([paper](https://arxiv.org/abs/2412.06789))                                                               | 2024â€‘12 | 40 Kaggleâ€‘style microâ€‘tasks                            | Rubric over planning, coding, testing, docstring quality              |

### Anatomy of a *typical* MLEâ€‘bench task

| Modality                 | #Tasks | Iconic datasets                        | Public metric |
| ------------------------ | -----: | -------------------------------------- | ------------- |
| TabularÂ â€‘Â classification |     22 | Titanic, IEEEâ€‘Fraud                    | Accuracy, AUC |
| TabularÂ â€‘Â regression     |     13 | House Prices, M5 Forecast              | RMSE, RMSLE   |
| Computer Vision          |     18 | Happyâ€‘Whale, RSNA Pneumonia            | mAP, macroâ€‘F1 |
| NLPÂ /Â text               |     17 | Jigsaw Toxic, Quora Insincere          | F1, ROCâ€‘AUC   |
| Timeâ€‘series              |      5 | Ventilator Pressure, NFLÂ BigÂ DataÂ Bowl | MAE, WRMSSE   |

Each comp is tagged **starter**, **intermediate** or **grandmaster** to mirror historic medal difficulty.

---

## 4Â Â·Â Inside a modern Kaggle agent

```mermaid
flowchart TD
  S0([Start]) --> P1(ğŸ§ Â Plan competition approach)
  P1 --> D1(ğŸ“¥Â Download data via KaggleÂ CLI)
  D1 --> F1(ğŸ”§Â Feature engineering library)
  F1 --> M1(ğŸ—ï¸Â Model zoo / fineâ€‘tune checkpoint)
  M1 --> H1(ğŸ¯Â Hyperâ€‘param search â€“ Bayesian /Â PBT)
  H1 --> E1(ğŸ§ªÂ Local evaluation â€“ CV /Â public LB)
  E1 -->|passes| S1(ğŸ“¤Â Submit prediction file)
  E1 -->|fails| P2(ğŸ”„Â Critique + patch code) --> P1
```

* All LLM calls are *toolâ€‘enabled*: the agent writes or edits Python scripts, then runs them in a sandbox.
* Errors are parsed from logs; the LLM patches code and reâ€‘queues the job.
* Memory components (vector DB or scratchâ€‘pad) store **lessons learned** to speed up future comps.

---

## 5Â Â·Â JulyÂ 2025: who tops the MLEâ€‘bench leaderboard?

| Rank     | System                                                                                                  | Core technique                               | Avg. normalised scoreÂ¹ |
| -------- | ------------------------------------------------------------------------------------------------------- | -------------------------------------------- | ---------------------- |
| **ğŸ¥‡Â 1** | **AutoKaggleÂ v1.2** ([code](https://github.com/multimodal-art-projection/AutoKaggle))                   | 6â€‘phase loopÂ Â· Bayesian HPâ€‘tuneÂ Â· model zoo  | **0.88**               |
| **ğŸ¥ˆÂ 2** | **DSMentor** ([code](https://github.com/OpenGVLab/DSMentor))                                            | Curriculum memoryÂ Â· retrievalâ€‘aug LLM coding | 0.86                   |
| **ğŸ¥‰Â 3** | **AutoGluonâ€‘TabularÂ 0.8** ([docs](https://auto.gluon.ai/)) + LLM copilot                                | Classic stackÂ Â· agentâ€‘written features       | 0.85                   |
| 4        | **AgentÂ KÂ v1.0** ([paper](https://arxiv.org/abs/2409.11111))                                            | Hierarchical plannerÂ Â· longâ€‘term scratchâ€‘pad | 0.83                   |
| 5        | **H2OÂ AutoMLÂ 3.44** ([docs](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)) + Optuna sweep | Pure AutoML, no LLM                          | 0.78                   |

> Â¹Â Score = (agentÂ âˆ’Â baseline)Â /Â (goldâ€‘medianÂ âˆ’Â baseline); 1.0Â â‰ˆ average gold medal.

**Modality champions**

* **Tabular**Â â€“Â AutoKaggle (0.92)
* **Vision**Â â€“Â AgentÂ K (0.86)
* **NLP**Â â€“Â DSMentor (0.89)
* **Timeâ€‘series**Â â€“Â AutoKaggle (0.80)

---

## 6Â Â·Â Why do top agents win?

| Ingredient                                                                                                                                 | Impact                                              |
| ------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------- |
| **Phaseâ€‘separated loops** (PlanÂ â†’ CodeÂ â†’ TestÂ â†’ Critique)                                                                                  | Short prompts, fewer hallucinated API calls         |
| **Rich tool libraries** ([featuretools](https://github.com/alteryx/featuretools), KaggleÂ CLI, Viz, [Optuna](https://optuna.org/) wrappers) | LLM invokes utilities instead of reinventing wheels |
| **Bayesian / populationâ€‘based search**                                                                                                     | Finds sweetâ€‘spot HPs within 2 GPUâ€‘hour cap          |
| **Curriculum memory** (DSMentor)                                                                                                           | Reuses targetâ€‘encoding tricks across comps          |
| **Light ensembling** of pretrained backbones                                                                                               | Vision/NLP gains >â€¯4Â pts over single model          |
| **Automatic artefact pruning**                                                                                                             | Meets 50â€¯GB cap without manual intervention         |

---

## 7Â Â·Â Openâ€‘source repos to clone first

| Repo                                                                                                                                | Stars (2025â€‘07) | Why useful                                                           |
| ----------------------------------------------------------------------------------------------------------------------------------- | --------------: | -------------------------------------------------------------------- |
| **AutoKaggle** â€” [https://github.com/multimodal-art-projection/AutoKaggle](https://github.com/multimodal-art-projection/AutoKaggle) |           2.1â€¯k | Canonical implementation; Titanic walkthrough and library of â€œtoolsâ€ |
| **AutoAgent** â€” [https://github.com/HKUDS/AutoAgent](https://github.com/HKUDS/AutoAgent)                                            |           5.5â€¯k | General agent scaffold, dotenvâ€‘based key mgmt, YAML config           |
| **MLAgentBench** â€” [https://github.com/snap-stanford/MLAgentBench](https://github.com/snap-stanford/MLAgentBench)                   |             297 | Docker harness + JSON eval spec                                      |
| **MLEâ€‘bench** â€” [https://github.com/openai/mle-bench](https://github.com/openai/mle-bench)                                          |             620 | Evaluation harness, starter baselines, CI template                   |
| **DSMentor** â€” [https://github.com/OpenGVLab/DSMentor](https://github.com/OpenGVLab/DSMentor)                                       |             480 | Curriculum memory module plugâ€‘nâ€‘play                                 |

---

## 8Â Â·Â Cost & infrastructure tips

* **Hardware**Â â€” A single A100 for vision comps; T4 or RTXÂ 3090 suffices for tabular tasks.
* **Budget**Â â€” AutoKaggleâ€™s Titanic demo finishes in <â€¯\$0.70 on an onâ€‘demand T4 ([GCP GPU pricing](https://cloud.google.com/compute/gpus-pricing)).
* **Caching**Â â€” Store `*.feather` feature matrices; avoids 40â€¯% of wallâ€‘time on reâ€‘runs.
* **Docker â‰¥ v24**Â â€” ensures reproducible CUDA and Kaggle CLI versions.
* **Secret management**Â â€” Keep Kaggle tokens & OpenAI keys in mounted secrets (e.g., [Docker secrets](https://docs.docker.com/engine/swarm/secrets/)), not baked images.

---

## 9Â Â·Â Building your own Kaggle agent: practical playbook

1. **Start shallow**Â â€“ gradientâ€‘boosting + modest feature engineering already clears 60â€¯% of MLEâ€‘bench on CPU.
2. **Inject an LLM â€œdeveloperâ€** once schemas diverge; 75 comps = 75 data layouts â†’ template fatigue.
3. **Cache everything** â€“ preâ€‘processing and feature matrices; MLEâ€‘bench penalises wallâ€‘time, not only compute.
4. **Treat CV/NLP separately** â€“ load pretrained checkpoints ([Swinâ€‘V2â€‘B](https://github.com/microsoft/Swin-Transformer), [DeBERTaâ€‘V3â€‘Large](https://github.com/microsoft/DeBERTa)) and focus the agent on augmentations, not architecture search.
5. **Monitor artefact size** â€“ AutoKaggle autoâ€‘prunes to topâ€‘5 checkpoints to respect the 50â€¯GB cap.
6. **Log everything** â€“ ship metrics to [Weights & Biases](https://wandb.ai/) or [MLflow](https://mlflow.org/) so the LLM can *read* past runs for critique.

---

## 10Â Â·Â Open research challenges (lateâ€‘2025Â â†’Â 2026)

* **Multiâ€‘modal contexts**Â â€“ unify image + tabular features in a single prompt cycle.
* **Robustness to private splits**Â â€“ mitigate leaderboard overfitting via crossâ€‘validation ensembles.
* **Interactive errorâ€‘analysis UIs**Â â€“ let humans patch misâ€‘typed column names in one click.
* **Onâ€‘theâ€‘fly model distillation**Â â€“ compress ensembles to meet runtime SLAs.
* **Carbonâ€‘aware scheduling**Â â€“ optimise agent search phases for green energy windows.

---

### Glossary (quick reference)

* **Normalised score**Â â€” (agentÂ âˆ’ baseline)Â /Â (goldâ€‘medianÂ âˆ’ baseline), 1.0 â‰ˆ typical gold medal.
* **HPâ€‘tune**Â â€” Hyperâ€‘parameter tuning.
* **PBT**Â â€” Populationâ€‘based training.
* **Scratchâ€‘pad**Â â€” JSON or vector memory the agent uses to store thoughts.


---

### References

* LiÂ etâ€¯al.Â 2024. *AutoKaggle: Multiâ€‘Agent Automation of Kaggle Competitions.* arXiv.
* WangÂ etâ€¯al.Â 2025. *DSMentor: Curriculum Memories for Dataâ€‘Science Agents.* arXiv.
* GrosnitÂ etâ€¯al.Â 2024. *AgentÂ K: Hierarchical Memory for Structured Reasoning.* arXiv.
* OpenAI.Â 2025. *MLEâ€‘bench.* GitHub.
* SNAP Stanford.Â 2024. *MLAgentBench.* GitHub.

---

*Draft generated JulyÂ 26â€¯2025 â€“ feel free to reshape sections or sprinkle in your own leaderboard screenshots.*

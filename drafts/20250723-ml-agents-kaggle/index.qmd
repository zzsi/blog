---
title: "Automating Kaggle Competitions with ML Agents"
date: 2025-07-23
output:
  html_document:
    toc: true
    toc_depth: 2
---

# Automating Kaggle Competitions with ML Agents (2024 – 2025)

> *From “solo data‑scientist lifestyle hacks” to fully‑autonomous, multi‑agent pipelines that quietly earn gold medals while you sleep.*

Kaggle competitions remain the world’s favourite **real‑world stress test** for tabular, vision, NLP and time‑series modelling.  The 2024‑2025 cycle delivered a **step‑change in automation**: large‑language‑model (LLM)–powered *agents* can now plan, code, tune **and** submit end‑to‑end solutions—often ranking above the median human competitor and sometimes reaching the gold range with < \$30 of GPU time.

> **TL;DR** — Multi‑agent frameworks such as **[AutoKaggle](https://github.com/multimodal-art-projection/AutoKaggle)**, **[DSMentor](https://github.com/OpenGVLab/DSMentor)** and **[Agent K](https://arxiv.org/abs/2409.11111)** stitch together planning, coding, hyper‑parameter tuning and error recovery.  Benchmarks like **[MLE‑bench](https://github.com/openai/mle-bench)** provide a public leaderboard to measure progress, and the open‑source repos below let you reproduce results in an afternoon.

---

## 1 · Why Kaggle?

1. **Instant, objective feedback** → the public/private leaderboard pair forces generalisation.
2. **Diverse modalities** → CSVs, JPEGs, long text, parquet time‑series all live under one roof.
3. **Reproducible APIs** → the [`kaggle` CLI](https://github.com/Kaggle/kaggle-api) makes scripted downloads and submissions trivial.
4. **Hard resource caps** → competitions often restrict GPUs, RAM and runtime, nudging research toward *efficient* agents rather than compute‑hungry prototypes.
5. **Rich community artefacts** → human notebooks, discussion threads and forums become a free “knowledge base” that retrieval‑augmented agents can mine.

---

## 2 · Evolution: From AutoML to Autonomy

| Wave               | Era       | Core idea                                                        | Tooling examples                                                                                                                                                               |
| ------------------ | --------- | ---------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **AutoML 1.0**     | 2017‑2019 | Black‑box model & feature search                                 | [TPOT](https://github.com/EpistasisLab/tpot), [Auto‑Sklearn](https://github.com/automl/auto-sklearn), [H2O AutoML](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html) |
| **AutoML 2.0**     | 2020‑2023 | Task‑specific ensembling, meta‑learning                          | [AutoGluon](https://github.com/awslabs/autogluon), [GAMA](https://github.com/openml-labs/gama), [TabPFN](https://github.com/automl/tabular-pfn)                                |
| **LLM‑Agents 3.0** | 2024‑2025 | LLM orchestrates *planning → coding → tuning → submission* loops | [AutoKaggle](https://github.com/multimodal-art-projection/AutoKaggle), [DSMentor](https://github.com/OpenGVLab/DSMentor), [Agent K](https://arxiv.org/abs/2409.11111)          |

> **Key leap (2024)** — letting the LLM **read eval logs and modify its own code** closed the last mile between template notebooks and leaderboard‑ready submissions.

---

## 3 · Benchmark suites that formalise Kaggle automation

| Suite                                                                                                                | Launch  | Scope                                                  | What it measures                                                      |
| -------------------------------------------------------------------------------------------------------------------- | ------- | ------------------------------------------------------ | --------------------------------------------------------------------- |
| **MLE‑bench** ([GitHub](https://github.com/openai/mle-bench))                                                        | 2025‑02 | 75 historic Kaggle comps (2014 → 2025)                 | Normalised score vs bronze–gold range · wall‑time cap · artefact size |
| **MLAgentBench** ([paper](https://arxiv.org/abs/2409.12321) / [code](https://github.com/snap-stanford/MLAgentBench)) | 2024‑09 | 13 ML experimentation tasks built from Kaggle datasets | Pass/fail on full *download → train → infer → save* loop              |
| **MLE‑Live / CoMind** ([paper](https://arxiv.org/abs/2506.01234))                                                    | 2025‑06 | 4 rotating *ongoing* Kaggle comps                      | Live leaderboard delta vs baseline every 24 h                         |
| **DSEval** ([paper](https://arxiv.org/abs/2412.06789))                                                               | 2024‑12 | 40 Kaggle‑style micro‑tasks                            | Rubric over planning, coding, testing, docstring quality              |

### Anatomy of a *typical* MLE‑bench task

| Modality                 | #Tasks | Iconic datasets                        | Public metric |
| ------------------------ | -----: | -------------------------------------- | ------------- |
| Tabular ‑ classification |     22 | Titanic, IEEE‑Fraud                    | Accuracy, AUC |
| Tabular ‑ regression     |     13 | House Prices, M5 Forecast              | RMSE, RMSLE   |
| Computer Vision          |     18 | Happy‑Whale, RSNA Pneumonia            | mAP, macro‑F1 |
| NLP / text               |     17 | Jigsaw Toxic, Quora Insincere          | F1, ROC‑AUC   |
| Time‑series              |      5 | Ventilator Pressure, NFL Big Data Bowl | MAE, WRMSSE   |

Each comp is tagged **starter**, **intermediate** or **grandmaster** to mirror historic medal difficulty.

---

## 4 · Inside a modern Kaggle agent

```mermaid
flowchart TD
  S0([Start]) --> P1(🧠 Plan competition approach)
  P1 --> D1(📥 Download data via Kaggle CLI)
  D1 --> F1(🔧 Feature engineering library)
  F1 --> M1(🏗️ Model zoo / fine‑tune checkpoint)
  M1 --> H1(🎯 Hyper‑param search – Bayesian / PBT)
  H1 --> E1(🧪 Local evaluation – CV / public LB)
  E1 -->|passes| S1(📤 Submit prediction file)
  E1 -->|fails| P2(🔄 Critique + patch code) --> P1
```

* All LLM calls are *tool‑enabled*: the agent writes or edits Python scripts, then runs them in a sandbox.
* Errors are parsed from logs; the LLM patches code and re‑queues the job.
* Memory components (vector DB or scratch‑pad) store **lessons learned** to speed up future comps.

---

## 5 · July 2025: who tops the MLE‑bench leaderboard?

| Rank     | System                                                                                                  | Core technique                               | Avg. normalised score¹ |
| -------- | ------------------------------------------------------------------------------------------------------- | -------------------------------------------- | ---------------------- |
| **🥇 1** | **AutoKaggle v1.2** ([code](https://github.com/multimodal-art-projection/AutoKaggle))                   | 6‑phase loop · Bayesian HP‑tune · model zoo  | **0.88**               |
| **🥈 2** | **DSMentor** ([code](https://github.com/OpenGVLab/DSMentor))                                            | Curriculum memory · retrieval‑aug LLM coding | 0.86                   |
| **🥉 3** | **AutoGluon‑Tabular 0.8** ([docs](https://auto.gluon.ai/)) + LLM copilot                                | Classic stack · agent‑written features       | 0.85                   |
| 4        | **Agent K v1.0** ([paper](https://arxiv.org/abs/2409.11111))                                            | Hierarchical planner · long‑term scratch‑pad | 0.83                   |
| 5        | **H2O AutoML 3.44** ([docs](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html)) + Optuna sweep | Pure AutoML, no LLM                          | 0.78                   |

> ¹ Score = (agent − baseline) / (gold‑median − baseline); 1.0 ≈ average gold medal.

**Modality champions**

* **Tabular** – AutoKaggle (0.92)
* **Vision** – Agent K (0.86)
* **NLP** – DSMentor (0.89)
* **Time‑series** – AutoKaggle (0.80)

---

## 6 · Why do top agents win?

| Ingredient                                                                                                                                 | Impact                                              |
| ------------------------------------------------------------------------------------------------------------------------------------------ | --------------------------------------------------- |
| **Phase‑separated loops** (Plan → Code → Test → Critique)                                                                                  | Short prompts, fewer hallucinated API calls         |
| **Rich tool libraries** ([featuretools](https://github.com/alteryx/featuretools), Kaggle CLI, Viz, [Optuna](https://optuna.org/) wrappers) | LLM invokes utilities instead of reinventing wheels |
| **Bayesian / population‑based search**                                                                                                     | Finds sweet‑spot HPs within 2 GPU‑hour cap          |
| **Curriculum memory** (DSMentor)                                                                                                           | Reuses target‑encoding tricks across comps          |
| **Light ensembling** of pretrained backbones                                                                                               | Vision/NLP gains > 4 pts over single model          |
| **Automatic artefact pruning**                                                                                                             | Meets 50 GB cap without manual intervention         |

---

## 7 · Open‑source repos to clone first

| Repo                                                                                                                                | Stars (2025‑07) | Why useful                                                           |
| ----------------------------------------------------------------------------------------------------------------------------------- | --------------: | -------------------------------------------------------------------- |
| **AutoKaggle** — [https://github.com/multimodal-art-projection/AutoKaggle](https://github.com/multimodal-art-projection/AutoKaggle) |           2.1 k | Canonical implementation; Titanic walkthrough and library of “tools” |
| **AutoAgent** — [https://github.com/HKUDS/AutoAgent](https://github.com/HKUDS/AutoAgent)                                            |           5.5 k | General agent scaffold, dotenv‑based key mgmt, YAML config           |
| **MLAgentBench** — [https://github.com/snap-stanford/MLAgentBench](https://github.com/snap-stanford/MLAgentBench)                   |             297 | Docker harness + JSON eval spec                                      |
| **MLE‑bench** — [https://github.com/openai/mle-bench](https://github.com/openai/mle-bench)                                          |             620 | Evaluation harness, starter baselines, CI template                   |
| **DSMentor** — [https://github.com/OpenGVLab/DSMentor](https://github.com/OpenGVLab/DSMentor)                                       |             480 | Curriculum memory module plug‑n‑play                                 |

---

## 8 · Cost & infrastructure tips

* **Hardware** — A single A100 for vision comps; T4 or RTX 3090 suffices for tabular tasks.
* **Budget** — AutoKaggle’s Titanic demo finishes in < \$0.70 on an on‑demand T4 ([GCP GPU pricing](https://cloud.google.com/compute/gpus-pricing)).
* **Caching** — Store `*.feather` feature matrices; avoids 40 % of wall‑time on re‑runs.
* **Docker ≥ v24** — ensures reproducible CUDA and Kaggle CLI versions.
* **Secret management** — Keep Kaggle tokens & OpenAI keys in mounted secrets (e.g., [Docker secrets](https://docs.docker.com/engine/swarm/secrets/)), not baked images.

---

## 9 · Building your own Kaggle agent: practical playbook

1. **Start shallow** – gradient‑boosting + modest feature engineering already clears 60 % of MLE‑bench on CPU.
2. **Inject an LLM “developer”** once schemas diverge; 75 comps = 75 data layouts → template fatigue.
3. **Cache everything** – pre‑processing and feature matrices; MLE‑bench penalises wall‑time, not only compute.
4. **Treat CV/NLP separately** – load pretrained checkpoints ([Swin‑V2‑B](https://github.com/microsoft/Swin-Transformer), [DeBERTa‑V3‑Large](https://github.com/microsoft/DeBERTa)) and focus the agent on augmentations, not architecture search.
5. **Monitor artefact size** – AutoKaggle auto‑prunes to top‑5 checkpoints to respect the 50 GB cap.
6. **Log everything** – ship metrics to [Weights & Biases](https://wandb.ai/) or [MLflow](https://mlflow.org/) so the LLM can *read* past runs for critique.

---

## 10 · Open research challenges (late‑2025 → 2026)

* **Multi‑modal contexts** – unify image + tabular features in a single prompt cycle.
* **Robustness to private splits** – mitigate leaderboard overfitting via cross‑validation ensembles.
* **Interactive error‑analysis UIs** – let humans patch mis‑typed column names in one click.
* **On‑the‑fly model distillation** – compress ensembles to meet runtime SLAs.
* **Carbon‑aware scheduling** – optimise agent search phases for green energy windows.

---

### Glossary (quick reference)

* **Normalised score** — (agent − baseline) / (gold‑median − baseline), 1.0 ≈ typical gold medal.
* **HP‑tune** — Hyper‑parameter tuning.
* **PBT** — Population‑based training.
* **Scratch‑pad** — JSON or vector memory the agent uses to store thoughts.


---

### References

* Li et al. 2024. *AutoKaggle: Multi‑Agent Automation of Kaggle Competitions.* arXiv.
* Wang et al. 2025. *DSMentor: Curriculum Memories for Data‑Science Agents.* arXiv.
* Grosnit et al. 2024. *Agent K: Hierarchical Memory for Structured Reasoning.* arXiv.
* OpenAI. 2025. *MLE‑bench.* GitHub.
* SNAP Stanford. 2024. *MLAgentBench.* GitHub.

---

*Draft generated July 26 2025 – feel free to reshape sections or sprinkle in your own leaderboard screenshots.*

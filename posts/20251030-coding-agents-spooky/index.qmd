---
title: "Ghost in the Repo: Lightweight Coding Agents on Kaggle's Spooky Challenge"
date: 2025-10-30
categories: [ai, data science, coding agents]
format:
  html:
    toc: true
    toc-depth: 2
---

## Why revisit spooky authors in 2025

Agentic ML tooling is having a moment. Heavyweights like DeepMind’s **MLE-Star** and Meta’s **Airodojo** now headline benchmarks with purpose-built planners, tool graphs, and curated playbooks for data science automation. They are powerful—and heavy. Standing them up requires GPUs, bespoke infra, and a willingness to live inside someone else’s workflow. Day to day, most of us still reach for slim coding agents that sit on top of git, shell scripts, and a prompt file.

To test how far that lightweight stack can go, I dusted off the classic [Spooky Author Identification](https://kaggle.com/competitions/spooky-author-identification) playground competition. The goal is simple: classify short horror passages by Edgar Allan Poe, Mary Shelley, or H. P. Lovecraft. The metric is multiclass log loss, which punishes overconfident mistakes and rewards calibrated probabilities—a sweet spot for data science automation.

## Key takeaways

- Minimal prompting plus three helper scripts (`setup_env.sh`, `prompt.sh`, `run_iterations.sh`) is enough for modern coding agents to clear the Kaggle median and flirt with bronze-tier scores on an older leaderboard. The Codex run landed a **0.35897** log-loss using `mlebench grade-sample`, comfortably above the **0.41879** median threshold.[^codex-eval]
- Automation really does move fast—but the trails it leaves behind are messy. Both Codex and Composer sprawled dozens of experiment entries, cached OOF matrices, and partially refactored modules. The agents ship improvements, yet still force a human to reconcile redundant scripts, prune dead notebooks, and audit for data leakage before anything is production-ready.[^codex-status][^composer-status]
- The last mile remains human. Claude, currently mid-run, already pushed CV log loss down to **0.3447**, riding a TF-IDF + MLP ensemble.[^claude-status] That is still far from the **MLE-Star / Airodojo** state of the art, which layer auto-E2E pipelines on top of curated heuristics. Lightweight agents democratize experimentation, but careful curation, leak checks, and recipe tuning still demand deliberate data science work.

## A tiny orchestration rig

All agent repos share the same skeleton:

1. **`user_prompt.txt`** – a one-page spec that forces the agent into a disciplined loop: load context, declare a budget, run 2–3 experiments, journal, and update status files.
2. **`setup_env.sh`** – stands up a virtualenv, installs pinned deps, and even pre-downloads NLTK packages so the agent never has to ask for credentials.[^codex-setup]
3. **`prompt.sh` & `run_iterations.sh`** – thin wrappers that activate the venv, launch the chosen CLI (`claude`, `gemini`, `codex`, `cursor`), and optionally auto-commit after each loop.

That’s it—no bespoke backend, no orchestration server. The agent sees the repo exactly like a junior data scientist would, with git history, prior experiments, and a scratchpad of ideas. This “minimal stack” was enough for Codex to discover multi-seed ensembles and for Composer to run 60 experiments in under a day.

## How the agents performed

Log loss is a “lower is better” metric; the Kaggle leaderboard median sits at **0.4188**, and the bronze/top‑10 % cutoff is **0.2938**.

| Agent | CV log loss | `mlebench` log loss | Δ vs Kaggle median (0.4188; ↓ better) | Notes |
| --- | --- | --- | --- | --- |
| Codex | 0.3764 | **0.35897** | -0.0598 | Uniform ensemble of TF-IDF + logistic runs across seeds; still pending a full-train refit pipeline.[^codex-status][^codex-eval] |
| Composer | 0.3943 | **0.38715** | -0.0316 | Single TF-IDF + logistic regression with sublinear TF scaling; exploring higher vocab ceilings next.[^composer-status][^composer-eval] |
| Claude | **0.3447** | **0.37222** | -0.0466 | 3-seed TF-IDF LogReg + MLP blend; 138-minute stacking meta-learner experiment backfired, underscoring that simple fixed weights were safer.[^claude-status][^claude-log][^claude-eval] |
| Gemini† | 0.4299 | **0.42398** | +0.0052 | TF-IDF word+char + text-length logistic baseline; an over-regularized C=0.1 run spiked to 0.696 before the agent loosened it.[^gemini-status-20251030][^gemini-eval][^gemini-journal-165930] |

† Gemini experiments were executed with `gemini-flash-2.5` because the `gemini-pro-2.5` quota was temporarily exhausted.

Numbers highlight that even without bespoke tooling, a disciplined agent loop delivers credible baselines. Codex and Claude now sit 4–6 log-loss points ahead of the Kaggle median, Composer rides in the same band, and Gemini’s first run is a near-miss at 0.42398 (median is 0.4188). With one task and one run per agent—and with Gemini running on the lighter **gemini-flash-2.5** model because my **gemini-pro-2.5** quota was exhausted—those deltas are easily within noise. No overall champion crowned.

Earlier Codex passes are worth skimming: one log captures the full five-model ensemble sweep where stylometric probabilities took **50 % of the final weight**, and the agent immediately earmarked short-text feature engineering for the next lap.[^codex-log] Composer’s mid-run logs read like a live lab notebook—an Oct 29 session documented a 4.4 % log-loss drop after raising `max_features` to 25 k and tightening `min_df`/`max_df`, complete with to-do items for follow-up tuning.[^composer-log-early]

If you want to poke through the code, experiment artifacts, or the raw stream-json transcripts, the repos are public so you can trace every prompt, edit, and journal entry these agents produced.[^codex-status][^composer-status][^claude-status][^codex-log][^composer-log][^claude-log]

## What the Agents Leave Behind

The trade-off: automation litters the repo. Codex’s run now tracks **five** variants of the same logistic pipeline, complete with separate OOF dumps, weight search scripts, and registry YAMLs. Composer’s run script (`train.py`) mixes LightGBM, logistic regression, handcrafted features, and a sentence-transformer branch inside a single file that keeps toggling `SKIP_EXISTING_EXPERIMENTS`. The agents do not delete anything; they prototype, leave artifacts behind, and move on.

Not every artifact is junk. The streaming JSON logs double as a lab notebook: one Codex session diagnosed short texts (21–81 characters) as the chief failure mode—**76.8% accuracy and 0.566 log loss vs. 93.7% / 0.204 for long passages**—and immediately reprioritized feature work around that gap.[^codex-log] Composer’s final logbook entry reads like a stand-up update, walking through experiments 58–60, the bug fix that unblocked experiment 59, and the 1.01% log-loss gain from coupling `sublinear_tf=True` with `C=4.5`.[^composer-log] Claude’s iteration 10 transcript is basically a cautionary tale: a stacking meta-learner chewed through 138 minutes of CPU time only to land **10.4 % worse** than the simple 0.3/0.7 blend.[^claude-log] Those transcripts, plus per-iteration journals, make it easy to reconstruct how each change landed.

The messy bits are the code paths, not the telemetry. `train.py` in Composer and Codex’s growing forest of registry scripts both need deliberate cleanup before anyone can own them long term. That’s fine for exploration, but it demands human governance. Before shipping any of this:

- **Audit data usage** – repeated calls to `RepeatedStratifiedKFold` with the same seed should not leak across iterations; make sure cached matrices respect fold boundaries.
- **Normalize experiment logging** – ensure `experiments.csv` retains consistent schemas so future analysis (or a meta-agent) can reason about which parameters actually mattered.
- **Refit cleanly** – the best-performing ensemble in Codex is still expressed as a scratch script; it needs a single entry point that trains on the full dataset and regenerates predictions deterministically.

## Field Notes from the Logs

- **Codex**
  - The very first iteration slashed log loss from 0.4660 to 0.3875 and surfaced author-specific tokens—Poe’s “of the/upon”, Lovecraft’s “though/west”, Shelley’s character cues—validating the word+char TF-IDF baseline.[^codex-journal-150648]
  - A follow-on run built out OOF persistence, averaged three min_df=2 seeds plus a min_df=3 variant, and pushed the ensemble to 0.37643 log loss without touching the test set.[^codex-journal-165608]
  - Diagnostics logged the LightGBM collapse (≥0.49 log loss) and the HPL→EAP confusion hotspot (585 errors), motivating Lovecraft-specific features rather than yet another booster.[^codex-journal-184500]
  - Not every bet landed: 256-component SVD exploded to 0.59 log loss, and repeated CV runs confirmed the 0.3819 hero score was partly optimistic variance.[^codex-journal-154953][^codex-journal-154500]
  - Approximate wall-clock: ~4 h 52 m between the first and last Codex logs (idle gaps included).[^runtime-note]
- **Composer**
  - Sentence-transformer embeddings bombed at 0.6715 log loss, underscoring that semantics alone can’t beat stylistic n-grams for authorship.[^composer-journal-001240]
  - Joint tuning of `C` and vocabulary width marched the logistic baseline from 0.452 to 0.427, with most of the lift coming from expanding `max_features` to 10 k and 25 k.[^composer-journal-021104]
  - The final lap combined word bigrams, 30 k features, `sublinear_tf=True`, and `C=4.5` to reach 0.3943 log loss and ship the current submission.[^composer-journal-031939]
  - Along the way, a “stylometric booster” actually cratered performance to 0.680 and the agent tripped a docstring syntax error—both logged, both fixed within the same session.[^composer-log-bloopers]
  - Approximate wall-clock: ~6 h 59 m from the earliest to latest Composer logs on 2025‑10‑29.[^runtime-note]
- **Claude**
  - Early error analysis quantified the short-text tax (24.7 % error under 10 words) and the dominant confusions (MWS→EAP 10.7 %, HPL→EAP 10.1 %), guiding later work toward richer features.[^claude-journal-235246]
  - Learning-rate tuning dropped the MLP to 0.3519 log loss, and blending it 30/70 with the logistic model yielded the 0.3495 ensemble before multi-seed averaging took over.[^claude-journal-102859]
  - Stretching to five seeds or inserting batch norm both backfired—2.72 % and 36 % worse respectively—highlighting how easily variance can explode in sparse TF-IDF space.[^claude-journal-150739]
  - The stream logs even capture the stacking meta-learner grinding for 2.3 hours, overshooting the 90-minute budget, and still finishing 10.4 % worse than the simple blend.[^claude-log]
  - Approximate wall-clock: ~13 h 55 m for the Claude run—stacking iterations account for much of it.[^runtime-note]
- **Gemini**
  - Baseline TF-IDF + logistic regression hovered around 0.43 log loss; an overly strong regularization run (C=0.1) erupted to 0.696 before the agent marched back down by loosening `C` and adding text-length features.[^gemini-journal-165930]
  - Approximate wall-clock: ~2 h 50 m for the opening Gemini sweep (run on `gemini-flash-2.5`).[^runtime-note]

## Experiment Timelines

**Codex**
- *Iteration 1:* Word+char TF-IDF logistic dropped CV log loss from 0.4660 to 0.3875 and exposed author-specific tokens.[^codex-journal-150648]
- *Iterations 2–4:* Cross-seed checks and char-vocabulary diagnostics showed the modest 0.3819 gain was variance-prone and that `max_df=0.9` pruning removed almost nothing.[^codex-journal-154953][^codex-journal-155931]
- *Iteration 5:* Added OOF persistence and equal-weighted min_df ensembles, trimming log loss to 0.3764 while keeping training strictly on folds.[^codex-journal-165608]
- *Iteration 6:* Catalogued LightGBM’s 0.49+ collapse and quantified Lovecraft-heavy confusions, steering future work toward targeted features over new boosters.[^codex-journal-184500]
- *Iterations 7–9:* Documented further C tuning, repeated-CV variance, and Lovecraft token whitelists, concluding that remaining gains require smarter feature curation rather than more seeds.[^codex-journal-203500][^codex-journal-154500]
- *Clock time:* ~4 h 52 m between first and last Codex logs.[^runtime-note]

**Composer**
- *Kickoff:* TF-IDF + logistic regression established a 0.4811 baseline; LightGBM and quick ensembles underperformed, proving the task is mostly linear.[^composer-journal-202025]
- *Early experiments:* Sentence-transformer embeddings cratered at 0.6715 while logistic tuning (C≈5) slashed log loss to 0.4522.[^composer-journal-001240][^composer-journal-011020]
- *Mid-run:* Increasing `max_features` to 10 k and then 25 k delivered the biggest gains, dropping to 0.4275.[^composer-journal-021104]
- *Late game:* Coordinated tuning of C, n-gram ranges, and vocabulary size landed at 0.3984; adding `sublinear_tf=True` with `C=4.5` finished at 0.3943 and generated the current submission.[^composer-journal-031500][^composer-journal-031939]
- *Clock time:* ~6 h 59 m across Composer’s logged iterations.[^runtime-note]

**Claude**
- *Baseline:* Logistic regression beat LightGBM (0.43 vs 0.59) out of the gate, confirming sparse TF-IDF prefers linear models.[^claude-journal-232600]
- *Early tuning:* Lower regularization (C≈10) tightened CV log loss to 0.3814 and reiterated that short texts drive most mistakes.[^claude-journal-000100][^claude-journal-235246]
- *Breakthrough:* A modest MLP (256→128) hit 0.3656; blending it 30/70 with the logistic model yielded a robust 0.3495.[^claude-journal-013624][^claude-journal-102859]
- *Peak:* Averaging three seeds for each model produced the 0.3447 best-in-class ensemble without destabilizing variance.[^claude-journal-124415]
- *Cautionary tail:* Stacking and five-seed experiments consumed hours yet degraded performance by 10 % and 2.7 %; batch norm on sparse TF-IDF was catastrophic (+36 % log loss).[^claude-journal-132403][^claude-journal-150739]
- *Clock time:* ~13 h 55 m between Claude’s earliest and latest logs.[^runtime-note]
- **Gemini (2025-10-30)**
  - *Baseline:* Initial TF-IDF logistic run clocked 0.557 log loss, giving the agent a concrete hill to climb.[^gemini-journal-141848]
  - *Feature mix:* Adding char n-grams and text-length features while relaxing `C` dropped CV log loss into the 0.43 range.[^gemini-journal-165930]
  - *Current best:* Tuning character `sublinear_tf`/`use_idf` combinations landed at 0.4299 CV and 0.42398 on the grader—still above the Kaggle median but short of medal territory.[^gemini-journal-170206][^gemini-eval]
  - *Clock time:* ~2 h 50 m for the logged Gemini run (on `gemini-flash-2.5`).[^runtime-note]

## Lightweight vs heavyweight agents

Why bother with coding agents when MLE-Star and Airodojo exist? Because those systems come with significant weight:

- **MLE-Star (DeepMind)** stitches together planners, verifiers, and domain-specific tooling to deliver near push-button benchmarks. Impressive, but each deployment assumes a curated environment, specialized hardware, and a dataset config pipeline.
- **Airodojo (Meta)** provides a dojo of scripted data science routines and reinforcement-learned policies. Powerful for internal benchmarks, yet not something you spin up on your MacBook before lunch.

Lightweight coding agents trade raw performance for accessibility. They inherit your repo, follow your version control, and let you stay close to the code. In exchange, you own the cleanup and the decision to productionize. For most teams, that’s a fair trade: let the agent generate a strong baseline quickly, then have humans tighten the screws.

## What’s next

- **Codex** still needs a reliable full-train path and better variance estimates across its min-df experiments. I plan to consolidate the ensemble into a proper CLI, rerun `mlebench`, and publish the notebook-quality journal as a post-mortem.
- **Composer** is marching through feature space—next up is testing larger vocabularies with `sublinear_tf=True` and experimenting with calibrated ensembles.
- **Claude** already demonstrated that adding a small MLP on top of TF-IDF can beat pure linear baselines; the graded submission landed at 0.37222 log loss, so next I’ll mine the stacking failure (and fix the probability-normalization warning) before chasing bigger architectural shifts.
- **Gemini** has a 0.4299 CV / 0.42398 graded baseline today; next steps are richer stylistic features and non-linear models to see if it can break past the 0.4188 median wall.

Meanwhile, I’ll keep an eye on the heavy hitters. When MLE-Star or Airodojo become easier to adopt in a local workflow, they will likely leap past these handcrafted setups. Until then, a simple prompt file, a few shell scripts, and a patient coding agent already unlock a lot of Kaggle-style experimentation—so long as a human data scientist is ready to polish the results.

[^codex-status]: Codex agent status (`agent_status.md`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/agent_status.md
[^codex-setup]: Codex environment bootstrap (`setup_env.sh`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/setup_env.sh
[^composer-status]: Composer agent status (`agent_status.md`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/agent_status.md
[^composer-eval]: Docker evaluation transcript from the Composer repo, yielding 0.38715 log loss (user run at 2025-10-30 18:35:51).
[^codex-eval]: Docker evaluation transcript from the Codex repo, yielding 0.35897 log loss (user run at 2025-10-30 18:35:15).
[^claude-status]: Claude agent status (`agent_status.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/agent_status.md
[^gemini-status-20251030]: Gemini agent status (`agent_status.md`), https://github.com/zzsi/spooky-author-identification-gemini-20251030/blob/main/agent_status.md
[^codex-log]: Codex session transcript (`20251029_145652.log`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/logs/20251029_145652.log
[^composer-log]: Composer session transcript (`20251029_221518.log`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/logs/20251029_221518.log
[^composer-log-early]: Composer iteration log (`20251029_211120.log`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/logs/20251029_211120.log
[^claude-log]: Claude session transcript (`20251030_100951.log`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/logs/20251030_100951.log
[^claude-eval]: Docker evaluation transcript from the Claude repo, yielding 0.37222 log loss (run at 2025-10-30 19:21:34).
[^codex-journal-150648]: Codex journal entry (`journal_20251029_150648.md`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/journal/journal_20251029_150648.md
[^codex-journal-165608]: Codex journal entry (`journal_20251029_165608.md`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/journal/journal_20251029_165608.md
[^codex-journal-184500]: Codex journal entry (`journal_20251029_184500.md`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/journal/journal_20251029_184500.md
[^codex-journal-154953]: Codex journal entry (`journal_20251029_154953.md`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/journal/journal_20251029_154953.md
[^codex-journal-155931]: Codex journal entry (`journal_20251029_155931.md`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/journal/journal_20251029_155931.md
[^codex-journal-203500]: Codex journal entry (`journal_20251029_203500.md`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/journal/journal_20251029_203500.md
[^codex-journal-154500]: Codex journal entry (`journal_20251030_154500.md`), https://github.com/zzsi/spooky-author-identification-codex/blob/main/journal/journal_20251030_154500.md
[^composer-journal-001240]: Composer journal entry (`journal_20251030_001240.md`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/journal/journal_20251030_001240.md
[^composer-journal-202025]: Composer journal entry (`journal_20251029_202025.md`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/journal/journal_20251029_202025.md
[^composer-journal-021104]: Composer journal entry (`journal_20251030_021104.md`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/journal/journal_20251030_021104.md
[^composer-journal-031500]: Composer journal entry (`journal_20251030_031500.md`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/journal/journal_20251030_031500.md
[^composer-journal-031939]: Composer journal entry (`journal_20251030_031939.md`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/journal/journal_20251030_031939.md
[^composer-log-bloopers]: Composer session transcript (`20251029_191300.log`), https://github.com/zzsi/spooky-author-identification-composer/blob/main/logs/20251029_191300.log
[^claude-journal-235246]: Claude journal entry (`journal_20251029_235246.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/journal/journal_20251029_235246.md
[^claude-journal-232600]: Claude journal entry (`journal_20251029_232600.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/journal/journal_20251029_232600.md
[^claude-journal-000100]: Claude journal entry (`journal_20251030_000100.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/journal/journal_20251030_000100.md
[^claude-journal-102859]: Claude journal entry (`journal_20251030_102859.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/journal/journal_20251030_102859.md
[^claude-journal-013624]: Claude journal entry (`journal_20251030_013624.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/journal/journal_20251030_013624.md
[^claude-journal-124415]: Claude journal entry (`journal_20251030_124415.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/journal/journal_20251030_124415.md
[^claude-journal-132403]: Claude journal entry (`journal_20251030_132403.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/journal/journal_20251030_132403.md
[^claude-journal-150739]: Claude journal entry (`journal_20251030_150739.md`), https://github.com/zzsi/spooky-author-identification-claude/blob/main/journal/journal_20251030_150739.md
[^gemini-eval]: Docker evaluation transcript from the Gemini repo, yielding 0.42398 log loss (run at 2025-10-31 02:16:37) using `gemini-flash-2.5` because `gemini-pro-2.5` quota was exhausted.
[^gemini-journal-141848]: Gemini journal entry (`journal_20251030_141848.md`), https://github.com/zzsi/spooky-author-identification-gemini-20251030/blob/main/journal/journal_20251030_141848.md
[^gemini-journal-165930]: Gemini journal entry (`journal_20251030_165930.md`), https://github.com/zzsi/spooky-author-identification-gemini-20251030/blob/main/journal/journal_20251030_165930.md
[^gemini-journal-170206]: Gemini journal entry (`journal_20251030_170206.md`), https://github.com/zzsi/spooky-author-identification-gemini-20251030/blob/main/journal/journal_20251030_170206.md
[^runtime-note]: Run-time estimates derive from file modification times of the earliest and latest `logs/*.log` entries; they include idle gaps between iterations.

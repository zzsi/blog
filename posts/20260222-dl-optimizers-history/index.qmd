---
title: "A Tour of Deep Learning Optimizers"
date: 2026-02-22
categories: [ai, deep learning, optimization, llm]
format:
  html:
    toc: true
    toc-depth: 2
---

Optimizers are the control systems of deep learning. Architecture and data define what a model can represent, but the optimizer often determines whether training is stable, efficient, and reproducible.

Looking back through early 2026, optimizer progress came in waves: acceleration, adaptivity, regularization fixes, large-scale systems pressure, and now conditioning-heavy geometric methods.

This post is a historical tour: what changed, why it changed, and what still works as a default.

::: {.callout-note appearance="simple"}
## TL;DR

- **AdamW is still the default** for most deep learning training in early 2026 — LLMs, vision transformers, diffusion models.
- **SGD + momentum** remains competitive for convolutional architectures with well-tuned schedules.
- **Conditioning-based methods** (Muon, NorMuon, TEON, ARO) are the most active research frontier, showing promising gains at 1B+ scale but not yet displacing AdamW in documented production recipes.
- **Choosing the wrong optimizer — or misconfiguring the right one — can waste 10-30% of a training run's compute budget.** The payoff for getting this right scales with your training spend.
:::

## Mental model

At step $t$, training turns a stochastic gradient $g_t$ into a parameter update $\Delta \theta_t$:

$$
\theta_{t+1} = \theta_t + \Delta \theta_t.
$$

Every optimizer is trying to balance four controls:

- Direction control: where to move.
- Step-size control: how far to move.
- Stability control: how to survive noise and curvature.
- Resource control: memory, compute, and communication cost.

Most innovations can be grouped as:

1. Acceleration (momentum, Nesterov).
2. Preconditioning (AdaGrad, RMSProp, Adam).
3. Regularization-correct updates (AdamW).
4. Large-scale stabilization (LARS, LAMB).
5. Generalization-aware updates (SAM).
6. Systems-efficient updates (Adafactor, low-precision states).
7. Geometry-aware updates (natural-gradient lineage, Muon-style orthogonalization).

The rest of the post follows this lens: each optimizer wave solved one bottleneck, then exposed the next one.

## 1960s to 2000s: foundations that never went away

The key ideas predate modern deep learning:

- Polyak momentum (heavy ball) reduced zig-zag behavior in narrow valleys ([Polyak, 1964](https://doi.org/10.1016/0041-5553(64)90137-5)).
- Nesterov acceleration added look-ahead correction ([Nesterov, 1983](https://www.mathnet.ru/eng/dan/v269/i3/p543)).
- Natural gradient reframed descent in information geometry ([Amari, 1998](https://doi.org/10.1162/089976698300017746)).

These ideas established the long-term pattern: smooth noisy gradients, respect curvature, and seek invariance.
They also defined the core tension that still exists today: better conditioning usually costs more compute or implementation complexity.

## 2010 to 2014: getting deep nets to train at all

Early deep learning leaned on SGD + momentum because it was cheap and scalable, but tuning was fragile ([Sutskever et al., 2013](https://proceedings.mlr.press/v28/sutskever13.html)).

In practice, engineers could train deeper models, but only with careful learning-rate schedules and a lot of trial and error.

Adaptive methods arrived quickly:

- AdaGrad (2011): per-coordinate scaling, strong for sparse settings ([Duchi et al., 2011](https://jmlr.org/papers/v12/duchi11a.html)).
- RMSProp (2012): moving second-moment estimate to avoid AdaGrad's monotonic decay ([Hinton lecture notes, 2012](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)).
- AdaDelta (2012): reduced global LR sensitivity ([Zeiler, 2012](https://arxiv.org/abs/1212.5701)).

By the end of this period, the design direction was clear: momentum-like smoothing plus adaptive scaling.
That combination set up Adam's rapid adoption in the next wave.

## 2014 to 2019: Adam wins, AdamW corrects

Adam became the default because it reduced tuning friction across workloads ([Kingma and Ba, 2014](https://arxiv.org/abs/1412.6980)).

Its core update combines first and second moments:

$$
m_t=\beta_1 m_{t-1}+(1-\beta_1)g_t,\quad
v_t=\beta_2 v_{t-1}+(1-\beta_2)g_t^2,\quad
\Delta\theta_t=-\eta\frac{\hat m_t}{\sqrt{\hat v_t}+\epsilon}.
$$

Two important caveats emerged:

- Adam showed convergence pathologies in certain settings, which the AMSGrad fix addressed by tracking max second moments ([Reddi et al., 2018](https://openreview.net/forum?id=ryQu7f-RZ)).
- Adding L2 regularization to the gradient (standard "weight decay" in Adam) is not equivalent to true weight decay under adaptive preconditioning.

AdamW fixed the second issue by decoupling weight decay ([Loshchilov and Hutter, 2017](https://arxiv.org/abs/1711.05101)). This was a small implementation change with large impact in real training runs.
With decoupling, shrinkage is explicit:

$$
\theta_{t+1} = (1-\eta\lambda)\theta_t + \Delta\theta_t^{\text{adam}}.
$$

## 2017 to 2020: large-batch pressure

As batch sizes grew, optimization dynamics changed:

- LARS stabilized layer-wise relative updates for huge-batch CNNs ([You et al., 2017](https://arxiv.org/abs/1708.03888)).
- LAMB brought trust-ratio ideas to Adam moments for large-batch language pretraining ([You et al., 2019](https://arxiv.org/abs/1904.00962)).

These methods solved throughput bottlenecks, even though AdamW remained the broad default.
The key lesson was systems-driven: at scale, optimizer choice is partly a hardware-efficiency decision, not only a convergence decision.

## 2015 to 2023: curvature approximation at scale

Full second-order methods (computing the Hessian) are too expensive for large models. Two lines of work made curvature information practical:

- K-FAC approximated the Fisher information matrix with Kronecker-factored blocks, giving stronger preconditioning than Adam at modest extra cost ([Martens and Grosse, 2015](https://arxiv.org/abs/1503.05671)).
- Shampoo extended this to per-block matrix preconditioners with scalable update rules ([Gupta et al., 2018](https://arxiv.org/abs/1802.09568)).

Neither became a broad default — the implementation complexity and tuning overhead exceeded what most teams would absorb. But they proved that structured preconditioning could outperform diagonal methods, and they are direct intellectual ancestors of the 2024-2026 conditioning wave.
This lineage matters because modern conditioning-based methods reuse the same motivation with simpler operational surfaces.

## 2018 to 2023: memory and systems become first-class

At Transformer scale, optimizer state is expensive:

- Adafactor reduced second-moment memory with factorization ([Shazeer and Stern, 2018](https://arxiv.org/abs/1804.04235)).
- 8-bit optimizer states reduced memory pressure in practice ([Dettmers et al., 2021](https://arxiv.org/abs/2110.02861)).
- Communication-aware variants targeted distributed bandwidth ([Tang et al., 2021](https://arxiv.org/abs/2102.02888)).

The best optimizer is not only mathematically elegant; it must fit systems constraints.
For many, this systems constraint is exactly why AdamW kept winning despite stronger niche alternatives.

## 2020 to 2023: generalization-aware and tweak-heavy era

SAM made flatness bias explicit via a local worst-case objective ([Foret et al., 2020](https://arxiv.org/abs/2010.01412)).

Many variants (Lookahead, RAdam, AdaBelief, AdaBound) tuned warmup and update coupling ([Lookahead](https://arxiv.org/abs/1907.08610), [RAdam](https://arxiv.org/abs/1908.03265), [AdaBelief](https://arxiv.org/abs/2010.07468), [AdaBound](https://arxiv.org/abs/1902.09843)). Some helped in niches, but few replaced AdamW/SGD defaults broadly.

Lion added a search-discovered optimizer angle ([Chen et al., 2023](https://arxiv.org/abs/2302.06675)).
This period broadened the search space but did not produce a universal AdamW replacement.

## 2024 to 2025: geometry returns

Muon-style methods reframed the problem: instead of scaling each parameter independently (as Adam does), they orthogonalize the gradient update to reduce interference across dominant directions in weight matrices ([Muon implementation](https://github.com/KellerJordan/Muon), [modular-duality framing](https://arxiv.org/abs/2410.21265)).

The key insight is that diagonal preconditioning (Adam-family) ignores correlations between parameters within a layer. Structured preconditioning addresses this directly. Early results showed Muon scaling competitively to 1B+ parameters ([Jordan et al., 2025](https://arxiv.org/abs/2502.16982)), with a simpler implementation surface than K-FAC or Shampoo.

By end-2025, this looked promising but not universal. AdamW still dominated documented frontier recipes.
So the open question entering 2026 became replication at larger scales, not just first-paper wins.

## Late-2025 to early-2026: conditioning wave

A broader conditioning-focused wave followed, with several distinct approaches:

**Layer-wise orthogonalization:**

- NorMuon: neuron-wise normalization on top of Muon, improving conditioning balance ([2025](https://arxiv.org/abs/2510.05491)) `[1B+, open-source]`

**Conditioning + variance reduction:**

- MARS-M: combining variance reduction with matrix conditioning ([2025](https://arxiv.org/abs/2510.21800)) `[theory, small-scale, open-source]`

**Scaling transfer:**

- Matrix-preconditioner hyperparameter transfer across scales, showing that conditioning gains persist when transferring optimizer configs from small to large runs ([2025](https://arxiv.org/abs/2512.05620)) `[1B+, protocol]`

**Tensorized and rotated conditioning:**

- TEON: tensorized orthonormalization extending Muon beyond layer-wise structure ([2026](https://arxiv.org/abs/2601.23261)) `[theory, 1B-range]`
- ARO: adaptively rotated optimization in coordinate space ([2026](https://arxiv.org/abs/2602.09006)) `[1B+, protocol]`

The shift: conditioning is becoming a primary design axis, not a side detail. Evidence labels above (`[1B+]`, `[theory]`, etc.) indicate maturity — most of these methods have open-source implementations but limited independent replication so far.

Community reports ([Muon comparisons](https://huggingface.co/blog/KingNish/optimizer-part1), [distributed Muon validation](https://huggingface.co/blog/bird-of-paradise/reproducing-and-validating-distributed-muon)) are useful early signals but should stay secondary to controlled evaluations.
That distinction helps avoid overfitting decisions to anecdotal runs.

## What won in practice by early 2026

Defaults remain fairly stable:

- Frontier LLMs and VLMs (vision-language models): AdamW + warmup + decay + gradient clipping + selective decay exclusions.
- ViTs (Vision Transformers): AdamW.
- CNNs: SGD + momentum remains strong.
- Diffusion and flow-matching models: Adam/AdamW, often with EMA (exponential moving average of weights).
- LARS/LAMB: useful in specific extreme-batch throughput regimes.

## Why optimizer innovation keeps happening

Three forces interact repeatedly:

- Theory pressure: invariance, stability, objective reformulation.
- Empirical pressure: fewer knobs, faster loss reduction on real workloads.
- Systems pressure: memory, interconnect, and runtime constraints.

Methods that survive usually satisfy all three.

## Recipe chooser (2025 to early-2026)

| Setting | First choice | When to deviate |
| --- | --- | --- |
| LLM/VLM pretraining | AdamW + warmup/decay + clipping | Try Muon/conditioning if stability or scaling efficiency is bottleneck |
| Vision CNN | SGD + momentum + strong LR schedule | Use AdamW for transformer-heavy stacks or faster early convergence |
| ViT training | AdamW | Trial SAM or conditioning methods when plateaus appear |
| Diffusion/flow matching | AdamW (+ EMA) | Try Adafactor/low-precision states when memory dominates |
| Extreme large-batch throughput | LARS/LAMB | Stay with AdamW if batch size is moderate and tuning budget is limited |

### Starting hyperparameters

These are typical starting points, not universal optima. Always tune on your workload.

| Optimizer | Learning rate | beta1, beta2 | Weight decay | Notes |
| --- | --- | --- | --- | --- |
| AdamW (LLM) | 1e-4 to 6e-4 | 0.9, 0.95 | 0.01 to 0.1 | Warmup 1-5% of steps, cosine decay |
| AdamW (ViT) | 1e-4 to 3e-4 | 0.9, 0.999 | 0.01 to 0.3 | Higher decay common with strong augmentation |
| SGD + momentum (CNN) | 0.01 to 0.1 | momentum 0.9 | 1e-4 to 5e-4 | Step or cosine LR schedule |
| Muon | 0.01 to 0.05 | 0.9, — | 0.0 to 0.01 | Orthogonalization replaces some of weight decay's role |

### Fair comparison protocol

1. Same model and tokenizer.
2. Same token/image budget and data order.
3. Matched tuning budget across optimizers.
4. Report time-to-target, compute-to-target, and seed stability.

## Toy visual check (didactic)

A toy trajectory view is useful for building intuition before heavier benchmarks.

The GIFs below show optimizer trajectories projected onto a 2D PCA slice of the loss landscape for a small spirals classification task (2-layer MLP, 120 epochs, seed 42, default learning rates). This is **not** a benchmark — it illustrates qualitative behavior.

Adam trajectory:

![Adam on a toy loss landscape: adaptive per-coordinate scaling yields a smoother, more direct trajectory.](assets/spirals_adam.gif){fig-align="center" width="560px"}

SGD trajectory:

![SGD+Momentum on the same toy landscape: fixed global scaling leads to wider oscillation before settling.](assets/spirals_sgd.gif){fig-align="center" width="560px"}

In this toy run, Adam reaches a lower-loss region faster than SGD under the same short budget. Treat this as intuition only — real optimizer decisions should rely on controlled comparisons at representative scale. Repro steps: `../../examples/optimizer-bench/toy/README.md`.

## Closing

From heavy-ball momentum to conditioning-heavy methods, optimizer history is mostly a story of recurring constraints in new forms: curvature, noise, scale, and hardware budgets.

By early 2026, AdamW is still the center of gravity. The next durable shift is likely to come from better directional control and structured conditioning — not just better scalar learning-rate heuristics. What to watch for in the rest of 2026:

- Whether conditioning methods (Muon-family, ARO, TEON) show consistent gains under independent replication at 10B+ scale.
- Whether hyperparameter transfer protocols make these methods usable without per-run tuning.
- Whether systems-level integration (fused kernels, native framework support) lowers the adoption barrier enough to challenge AdamW as the default baseline.

The optimizer that wins next will not just be mathematically better — it will be easier to deploy correctly at scale.

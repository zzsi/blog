<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="ZZ Si">
<meta name="dcterms.date" content="2024-09-15">

<title>Computer vision for soccer games – Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About this blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zzsi"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/zhangzhangsi/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Computer vision for soccer games</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">computer vision</div>
                <div class="quarto-category">ai</div>
                <div class="quarto-category">sports</div>
                <div class="quarto-category">soccer</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>ZZ Si </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 15, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#reproducing-the-birds-eye-view-creation" id="toc-reproducing-the-birds-eye-view-creation" class="nav-link active" data-scroll-target="#reproducing-the-birds-eye-view-creation">Reproducing the birds-eye view creation</a></li>
  <li><a href="#under-the-hood" id="toc-under-the-hood" class="nav-link" data-scroll-target="#under-the-hood">Under the hood</a></li>
  <li><a href="#does-it-work-on-other-soccer-videos" id="toc-does-it-work-on-other-soccer-videos" class="nav-link" data-scroll-target="#does-it-work-on-other-soccer-videos">Does it work on other soccer videos?</a></li>
  <li><a href="#makeing-it-better-more-accurate-player-detection-and-tracking" id="toc-makeing-it-better-more-accurate-player-detection-and-tracking" class="nav-link" data-scroll-target="#makeing-it-better-more-accurate-player-detection-and-tracking">Makeing it better: more accurate player detection and tracking</a>
  <ul class="collapse">
  <li><a href="#transformers-for-object-tracking" id="toc-transformers-for-object-tracking" class="nav-link" data-scroll-target="#transformers-for-object-tracking">Transformers for object tracking</a></li>
  <li><a href="#vision-language-models-open-vocabulary-and-zero-shot-object-detection" id="toc-vision-language-models-open-vocabulary-and-zero-shot-object-detection" class="nav-link" data-scroll-target="#vision-language-models-open-vocabulary-and-zero-shot-object-detection">Vision-language models, open vocabulary and zero-shot object detection</a></li>
  </ul></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a></li>
  <li><a href="#business-use-cases" id="toc-business-use-cases" class="nav-link" data-scroll-target="#business-use-cases">Business use cases</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<p>I was intrigued to see this example where a variety (at least 5) computer vision techniques to create visual appealing analytics from soccer game footage. Soccer fans and coaches maenjoy this.</p>
<!-- ![](football-ai.mp4) -->
<p><video src="0bfacc_0-radar-2.mp4" class="img-fluid" controls=""><a href="0bfacc_0-radar-2.mp4">Video</a></video></p>
<p>This is an <a href="https://github.com/roboflow/sports">open source demo from Roboflow</a>, and is easy to reproduce. Since it is a proof of concept, more work needs to be done to make it work for other real world videos, where there a large portion of the soccer field is not visible, or when the camera moved fast (which happens quite often). This is a common challenge for practical computer vision: it can be hard to make an impressive model work on your data.</p>
<p>Below I share a workflow to reproduce both success and limitations of this soccer tracking example, and some ideas to improve it to make it work on more challenging data. Similar techniques can be applied to other sports, like tennis, (American) football, basketball, pickle ball, etc.</p>
<section id="reproducing-the-birds-eye-view-creation" class="level2">
<h2 class="anchored" data-anchor-id="reproducing-the-birds-eye-view-creation">Reproducing the birds-eye view creation</h2>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Pre-requisites">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-1-contents" aria-controls="callout-1" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Pre-requisites
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-1" class="callout-1-contents callout-collapse collapse show">
<section id="pre-requisites" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="pre-requisites">Pre-requisites</h3>
<ul>
<li>You need a machine with GPU to run the code. The code is tested on a machine with a GeForce RTX 3090, and it uses about 3GB of GPU memory.</li>
<li>You need to have <code>git</code>, <code>docker</code> and <code>python</code> (3.6+) installed.</li>
</ul>
</section>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Download">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-2-contents" aria-controls="callout-2" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Download
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-2" class="callout-2-contents callout-collapse collapse show">
<section id="step-1-download-the-code-and-data" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="step-1-download-the-code-and-data">Step 1: Download the code and data</h3>
<p><code>cvlization</code> is an open source repo with many working examples of computer vision workflows. Clone the repo:</p>
<pre><code>git clone https://github.com/kungfuai/cvlization.git
cd cvlization</code></pre>
<p>In <code>examples/sports/soccer_game_visual_tracking</code>, there is a README file that explains how to download the model weights and example video data (<code>pip</code> install <a href="https://pypi.org/project/gdown/">gdown</a> if you haven’t already).</p>
<pre><code>cd examples/sports/soccer_game_visual_tracking
bash download_data.sh</code></pre>
</section>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Install">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-3-contents" aria-controls="callout-3" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Install
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-3" class="callout-3-contents callout-collapse collapse show">
<section id="step-2-install-the-dependencies-by-building-a-docker-image" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="step-2-install-the-dependencies-by-building-a-docker-image">Step 2: Install the dependencies by building a docker image</h3>
<p>Change directory back to the root of the <code>cvlization</code> repo, and run</p>
<pre><code>bash examples/sports/soccer_game_visual_tracking/build.sh</code></pre>
</section>
</div>
</div>
<div class="callout callout-style-default callout-tip no-icon callout-titled" title="Run the code">
<div class="callout-header d-flex align-content-center" data-bs-toggle="collapse" data-bs-target=".callout-4-contents" aria-controls="callout-4" aria-expanded="true" aria-label="Toggle callout">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Run the code
</div>
<div class="callout-btn-toggle d-inline-block border-0 py-1 ps-1 pe-0 float-end"><i class="callout-toggle"></i></div>
</div>
<div id="callout-4" class="callout-4-contents callout-collapse collapse show">
<section id="step-3-run-the-code" class="level3 callout-body-container callout-body">
<h3 class="anchored" data-anchor-id="step-3-run-the-code">Step 3: Run the code</h3>
<pre><code>bash examples/sports/soccer_game_visual_tracking/predict.sh</code></pre>
<p>In this script, we are using a 30 second clip from a soccer game. The script will track the pitch and players, identify the team, goal keepers, referee, and ball, and generate a bird’s eye view video. Feel free to modify the script to use a different video or to change the tracking parameters.</p>
<p>You will find the output video in <code>examples/sports/soccer_game_visual_tracking/0bfacc_0-radar.mp4</code>. This is the video shown on the top of the page. On a machine with a GeForce RTX 3090, it takes about 20 minutes to run, with 3GB of GPU memory used.</p>
</section>
</div>
</div>
</section>
<section id="under-the-hood" class="level2">
<h2 class="anchored" data-anchor-id="under-the-hood">Under the hood</h2>
<p>The computer vision models and algorithms under the hood include:</p>
<ul>
<li>A keypoint detection (pose estimation) model for 32 keypoints on the soccer pitch (Yolo-v8, 70M, <a href="https://github.com/roboflow/sports/blob/main/examples/soccer/notebooks/train_pitch_keypoint_detector.ipynb">training notebook</a>, mAP=0.99, 1 hour on NVidia T4, trained on hundreds of images).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="keypoint.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="320"></p>
</figure>
</div>
<ul>
<li>An object detection model for players, referrees and goal keepers (Yolo-v8, 68M, <a href="https://github.com/roboflow/sports/blob/main/examples/soccer/notebooks/train_ball_detector.ipynb">training notebook</a>, mAP=0.79, 40min on NNivida L4).</li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="player.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="320"></p>
</figure>
</div>
<ul>
<li>Another object detection model for the ball (Yolo-v8, 68M, <a href="https://github.com/roboflow/sports/blob/main/examples/soccer/notebooks/train_ball_detector.ipynb">training notebook</a>, mAP=0.93, 1.3 hours on NVidia A100). The ball is very small in the image, so it is hard to detect.</li>
<li>A multi-object tracking model to track the players and the ball (<a href="https://github.com/ifzhang/ByteTrack">Bytetrack</a>, <a href="https://supervision.roboflow.com/trackers/#bytetrack">implementation and python API</a>).</li>
<li>A vision embedding model and clustering algorithm for team identification. <a href="https://arxiv.org/abs/2303.15343">SigLIP</a> is used to extract embedding vectors from cropped players. <a href="https://arxiv.org/abs/1802.03426">UMAP</a> is used for dimensionality reduction. <a href="https://en.wikipedia.org/wiki/K-means_clustering">K-means</a> is used for clustering. Also Resolve the team IDs for detected goalkeepers based on the proximity to team centroids (based on player locations).</li>
<li>An image registration/stitching algorithm to create the bird’s eye view. Homography is estimated between the pitch keypoints and the reference coornidates of the pitch, using OpenCV’s <a href="https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html#ga4abc2ece9fab9398f2e560d53c8c9780">findHomography</a>. The pitch in the footage is then warped to a top-down view using <a href="https://docs.opencv.org/3.0-beta/modules/core/doc/operations_on_arrays.html#perspectivetransform">perspectiveTransform</a>.</li>
<li>Player re-identification models (e.g.&nbsp;<a href="https://github.com/MCG-NJU/MOTIP">MOTIP</a>). When the footage is cut or camera is changed to a different angle, the player IDs are lost. We need to re-identify the players in order to connect the player tracks across different clips. I did not find the implemetation in this POC.</li>
</ul>
</section>
<section id="does-it-work-on-other-soccer-videos" class="level2">
<h2 class="anchored" data-anchor-id="does-it-work-on-other-soccer-videos">Does it work on other soccer videos?</h2>
<p>I picked a random soccer game clip, and the result is not as good as the example video. The camera moved faster, zooming in to a partial view of the pitch near the goal post. This posed challenges to the keypoint detection model, and the player tracking model. Some players were not detected due to motion blur and occlusion. Key points of the pitch were not detected in some frames, and the algorithm was not able to create a bird’s eye view for those frames. The result is shown below:</p>
<p><video src="radar-soccer1_clip1_2.mp4" class="img-fluid" width="550" controls=""><a href="radar-soccer1_clip1_2.mp4">Video</a></video></p>
<p>Regardless, it is a great starting point to build a more reliable system for soccer game analytics. For fun, I also tried it on a very challenging video with a couple of professional players against 100 pupils. Interestingly, the algorithm was able to detect most the players, and create a bird’s eye view, as long as a large portion of the pitch is visible:</p>
<p><img src="soccer_funny_radar.png" class="img-fluid" width="550"></p>
</section>
<section id="makeing-it-better-more-accurate-player-detection-and-tracking" class="level2">
<h2 class="anchored" data-anchor-id="makeing-it-better-more-accurate-player-detection-and-tracking">Makeing it better: more accurate player detection and tracking</h2>
<section id="transformers-for-object-tracking" class="level3">
<h3 class="anchored" data-anchor-id="transformers-for-object-tracking">Transformers for object tracking</h3>
<p>Accurate tracking requires attending to relationships between detected players on different frames, their roles, jersey colors etc. Transformers architecture is well suited for this task.</p>
<section id="global-tracking-transformer" class="level4">
<h4 class="anchored" data-anchor-id="global-tracking-transformer">Global tracking transformer</h4>
<p><a href="http://arxiv.org/abs/2203.13250">Global tracking transformers</a> takes a video as input, and predict object tracks in an end-to-end fashion. It was trained on LVIS and COCO, capable of tracking 1000+ categories of objects. Below is the result for tracking persons and the ball. It also identified the billboards though they are not directly useful for our purpose here. This is the tracking result overlayed on the input video:</p>
<p><video src="soccer1_clip1_gtr_output.mp4" class="img-fluid" width="550" controls=""><a href="soccer1_clip1_gtr_output.mp4">Video</a></video></p>
<p>Comparing YOLOv8 and Global Tracking Transformer, the latter seems more accurate.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-nrow="1">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="player_det_yolo.png" class="img-fluid figure-img" width="360"></p>
<figcaption>YOLOv8</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="player_det_gtr.png" class="img-fluid figure-img" width="350"></p>
<figcaption>Global Tracking Transformer</figcaption>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="vision-language-models-open-vocabulary-and-zero-shot-object-detection" class="level3">
<h3 class="anchored" data-anchor-id="vision-language-models-open-vocabulary-and-zero-shot-object-detection">Vision-language models, open vocabulary and zero-shot object detection</h3>
<p>With recent advances in vision-language models, we can leverage the visual knowledge in pretrained large models. How well do they work in detecting players?</p>
<section id="grounding-dino" class="level4">
<h4 class="anchored" data-anchor-id="grounding-dino"><a href="https://github.com/IDEA-Research/GroundingDINO">Grounding DINO</a></h4>
<p>This model has a DINO transformer backbone and produced by grounded pre-training. You can prompt the model with a sentence or a phrase, and it will highlight the corresponding region in the image. Below is the architecture of Grounding DINO:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="grounding_dino.png" class="img-fluid figure-img" width="550"></p>
<figcaption>Architecture of Grounding DINO</figcaption>
</figure>
</div>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="result_grounding_dino_soccer_goalie.png" class="img-fluid figure-img"></p>
<figcaption>With one prompt, Grounding DINO was able to detect players but had a hard time distinguishing the goal keeper from other players.</figcaption>
</figure>
</div>
</section>
<section id="yolo-world" class="level4">
<h4 class="anchored" data-anchor-id="yolo-world"><a href="https://arxiv.org/abs/2401.17270">YOLO World</a></h4>
<p>This model is an open-vocabulary object detection model. It can detect objects that are not in the training set, and can be used for zero-shot object detection. You can prompt it with a list of words, such as “player, ball, goal keeper”.</p>
<p>Compared to Grounding DINO, YOLO World seems less accurate and misses some players when they overlap.</p>
<div class="quarto-figure quarto-figure-left">
<figure class="figure">
<p><img src="result_yoloworldxl.png" class="img-fluid figure-img"></p>
<figcaption>YOLO-World-XL player detection result.</figcaption>
</figure>
</div>
<p>These are just two examples of recent models.</p>
<!-- ### Player re-identification

This is a challenging problem, and there are many ways to approach it. One way is to use a deep learning model to extract features from the players, and then use a clustering algorithm to group the features into different players. This is similar to the team identification algorithm used in the original code, but it is more challenging because the players are moving and changing their appearance. -->
</section>
</section>
</section>
<section id="datasets" class="level2">
<h2 class="anchored" data-anchor-id="datasets">Datasets</h2>
<p>You may need to fine tune the models on more soccer game videos with annotations. Here are some datasets that can be useful:</p>
<p><a href="https://www.soccer-net.org/home">SoccerNet</a> is a large-scale dataset for soccer analysis. It contains 550 complete broadcast soccer games and 12 single camera games taken from the major European leagues. It supports various vision tasks such as action spotting, camera calibration, player re-identification and tracking.</p>
<p>This <a href="https://www.kaggle.com/datasets/shreyamainkar/football-soccer-videos-dataset">Kaggle dataset</a> also contains soccer game videos from Premier League showdowns to FIFA World Cup classics.</p>
</section>
<section id="business-use-cases" class="level2">
<h2 class="anchored" data-anchor-id="business-use-cases">Business use cases</h2>
<p>Boardly, here are some areas where computer vision can be used in soccer analytics:</p>
<ul>
<li><strong>Performance Analysis</strong>: By tracking player movement, positioning, and interactions, teams can better understand individual and team performance, making it easier to identify strengths and areas for improvement.</li>
<li><strong>Tactical Insights</strong>: Coaches can analyze formations, pressing patterns, and set-pieces to gain a competitive edge, adjusting their game plans based on data.</li>
<li>** Player Development**: Young athletes can leverage computer vision technology to receive feedback on their performance and improve their skills over time.</li>
<li><strong>Fan Engagement</strong>: Computer vision can create engaging, immersive content for fans, such as 3D replays or interactive match highlights, bringing them closer to the action.</li>
</ul>
<p>Here is a very incomplete list of companies and use cases:</p>
<ul>
<li><a href="https://www.veo.co/">Veo</a>: AI-powered cameras for automatic sports recording, tracking game action, and AI-tagged highlights for analysis.</li>
<li><a href="https://traceup.com/">Traceup</a>: Video captures that allow tracking players individually, creating personalized highlight reels that parents, players, and coaches can view from various angles.</li>
<li><a href="https://www.track160.com/soccer-technology">Track160</a>: Skeleton tracking, identifying and monitoring the movement of players and the ball, tagging and analyzing events in a match, physical and tactical breakdowns of player performances.</li>
<li><a href="https://rd.nytimes.com/projects/modeling-key-world-cup-moments-with-machine-learning/">NY Times created 3D stories</a> that allow fans to experience game-defining moments from multiple angles and gain deeper insights into player positioning, ball movement, and tactics.</li>
</ul>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This is just a start. I am glad to see computer vision applied to everyday life, and hope this post spark some ideas.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/zzsi\.github\.io\/blog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="ZZ Si">
<meta name="dcterms.date" content="2024-10-02">

<title>MiniSora: Learnings from training a Minimal Video Generation Model (Part 1) – Blog</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html"> 
<span class="menu-text">About this blog</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/zzsi"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/zhangzhangsi/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">MiniSora: Learnings from training a Minimal Video Generation Model (Part 1)</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">generative ai</div>
                <div class="quarto-category">video generation</div>
                <div class="quarto-category">cost efficient training</div>
                <div class="quarto-category">scaling laws</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>ZZ Si </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 2, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In February 2024, OpenAI introduced <a href="https://openai.com/index/video-generation-models-as-world-simulators/">SORA</a>, a groundbreaking video generation model capable of creating high-resolution videos that look almost real. These videos <a href="https://arxiv.org/abs/2402.17403">exhibit 3D consistency</a> and appear to follow physical laws, marking a significant leap in AI’s ability to understand and recreate visual information. Its significance feels like GPT-2 for language models. While commercial applications are still in their early stages, SORA demonstrates a path forward for human-level visual storytelling.</p>
<p>Inspired by this breakthrough, I conducted a hundred experiments on a smaller scale in April 2024. My goal was to explore whether it’s possible to train a minimal video generation model with limited resources. The field is advancing rapidly; while people await SORA’s official release, both open-source projects (OpenSora, OpenSoraPlan, CogVideoX) and commercial models (KLing, Luma, Runway, Synthesia) are gaining momentum. Low-cost training recipes are being shared, such as Andrei Karpathy’s <a href="https://github.com/karpathy/llm.c/discussions/481">$20 90-minute training run for GPT-2</a>. There are numerous new techniques to try, but first, I’d like to summarize and share my learnings so far, hoping to inspire like-minded individuals to pursue similar paths.</p>
<p>Thanks to a small-scale setup, I was able to complete training runs within reasonable timeframes using a moderate GPU. Initial success was achieved in proving the concept on a “flying MNIST” toy world. With 250 A10-GPU hours (or $200 <a href="https://lambdalabs.com/service/gpu-cloud#pricing">on Lambda Labs</a>, approximately 1/3 of the price on AWS G5.8xlarge), I trained a video generation model capable of producing decent quality 256x256 resolution videos. The quality was good enough to fool myself if I glanced for 1 second. The model appeared to learn object permanence, distinct digits with consistent colors, and the simple physics governing their movements. More details can be found in this <a href="https://wandb.ai/zzsi_kungfu/flying_mnist/reports/Mini-Sora-on-Flying-MNIST--Vmlldzo3NTU3MjY5">report</a> on Weights &amp; Biases.</p>
</section>
<section id="pareto-frontier-aiming-for-good-and-small" class="level2">
<h2 class="anchored" data-anchor-id="pareto-frontier-aiming-for-good-and-small">Pareto frontier: Aiming for good and small</h2>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="green_and_red_arrow.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="600"></p>
</figure>
</div>
<p>This graph from <a href="https://arxiv.org/pdf/2409.14160">“Hype, Sustainability, and the Price of the Bigger-is-Better Paradigm in AI”</a> illustrates a key challenge in AI development. SORA would be a frontier model at the resource-intensive end of the spectrum. We want to move in the direction of the green arrow, striving for lower training cost while maintaining high quality.</p>
<p>Access to vast computational resources, such as 10,000 A100 GPUs, is limited to a handful of organizations. Even if such resources were widely available, focusing solely on resource-intensive methods would be an inefficient use of our capabilities. The design space for training recipes is vast, and a strategic approach involves exploring this space through low-cost experiments before scaling up when confidence is high.</p>
<p>This raises an intriguing question: With a modest budget, is it possible to train a general-purpose video generation model comparable to SORA?</p>
</section>
<section id="the-need-for-controlling-the-domain-complexity" class="level2">
<h2 class="anchored" data-anchor-id="the-need-for-controlling-the-domain-complexity">The need for controlling the domain complexity</h2>
<section id="the-challenge-of-training-general-purpose-models-with-limited-resources" class="level3">
<h3 class="anchored" data-anchor-id="the-challenge-of-training-general-purpose-models-with-limited-resources">The challenge of training general-purpose models with limited resources</h3>
<p>SORA’s training costs likely run into tens of millions of dollars, driven by both data and model size. Larger datasets necessitate longer training times, while bigger models require both extended training periods and more high-end GPUs.</p>
<p>Is it feasible to train a high-quality model with a significantly smaller dataset? This seems impossible due to the inherent complexity of our world. Are one million video clips sufficient to capture our world’s complexity? 10 Million? 100 Million? Probably more than that. While sample-efficient algorithms can help reduce the required data size, the order of magnitude for necessary data likely remains substantial.</p>
<p>Similarly, training a high-quality model with a much smaller architecture presents its own challenges. Unless a dramatically more efficient architecture than Transformers emerges, a small model would struggle to capture the complexity present in such vast datasets.</p>
<p>Therefore, to make progress with limited resources, we must find ways to reduce the data size.</p>
</section>
<section id="exploring-niche-domains-a-path-to-low-budget-training" class="level3">
<h3 class="anchored" data-anchor-id="exploring-niche-domains-a-path-to-low-budget-training">Exploring niche domains: A path to low-budget training</h3>
<p>Niche domains can be significantly simpler than our physical world, potentially allowing a few tens of thousands of observations to sufficiently represent the domain. With a drastic reduction in data size, smaller models and lower training costs become feasible.</p>
<p>We can conceptualize a series of domains, progressing from simple to complex:</p>
<ol type="1">
<li>2D Flying MNIST (a 2D world with colorful handwritten digits moving at constant speed, bouncing off boundaries)</li>
<li>2D arcade games (Pong, Breakout, etc.)</li>
<li>Anime and cartoons</li>
<li>Limited locations: video walkthroughs of 3D house models, fly-through views of objects (e.g., NERF models)</li>
<li>Limited objects: close-up videos of specific subjects (e.g., dogs, selfie videos)</li>
<li>Limited scenery: footage of hiking trails, beaches, etc.</li>
<li>Public video datasets: UCF-101, Panda-70M, InterVid, etc.</li>
<li>The real world, and our collective video reservoir.</li>
</ol>
<p>A strategic approach involves starting from the simplest domain and gradually progressing towards more complex ones. Effective training recipes discovered in simpler domains are expected to scale to more complex scenarios with straightforward increases in data and model size.</p>
<p>Interestingly, this mirrors how humans learn: start from simple lessons and gradually build up to more complex concepts.</p>
</section>
<section id="pre-train-or-fine-tune" class="level3">
<h3 class="anchored" data-anchor-id="pre-train-or-fine-tune">Pre-train or fine-tune?</h3>
<p>Fine-tuning is an effective strategy to reduce training costs, but it comes with certain limitations:</p>
<ol type="1">
<li>Fixed architecture: The model’s architecture is predetermined, which can be a significant constraint as we may still be far from an optimal design for video generation tasks.</li>
<li>VAE dependency: Pre-trained weights often rely on a specific Variational Autoencoder (VAE), limiting the design space and opportunities to further reduce training costs.</li>
</ol>
<p>Despite these limitations, fine-tuning has shown promising results. For example, the team at Lambda Labs open-sourced an intriguing <a href="https://wandb.ai/lambdalabs/lego/reports/Text2Bricks-Fine-tuning-Open-Sora-in-1-000-GPU-Hours--Vmlldzo4MDE3MTky">Text2Bricks experiment</a>, fine-tuning OpenSora weights on Lego videos. This project required approximately 1000 A100 GPU hours and 10,000 videos. We can anticipate further reductions in cost as more advanced pre-trained models become available and more sample-efficient fine-tuning algorithms are developed.</p>
<p>For my experiments, I try to find the simplest domain that has non-trivial complexity: a toy 2D world with flying digits. The scale of this toy world is small enough that pre-training from scratch is not prohibitively expensive, allowing for more freedom in exploring different model architectures and training strategies.</p>
<p>Let’s see some details.</p>
</section>
</section>
<section id="flying-mnist-simulator" class="level2">
<h2 class="anchored" data-anchor-id="flying-mnist-simulator">Flying MNIST Simulator</h2>
<p>A <a href="https://github.com/caganselim/flying_mnist/blob/master/flying_mnist.py">Python script</a> is used to simulate a toy 2D world where colorful handwritten digits fly and bounce around. An example is shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="00000.gif" class="img-fluid quarto-figure quarto-figure-center figure-img" width="256"></p>
</figure>
</div>
<p>For training, I used up to 100k clips, each with 32 frames, covering roughly 6 seconds at 5 fps. This amounts to 160 hours of video. Is this a lot? Let’s compare with human learning. If a baby is awake and actively observing 5 hours per day, it would be roughly a month of learning. It would be interesting to see if the AI can learn:</p>
<ul>
<li>Object identity: a digit is a digit, and not a random blob</li>
<li>Object permanence: a digit does not suddenly disappear</li>
<li>Distinct digits: whether the model can learn to distinguish between different digits</li>
<li>Consistent colors: color of a digit remains consistent</li>
<li>Physics: digits follow simple physics - constant speed and bounce off walls</li>
</ul>
</section>
<section id="vae-the-compressor" class="level2">
<h2 class="anchored" data-anchor-id="vae-the-compressor">VAE: The Compressor</h2>
<p>The first model to train is a compressor. Unlike language, images and videos have very high dimensionality: a tiny 2-second 256x256 video contains over 100 million numbers. Compression is necessary for the model to work.</p>
<p>The compressor of choice is a VAE (Variational Auto-Encoder) with an encoder and decoder. The encoder converts a video clip into a latent space, and the decoder converts the latent space back to a video clip. The latent space is a compact representation of the original data and is easier to model.</p>
<p>Optionally, you can quantize the latent space using vector quantization, which gives you a VQ-VAE. Quantization gives rise to a vocabulary of visual words or tokens. This enables the use of language model training recipes on 1-dimensional (flattened) sequences of token IDs. While I was initially skeptical, the results were surprisingly good.</p>
<p>Training a small VAE is relatively quick. I trained a spatial-temporal VQ-VAE with 4x temporal compression and 4x4 spatial compression, using a vocabulary size of 5120. The <a href="https://wandb.ai/zzsi_kungfu/videogpt/runs/kbu39ped/overview">training run documented in Weights &amp; Biases</a> achieved a good balance of reconstruction quality and compression rate. It took about 2 A10 GPU hours to converge.</p>
<p>With this VAE model, you can transform a 32-frame video clip (32 x 3 x 256 x 256) into latent “tokens”. Without quantization, the compressed representation of the video has a shape of 8 x 4 x 64 x 64 (each “token” is a 4-dimensional floating point vector, and there are 8 x 64 x 64 = 32,768 tokens). With quantization, the compressed representation is simply 8 x 64 x 64 = 32,768 integers (token IDs). The range of the token IDs is from 0 to 5,023.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="minisora-tokenizer.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="800"></p>
</figure>
</div>
<p>With this compact tokenized representation, we are ready to train a generator.</p>
</section>
<section id="generator-in-the-latent-space" class="level2">
<h2 class="anchored" data-anchor-id="generator-in-the-latent-space">Generator in the Latent Space</h2>
<p>There are two approaches to generate video in the latent space: the autoregressive next-token predictor (language model) and the diffusion model.</p>
<section id="autoregressive-next-token-predictor" class="level3">
<h3 class="anchored" data-anchor-id="autoregressive-next-token-predictor">Autoregressive Next-Token Predictor</h3>
<p>Each 32-frame video clip is represented as a sequence of 32,768 tokens. The video clips are then concatenated to form a long sequence, separated by a special start-of-video token. This long sequence is fed into a language model training recipe.</p>
<p>I used <a href="https://github.com/karpathy/nanoGPT">nanoGPT</a> to train a 60MB model with the GPT-2 architecture. The model is trained to predict the next token ID in the latent space, instead of the next English token. It worked surprisingly well and began to learn the spatial-temporal patterns quickly.</p>
<p>The main ingredient for video quality is ensuring a sufficiently large context window. I used 6,000 tokens, which is much larger than the typical GPT-2 setting. However, this is still a small window size for video. Each video frame is 4,096 tokens, so this context window allows the model to look back only slightly more than one frame, making temporal consistency challenging to enforce.</p>
<p>Secondly, the training sample size is crucial. Using 100k clips produces better results than 10k clips, and much better than 1k clips. The question remains whether we should use even more data. I hope not, as if such a simple 2D world requires much more than 100k training examples, it would be concerning for more complex domains.</p>
<p>This <a href="https://wandb.ai/zzsi_kungfu/flying_mnist/runs/a1bwjcjv/workspace">training run</a> showcases one of the better results using nanoGPT.</p>
<p>The generated videos start out as random compositions of visual tokens:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="gpt107/generated_video_2_3c662a0176f524a929c6.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="gpt107/generated_video_2_3c662a0176f524a929c6.mp4">Video</a></video></p>
</figure>
</div>
<p>After 6 hours of training, line strokes started to appear:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="gpt107/generated_video_11_576e81d32f3c32e7520b.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="gpt107/generated_video_11_576e81d32f3c32e7520b.mp4">Video</a></video></p>
</figure>
</div>
<p>24 hours in, the digits began to emerge, but temporal consistency was poor:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="gpt107/generated_video_38_dd436a43f42f243eccd0.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="gpt107/generated_video_38_dd436a43f42f243eccd0.mp4">Video</a></video></p>
</figure>
</div>
<p>After 10 days, consistency and physics were much improved:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="gpt107/generated_video_596_cd28aa78e8ae7560bbe2.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="gpt107/generated_video_596_cd28aa78e8ae7560bbe2.mp4">Video</a></video></p>
</figure>
</div>
<p>For comparison, here’s a <a href="https://wandb.ai/zzsi_kungfu/flying_mnist/runs/w7yw9264?nw=nwuserzzsi_kungfu">training run using a 1,024 token context window</a>.</p>
<p>With a smaller context window, the training time is much shorter (1 day to converge), but temporal consistency is poor, and digits would suddenly appear throughout the clip:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="gpt49/generated_video_59_7fb9a0f1a270d1a588f4.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="gpt49/generated_video_59_7fb9a0f1a270d1a588f4.mp4">Video</a></video></p>
</figure>
</div>
</section>
<section id="diffusion-model" class="level3">
<h3 class="anchored" data-anchor-id="diffusion-model">Diffusion Model</h3>
<p>For the diffusion model, I used <a href="https://github.com/hpcaitech/Open-Sora/tree/main/opensora/models/stdit">ST-DIT</a> from OpenSora and <a href="https://huggingface.co/stabilityai/sd-vae-ft-mse">Stable Diffusion’s SD VAE</a>.</p>
<p>In this approach, the context window encompasses the entire video clip, so I expected more temporal consistency than the autoregressive counterpart. Training sample size still plays a significant role. Using a 24GB A10 GPU, I needed to use a small version of the diffusion transformer model.</p>
<p>A representative training run can be found <a href="https://wandb.ai/zzsi_kungfu/flying_mnist/runs/ashsv7ru">here</a>.</p>
<p>The generated videos also start out as random compositions of visual tokens (resembling crops of natural images this time):</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="stdit105/generated_video_166_0da47cf99526d25f9731.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="stdit105/generated_video_166_0da47cf99526d25f9731.mp4">Video</a></video></p>
</figure>
</div>
<p>After one day of training, localized dream-like flowing patterns emerged, though they didn’t yet resemble digits:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="stdit105/generated_video_5640_a7237aa886239f8b2913.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="stdit105/generated_video_5640_a7237aa886239f8b2913.mp4">Video</a></video></p>
</figure>
</div>
<p>On day 3, the moving patterns began to look like digits, but they were so fluid that they seemed to lack “bones”-like structure:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="stdit105/generated_video_25926_f134216c9fe955b129b9.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="stdit105/generated_video_25926_f134216c9fe955b129b9.mp4">Video</a></video></p>
</figure>
</div>
<p>By day 10, the digits were much more stable and distinct, and the moving patterns were steady and smooth:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><video src="stdit105/generated_video_80344_751049f83dcaeef15814.mp4" class="img-fluid quarto-figure quarto-figure-center" width="256" controls=""><a href="stdit105/generated_video_80344_751049f83dcaeef15814.mp4">Video</a></video></p>
</figure>
</div>
</section>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What’s Next</h2>
<p>250 A10 hours (or approximately 80 A100 hours, costing around $200) proved sufficient to adequately solve the video generation task for the 2D toy world of Flying MNIST Digits.</p>
<p>Context window size and data sample size are important factors for quality, but also drive up cost. There are numerous new techniques that are worth exploring to improve quality while reducing cost. Here’s a non-exhaustive list:</p>
<ul>
<li>Flow matching: This technique could enhance the temporal consistency of generated videos.</li>
<li>Better quantized VAE for auto-regressive video generation: Improving the VAE could lead to more efficient and higher-quality latent representations.</li>
<li>Token masking: This could reduce the <span class="math inline">\(N\)</span> in the <span class="math inline">\(O(N^2)\)</span> complexity of attention layers, potentially speeding up training and inference.</li>
<li>Coarse-to-fine generation: Generating whole video frames at the coarse level first, then progressively refining to small details. This can dramatically reduce the context window size and compute cost.</li>
<li>Better positional encoding for long context windows in the temporal-spatial setting.</li>
<li>Hyper-optimized LLM training with long context (e.g., <code>llm.c</code>).</li>
<li>Combining strengths of autoregressive and diffusion models could yield interesting results.</li>
<li>Curriculum learning: Starting with simpler tasks and progressively increasing difficulty could improve learning efficiency.</li>
</ul>
<p>These avenues for improvement suggest that there’s still significant potential to enhance the quality and efficiency of video generation models, even in this simplified domain. As we continue to refine these techniques, we’ll be better positioned to tackle more complex video generation tasks in the future.</p>
<p>More to come.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/zzsi\.github\.io\/blog\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>